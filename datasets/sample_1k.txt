title : classifying documents within multiple hierarchical datasets using multi-task_learning ; abstract : multi-task_learning ( mtl ) is a supervised_learning paradigm in which the prediction models for several related_tasks are learned jointly to achieve better generalization performance . when there are only a few training_examples per task , mtl considerably outperforms the traditional single task learning ( stl ) in terms of prediction_accuracy . in this work we develop an mtl based approach for classifying documents that are archived within dual concept hierarchies , namely , dmoz and wikipedia . we solve the multi-class classification problem by defining oneversus-rest binary_classification tasks for each of the different classes across the two hierarchical datasets . instead of learning a linear_discriminant for each of the different tasks independently , we use a mtl approach with relationships between the different tasks across the datasets established using the non-parametric , lazy , nearest_neighbor approach . we also develop and evaluate a transfer_learning ( tl ) approach and compare the mtl ( and tl ) methods against the standard single task learning and semi-supervised_learning approaches . our empirical results demonstrate the strength of our developed methods that show an improvement especially when there are fewer number of training_examples per classification_task . © 2013 ieee .
title : a novel model combining transformer and bi-lstm for news categorization ; abstract : news categorization ( nc ) , the aim of which is to identify distinct categories of news through analyzing the contents , has acquired substantial progress since deep_learning was introduced into the natural_language_processing ( nlp ) field . as a state-of-art model , transformer & # x2019 ; s classification performance is not satisfied compared with recurrent_neural_network ( rnn ) and convolutional_neural_network ( cnn ) if it does not get pretrained . based on the transformer model , this article proposes a novel framework that combines bidirectional_long_short-term_memory ( bi-lstm ) network and transformer to solve this problem . in the suggested framework , the self-attention_mechanism is substituted with bi-lstm to capture the semantic information from sentences . meanwhile , an attention_mechanism model is applied to focus on those important words and adjust their weights to solve the problem of long-distance information loss . with pooling network , the network complexity can be reduced and the main features can be highlighted by halving the dimension of the hidden_state . finally , after acquiring the hidden_representation by the above structures , we utilize a contraction network to further capture the long-range associations from a text . experiments on three large-scale corpora were performed to evaluate the suggested framework , and the results demonstrate that our model outperforms other models such as deep pyramid cnn ( dpcnn ) , transformer .
title : cross-lingual consistency of phonological features : an empirical_study ; abstract : the concept of a phoneme arose historically as a theoretical abstraction that applies language-internally . using phonemes and phonological features in cross-linguistic settings raises an important question of conceptual validity : are contrasts that are meaningful within a language also empirically robust across languages ? this paper develops a method for assessing the cross-linguistic consistency of phonological features in phoneme inventories . the method involves training separate binary neural classifiers for several phonological contrast in audio spans centered on particular segments within continuous speech . to assess cross-linguistic consistency , these classifiers are evaluated on held-out languages and classification quality is reported . we apply this method to several common phonological contrasts , including vowel height , vowel frontness , and retroflex consonants , in the context of multi-speaker corpora for ten languages from three language_families ( indo-aryan , dravidian , and malayo-polynesian ) . we empirically_evaluate and discuss the consistency of phonological contrasts derived from features found in phonological ontologies such as panphon and phoible .
title : nycu-two at memotion 3 : good foundation , good teacher , then you have good meme analysis ; abstract : this paper presents a robust solution to the memotion 3.0 shared_task . the goal of this task is to classify the emotion and the corresponding intensity expressed by memes , which are usually in the form of images with short captions on social_media . understanding the multi-modal features of the given memes will be the key to solving the task . in this work , we use clip to extract aligned image-text features and propose a novel meme sentiment_analysis framework , consisting of a cooperative teaching model ( ctm ) for task a and a cascaded emotion classifier ( cec ) for tasks b & c . ctm is based on the idea of knowledge distillation , and can better predict the sentiment of a given meme in task a ; cec can leverage the emotion intensity suggestion from the prediction of task c to classify the emotion more precisely in task b . experiments show that we achieved the 2nd place ranking for both task a and task b and the 4th place ranking for task c , with weighted f1-scores of 0.342 , 0.784 , and 0.535 respectively . the results show the robustness and effectiveness of our framework . our code is released at github .
title : development of information system for textual_content categorizing based on ontology ; abstract : the methods and means of using ontologies within systems for the categorization of textual_content were created . also , a method for optimizing the definition of which rubrics best relate to a certain text content was developed . the intellectual system that will use the methods developed earlier , as well as other research results was implemented . the results will allow users to easily filter their text content . the system developed has an intuitive user_interface .
title : e-commerce merchant classification using website information ; abstract : with the rapid_growth of the e-commerce landscape , classifying e-commerce merchants has become an important task as it is an integral part of various processes in e-commerce . one of the examples is merchant on boarding , where the category of an e-commerce merchant has proven to be a good indicator of the risk of the merchant . however , since most of e-commerce businesses do not have brick-and-mortar stores from which we can assess it directly , the only source of information regarding the merchant itself is its website . thus , we can view this problem as a web classification problem , where we classify e-commerce websites into a category . in this research , we aim to build an end-to-end classification system for e-commerce websites . there are a few challenges such as the number of pages to be processed , imbalanced_dataset , and the language of e-commerce websites that can be mixed_language . we built a website classification system and experimented with case_study of indonesian and english e-commerce webs , that are classified into 37 different categories . our best result achieved an f-score of 0.83 .
title : comparison of machine_learning for sentiment_analysis in detecting anxiety based on social_media data ; abstract : all groups of people felt the impact of the covid-19_pandemic . this situation triggers anxiety , which is bad for everyone . the government 's role is very influential in solving these problems with its work program . it also has many pros_and_cons that cause public anxiety . for that , it is necessary to detect anxiety to improve government programs that can increase public expectations . this study applies machine_learning to detecting anxiety based on social_media comments regarding government programs to deal with this pandemic . this concept will adopt a sentiment_analysis in detecting anxiety based on positive and negative comments from netizens . the machine_learning methods implemented include k-nn , bernoulli , decision_tree classifier , support_vector classifier , random_forest , and xg-boost . the data sample used is the result of crawling youtube comments . the data used amounted to 4862 comments consisting of negative and positive data with 3211 and 1651 . negative data identify anxiety , while positive data identifies hope ( not anxious ) . machine_learning is processed based on feature_extraction of count-vectorization and tf-idf . the results showed that the sentiment data amounted to 3889 and 973 in testing , and training with the greatest accuracy was the random_forest with feature_extraction of vectorization count and tf-idf of 84.99 % and 82.63 % , respectively . the best precision test is k-nn , while the best recall is xg-boost . thus , random_forest is the best accurate to detect someone 's anxiety based-on data from social_media .
title : linear transformations for cross-lingual sentiment_analysis ; abstract : this paper deals with cross-lingual sentiment_analysis in czech , english and french languages . we perform zero-shot cross-lingual classification using five linear transformations combined with lstm and cnn based classifiers . we compare the performance of the individual transformations , and in addition , we confront the transformation-based approach with existing state-of-the-art bert-like models . we show that the pre-trained embeddings from the target domain are crucial to improving the cross-lingual classification results , unlike in the monolingual classification , where the effect is not so distinctive .
title : evaluation and improvement of chatbot text_classification data quality using plausible negative_examples ; abstract : we describe and validate a metric for estimating multi-class classifier performance based on cross-validation and adapted for improvement of small , unbalanced natural-language datasets used in chatbot design . our experiences draw upon building recruitment chatbots that mediate communication between job-seekers and recruiters by exposing the ml/nlp dataset to the recruiting team . evaluation approaches must be understandable to various stakeholders , and useful for improving chatbot performance . the metric , nex-cv , uses negative_examples in the evaluation of text_classification , and fulfils three requirements . first , it is actionable : it can be used by non-developer staff . second , it is not overly optimistic compared to human ratings , making it a fast method for comparing classifiers . third , it allows model-agnostic comparison , making it useful for comparing systems despite implementation differences . we validate the metric based on seven recruitment-domain datasets in english and german over the course of one year .
title : a c4.5_algorithm for english emotional classification ; abstract : the solutions for processing sentiment_analysis are very important and very helpful for many researchers , many applications , etc . this new model has been proposed in this paper , used in the english document-level sentiment_classification . in this research , we propose a new model using c4.5_algorithm of a decision_tree to classify semantics ( positive , negative , neutral ) for the english documents . our english training_data set has 140,000 english sentences , including 70,000 english positive sentences and 70,000 english negative sentences . we use the c4.5_algorithm on the 70,000 english positive sentences to generate a decision_tree and many association_rules of the positive polarity are created by the decision_tree . we also use the c4.5_algorithm on the 70,000 english negative sentences to generate a decision_tree and many association_rules of the negative_polarity are created by the decision_tree . classifying sentiments of one english document is identified based on the association_rules of the positive polarity and the negative_polarity . our english testing data set has 25,000 english documents , including 12,500 english positive reviews and 12,500 english negative reviews . we have tested our new model on our testing data set and we have achieved 60.3 % accuracy of sentiment_classification on this english testing data set .
title : fine-tuning arabic pre-trained_transformer models for egyptian-arabic dialect offensive_language and hate_speech detection and classification ; abstract : offensive_language and hate_speech are rampant on social_media platforms ( facebook , twitter , etc . ) in egypt for quite a while now , appearing in tweets , facebook posts and comments , etc. , it is an increasingly outreaching problem that needs immediate attention . this paper focuses on the problem of detecting and classifying both offensive_language and hate_speech using state-of-the-art techniques in text_classification . pre-trained_transformer models have gained a reputation of astounding general language underst and ing that could be fine-tuned for language-specific tasks like text_classification , we collected an egyptian-arabic dialect custom dataset of about 8,000 text samples manually_labelled into 5 distinct classes : ( neutral , offensive , sexism , religious_discrimination , racism ) , it was used to fine-tune and evaluate multiple different arabic pre-trained_transformer models based on different transformer architectures and pre-training approaches for the natural_language_processing downstream_task of text_classification . we achieved an average accuracy of about 96 % across all fine-tuned transformer models .
title : a category detection method for evidence-based_medicine ; abstract : evidence-based_medicine ( ebm ) gathers evidence by analyzing large databases of medical literatures and retrieving relevant clinical thematic texts . however , the abstracts of medical articles generally show the themes of clinical_practice , populations , research methods and experimental results of the thesis in an unstructurized manner , rendering inefficient retrieval of medical evidence . abstract sentences contain contextual_information , and there are complex semantic and grammatical correlations between them , making its classification different from that of independent sentences . this paper proposes a category detection algorithm based on hierarchical multi-connected network ( hmcn ) , regarding the category detection of ebm as a matter of classification of sequential sentences . the algorithm contains multiple structures : ( 1 ) the underlying layer produces a sentence vector by combining the pre-trained_language_model with bi-directional_long_short_term_memory network ( bi-lstm ) , and applies a multi-layered self-attention structure to the sentence vector so as to work out the internal dependencies of the sentences . ( 2 ) the upper layer uses the multi-connected bi-lstms model to directly read the original input sequence to add the contextual_information for the sentence vector in the abstract . ( 3 ) the top layer optimizes the tag sequence by means of the conditional_random_field ( crf ) model . the extensive_experiments on public datasets have demonstrated that the performance of the hmcn model in medical category detection is superior to that of the state-of-the-art text_classification method , and the f1 value has increased by 0.4 % –0.9 % .
title : a comparison of classification methods for predicting deception in computer-mediated_communication ; abstract : the increased chance of deception in computer-mediated_communication and the potential risk of taking action based on deceptive information calls for automatic detection of deception . to achieve the ultimate_goal of automatic prediction of deception , we selected four common classification methods and empirically compared their performance in predicting deception . the deception and truth data were collected during two experimental studies . the results suggest that all of the four methods were promising for predicting deception with cues to deception . among them , neural_networks exhibited consistent performance and were robust across test settings . the comparisons also highlighted the importance of selecting important input variables and removing noise in an attempt to enhance the performance of classification methods . the selected cues offer both methodological and theoretical contributions to the body of deception and information systems research . © 2004 m.e . sharpe , inc .
title : native language identification of fluent and advanced non-native writers ; abstract : native language identification ( nli ) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language . most existing_studies investigate this task for educational applications such as second_language_acquisition and require the learner corpora . this article performs nli in a challenging context of the user-generated-content ( ugc ) where authors are fluent and advanced non-native speakers of a second language . existing nli studies with ugc ( i ) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets , ( ii ) are unable to capture the variations of the language-usage-patterns within a text sample , and ( iii ) are not associated with any outlier handling mechanism . moreover , since there is a sizable number of people who have acquired non-english second languages due to the economic and immigration policies , there is a need to gauge the applicability of nli with ugc to other languages . unlike existing_solutions , we define a topic-independent feature_space , which makes our solution generalizable to other domains and datasets . based on our feature_space , we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample . specifically , we represent each text sample as a point set and identify the top-k stylistically similar text samples ( ssts ) from the corpus . we then apply the probabilistic k_nearest_neighbors ' classifier on the identified top-k ssts to predict the native languages of the authors . to conduct_experiments , we create three new corpora where each corpus is written in a different language , namely , english , french , and german . our experimental studies show that our solution outperforms competitive methods and reports more than 80 % accuracy across languages .
title : an identification method of news scientific intelligence based on tf-idf ; abstract : with the development of internet , the amount of information has been rapidly_growing which is spread widely . in order to improve the value and accuracy of science information that is pushed in this paper , an intelligence dichotomous method for science information categorization to identify science information from massive web news is presents . during the experiment , 85.3 % recognition_rate of the recognition non-tech news are realized and 82.9 % accuracy_rate , the results show that the method can effectively identify web science information news and reduce the amount of independent news .
title : arabic_text_categorization via binary particle_swarm_optimization and support_vector_machines ; abstract : document_categorization concerns automatically assigning a category label to a text document , and has increasingly many applications , particularly in the domains of organizing , browsing and search in large_document_collections . it is typically achieved via machine_learning , where a model is built on the basis of a ( typically ) large collection of document features . feature_selection is critical in this process , since there are typically several thousand potential features ( distinct words or terms ) . here we explore binary particle_swarm_optimization ( bpso ) hybridized with either k-nearest-neighbour ( knn ) or a support_vector_machine ( svm ) , for feature_selection in arabic document_categorization tasks . comparison between feature_selection methods is done on the basis of using the selected features , in conjunction with each of svm , c4.5 and naive_bayes , to classify a holdout test set . using publicly available datasets , we show that the bpsosvm approach seems promising in this domain . we also analyse the sets of selected features and consider the differences between the types of feature that bpsoknn and bpsosvm tend to choose ; this leads to speculations concerning the appropriate feature_selection strategy , based on the relationship between the classes in the document_categorization task at hand .
title : a novel approach for ontology-based dimensionality_reduction for web text document_classification ; abstract : dimensionality_reduction of feature_vector size plays a vital role in enhancing the text processing capabilities ; it aims in reducing the size of the feature_vector used in the mining tasks ( classification , clustering ... etc. ) . this paper proposes an efficient approach to be used in reducing the size of the feature_vector for web text document_classification process . this approach is based on using wordnet ontology , utilizing the benefit of its hierarchal structure , to eliminate words from the generated feature_vector that has no relation with any of wordnet lexical categories ; this leads to the reduction of the feature_vector size without losing information on the text . for mining tasks , the vector_space_model ( vsm ) is used to represent text documents and the term_frequency_inverse_document_frequency ( tfidf ) is used as a term_weighting_method . the proposed ontology based approach was evaluated against the principal_component_analysis ( pca ) approach using several experiments . the experimental results reveal the effectiveness of our proposed approach against other traditional_approaches to achieve a better classification_accuracy , f-measure , precision , and recall .
title : semantic indexing of 19th-century greek_literature using 21st-century linguistic resources ; abstract : manual classification of works of literature with genre/form concepts is a time-consuming task requiring domain_expertise . building automated systems based on language understanding can help humans to achieve this work faster and more consistently . towards this direction , we present a case_study on automatic classification of greek_literature books of the 19th century . the main challenges in this problem are the limited number of literature books and resources of that age and the quality of the source text . we propose an automated classification system based on the bidirectional_encoder_representations_from_transformers ( bert ) model trained on books from the 20th and 21st_century . we also dealt with bert ’ s constraint on the maximum sequence length of the input , leveraging the textrank_algorithm to construct representative sentences or phrases from each book . the results show that bert trained on recent literature books correctly classifies most of the books of the 19th century despite the disparity between the two collections . additionally , the textrank_algorithm improves the performance of bert .
title : sentiment_analysis of twitter data ; abstract : nowadays , people from all around the world use social_media sites to share information . twitter for example is a platform in which users send , read posts known as 'tweets ' and interact with different communities . users share their daily lives , post their opinions on everything such as brands and places . companies can benefit from this massive platform by collecting data related to opinions on them . the aim of this paper is to present a model that can perform sentiment_analysis of real data collected from twitter . data in twitter is highly unstructured which makes it difficult to analyze . however , our proposed model is different from prior work in this field because it combined the use of supervised and unsupervised machine_learning algorithms . the process of performing sentiment_analysis as follows : tweet extracted directly from twitter api , then cleaning and discovery of data performed . after that the data were fed into several models for the purpose of training . each tweet extracted classified based on its sentiment whether it is a positive , negative or neutral . data were collected on two subjects mcdonalds and kfc to show which restaurant has more popularity . different machine_learning algorithms were used . the result from these models were tested using various testing metrics like cross_validation and f-score . moreover , our model demonstrates strong performance on mining texts extracted directly from twitter .
title : sentiment_analysis on social_media data using intelligent techniques ; abstract : social_media gives a simple method of communication technology for people to share their opinion , attraction and feeling . the aim of the paper is to extract various sentiment behaviour and will be used to make a strategic decision and also aids to categorize sentiment and affections of people as clear , contradictory or neutral . the data was preprocessed with the help of noise_removal for removing the noise . the research work applied various techniques . after the noise_removal , the popular classification methods were applied to extract the sentiment . the data were classified with the help of multi-layer_perceptron ( mlp ) , convolutional_neural_networks ( cnn ) . these two classification results were checked against the others classified such as support_vector_machine ( svm ) , random_forest , decision_tree , naïve_bayes , etc. , based on the sentiment_classification from twitter data and consumer affairs website . the proposed work found that multi-layer_perceptron and convolutional_neural_networks performs better than another machine_learning classifier .
title : effective lstms for target-dependent sentiment_classification ; abstract : target-dependent sentiment_classification remains a challenge : modeling the semantic relatedness of a target with its context words in a sentence . different context words have different influences on determining the sentiment_polarity of a sentence towards the target . therefore , it is desirable to integrate the connections between target word and context words when building a learning system . in this paper , we develop two target dependent long short-term_memory ( lstm ) models , where target information is automatically taken into account . we evaluate our methods on a benchmark_dataset from twitter . empirical results show that modeling sentence_representation with standard lstm does not perform well . incorporating target information into lstm can significantly boost the classification_accuracy . the target-dependent lstm models achieve state-of-the-art performances without using syntactic parser or external sentiment_lexicons .
title : software categorization using low-level distributional features ; abstract : in recent_years , there has been a growing interest in applying deep_learning techniques for automatic generation of software . to achieve this ambitious objective , a number of smaller research goals need to be reached , one of which is automatic categorization of software , used in numerous tasks of software intelligence . we present here an approach to this problem using a set of low-level features derived from lexical_analysis of software code . we compare different feature_sets for categorizing software and also apply different supervised_machine_learning_algorithms to perform the classification_task . the representation allows us to identify the most relevant libraries used for each class , and we use the best-performing classifier to accomplish this . we evaluate our approach by applying it to categorize popular python projects from github .
title : hyperbolic interaction model for hierarchical multi-label_classification ; abstract : different from the traditional classification_tasks which assume mutual_exclusion of labels , hierarchical multi-label_classification ( hmlc ) aims to assign multiple_labels to every instance with the labels organized under hierarchical relations . besides the labels , since linguistic ontologies are intrinsic hierarchies , the conceptual relations between words can also form hierarchical_structures . thus it can be a challenge to learn mappings from word hierarchies to label hierarchies . we propose to model the word and label hierarchies by embedding them jointly in the hyperbolic_space . the main reason is that the tree-likeness of the hyperbolic_space matches the complexity of symbolic data with hierarchical_structures . a new hyperbolic interaction model ( hyperim ) is designed to learn the label-aware document_representations and make predictions for hmlc . extensive_experiments are conducted on three benchmark_datasets . the results have demonstrated that the new model can realistically capture the complex data_structures and further improve the performance for hmlc comparing with the state-of-the-art methods . to facilitate future_research , our code is publicly available .
title : a handwritten chinese_characters files text recognition method based on inception structure ; abstract : objectives : in order to solve the problem that the accuracy of handwritten chinese character text recognition is not high , an end⁃to⁃end method for handwritten chinese character text recognition based on convolutional_neural_network and recurrent_neural_network is proposed . methods : firstly , the convolutional_neural_network constructed by inception module is used to extract the basic features of the text image . secondly , the recurrent_neural_network is used to predict the extracted features and output a probability_distribution about the chinese_character set . finally , the connectionist temporal classification algorithm is used to calculate the recognition results and construct the loss_function . results : the proposed method is tested on the handwritten chinese character text dataset , experimental_result_shows that the inception module and data enhancement method can effectively_improve the performance of the algorithm , obtain the recognition_accuracy of 71.2 % and the text editing distance of 0.060 . conclusion : our proposed method can conduct end⁃to⁃end handwritten chinese character text recognition , and improve the recognition_accuracy compared with the existing_methods .
title : on applying linear_discriminant_analysis for multi-labeled problems ; abstract : linear_discriminant_analysis ( lda ) is one of the most popular dimension reduction methods , but it is originally focused on a single-labeled problem . in this paper , we derive the formulation for applying lda for a multi-labeled problem . we also propose a generalized lda algorithm which is effective in a high dimensional multi-labeled problem . experimental results demonstrate that by considering multi-labeled structure , lda can achieve computational_efficiency and also improve classification performances . © 2008 elsevier b.v. all rights_reserved .
title : primesrl-eval : a practical quality metric for semantic_role_labeling systems evaluation ; abstract : semantic_role_labeling ( srl ) identifies the predicate-argument_structure in a sentence . this task is usually accomplished in four steps : predicate identification , predicate sense_disambiguation , argument_identification , and argument_classification . errors introduced at one step propagate to later steps . unfortunately , the existing srl evaluation scripts do not consider the full effect of this error_propagation aspect . they either evaluate arguments independent of predicate sense ( conll09 ) or do not evaluate predicate sense at all ( conll05 ) , yielding an inaccurate srl model performance on the argument_classification task . in this paper , we address key practical issues with existing evaluation scripts and propose a more strict srl evaluation_metric primesrl . we observe that by employing primesrl , the quality_evaluation of all sota srl models drops significantly , and their relative rankings also change . we also show that primesrlsuccessfully penalizes actual failures in sota srl models .
title : a hybrid method for fake_news_detection using cosine_similarity scores ; abstract : in this work , we propose a novel hybrid method for fake_news_detection . two approaches have been used to assess the authenticity of the news using web-scrapped data . in the first approach the data is the pre-processed using nlp techniques like extraction of raw_text , the removal of special-characters , white-spaces , and stop_words . this is followed by lemmatization which groups words with similar_meanings . after lemmatization we apply , term_frequency_-_inverse_document_frequency ( tf-idf ) vectorization to form a corpus which is further used to train the models . we propose the use of cosine_similarity score , obtained after performing topic_modelling along with the corpus to improve the classification_accuracies . the classifiers are knn , decision_tree , naive_bayes , logistic_regression , passive-aggressive_classifier , and svm to determine the news is reliable or unreliable . more focus has been given to improve the classification_accuracies of the passive_aggressive_classifier which is the most widely used classifier in fake_news_detection . in the second approach , we use ensemble_learning technique called as stacking along with cosine_similarity score to train another model which gives the result as reliable or unreliable . it is observed that the second approach shows good improvement in the accuracy of fake_news_detection .
title : novel convolutional_neural_networks for efficient classification of rotated and scaled images ; abstract : this paper presents a novel method for improving the invariance of convolutional_neural_networks ( cnns ) to selected geometric transformations in order to obtain more efficient image classifiers . a common strategy employed to achieve this aim is to train the network using data augmentation . such a method alone , however , increases the complexity of the neural_network model , as any change in the rotation or size of the input image results in the activation of different cnn feature_maps . this problem can be resolved by the proposed novel convolutional_neural_network models with geometric transformations embedded into the network_architecture . the evaluation of the proposed cnn model is performed on the image_classification task with the use of diverse representative data sets . the cnn models with embedded geometric transformations are compared to those without the transformations , using different data augmentation setups . as the compared approaches use the same amount of memory to store the parameters , the improved classification score means that the proposed architecture is more optimal .
title : on textual_documents classification using fourier domain scoring ; abstract : recently , fourier and cosine discrete transformations have been proposed for textual document ranking . the advantage of the methods is that not only the count of a frequency term within documentis used ; the spatial_information about presence of the term is also considered . here , this novel approach is used to improve performance of classifiers . © 2006 ieee .
title : sentiment_analysis of chinese_micro-blog using semantic sentiment space model ; abstract : recently public , government , company and other forms of communities are extremely vocal about their opinions and perceptions on their appeals , policies and products on the micro-blog . sentiment_analysis on micro-blog data has attracted much attention for its application on opinion_polarity , classification , summarization and query . however , the related state-of-the-art approaches for sentiment_analysis are mainly focused on twitter data . they do n't work well with chinese_micro-blog for word_segmentation , feature word combination and feature_extraction problems . in this paper , we propose to improve chinese_micro-blog sentiment_analysis performance by 1 ) use sliding_window feature_combination detection and sentiment phrase dictionary combined method to address semantic recognition problems on metaphor , adversative , multiple negation and irony ; 2 ) propose ten chinese_micro-blog features for sentiment_analysis ; 3 ) find most impactful feature_combination for the sentiment classifier . experimental results show that our semantic sentiment space model is helpful to chinese_micro-blog sentiment_classification and our basic feature_combination outperforms the traditional classification algorithm td-idf and knn on standard measurement . © 2012 ieee .
title : speaker-invariant speech-to-intent_classification for low-resource_languages ; abstract : deep_neural_networks based speech embedding techniques have delivered significant results in speech_processing applications such as automatic_speech_recognition and spoken intent_detection systems . still , the presence of para-linguistic information such as speaker characteristics , accent , pronunciation , and emotional_expression cause performance_degradation in speech tasks where only the linguistic content is required . over time many techniques have been proposed to address this problem by disentangling the underlying para-linguistic content from the speech_signal . the most common approach is to incorporate speaker representations in speech_recognition systems . however , it has been less studied for speech intent identification and these speech_recognition models require large amounts of labeled_training_data . in the case of low_resource_languages , when only a limited amount of data is available , transfer_learning approach is adopted . in this paper , we present a speech-to-intent_classification model with i-vector based speaker normalization evaluated on sinhala , and tamil speech intent datasets . we explore the use of pre-trained acoustic models to address the problem of data scarcity . experimental results show that the proposed approach is effective in improving the performance of the speech intent_classification system .
title : exploring weakly_supervised latent sentiment explanations for aspect-level review analysis ; abstract : in sentiment_analysis , aspect-level review analysis has been an important task because it can catalogue , aggregate , or summarize various opinions according to a product 's properties . in this paper , we explore a new concept for aspect-level review analysis , latent sentiment explanations , which are defined as a set of informative aspect-specific sentences whose polarities are consistent with that of the review . in other words , sentiment explanations best represent a review in terms of both aspect and polarity . we formulate the problem as a structure learning problem , and sentiment explanations are modeled with latent_variables . training_samples are automatically identified through a set of pre-defined aspect signature terms ( i.e. , without manual_annotation on samples ) , which we term the way weakly_supervised . our major contributions lie in two folds : first , we formalize the use of aspect signature terms as weak_supervision in a structural learning framework , which remarkably promotes aspect-level analysis ; second , the performance of aspect analysis and document-level sentiment_classification are mutually enhanced through joint_modeling . the proposed method is evaluated on restaurant and hotel reviews respectively , and experimental results demonstrate promising performance in both document-level and aspect-level sentiment_analysis . copyright 2013 acm .
title : multi-modal multi-layered topic classification model for social_event analysis ; abstract : in this paper , we pay attention to reveal the event topics and track the evolutionary trend of social_event and a novel probabilistic_topic_model is proposed . the multi-modal multi-layered topic classification model ( tm_mmc ) for social_event analysis has the capacity for revealing visual and non-visual topics , by jointly_modeling the textual and visual information while simultaneously learning and predicting the multi-layered category_labels . in order to track the evolutionary trends of the topics online , tm_mmc uses topic intensity and heritability to incrementally build an up-to-date model . to evaluate the effectiveness of our model , we experiment using a collected data , and compare the results with those of other traditional models . the results demonstrate the effectiveness and advantages of our model against several state-of-the-art methods .
title : a collaborative_filtering-based approach to personalized document_clustering ; abstract : document_clustering is an intentional act that reflects individual preferences with regard to the semantic coherency and relevant categorization of documents . hence , effective document_clustering must consider individual preferences and needs to support personalization in document_categorization . most existing document-clustering techniques , generally anchoring in pure content-based analysis , generate a single set of clusters for all individuals without tailoring to individuals ' preferences and thus are unable to support personalization . the partial-clustering-based personalized document-clustering approach , incorporating a target individual 's partial clustering into the document-clustering process , has been proposed to facilitate personalized document_clustering . however , given a collection of documents to be clustered , the individual might have categorized only a small subset of the collection into his or her personal folders . in this case , the small partial clustering would degrade the effectiveness of the existing personalized document-clustering approach for this particular individual . in response , we extend this approach and propose the collaborative-filtering-based personalized document-clustering ( cfc ) technique that expands the size of an individual 's partial clustering by considering those of other users with similar categorization preferences . our empirical_evaluation results suggest that when given a small-sized partial clustering established by an individual , the proposed cfc technique generally achieves better clustering effectiveness for the individual than does the partial-clustering-based personalized document-clustering technique . © 2007 elsevier b.v. all rights_reserved .
title : mrpredt : using text_mining for metamorphic relation_prediction ; abstract : metamorphic relations ( mrs ) are an essential component of metamorphic testing ( mt ) that highly affects its fault detection effectiveness . mrs are usually identified with the help of a domain expert , which is a labor-intensive task . in this work , we explore the feasibility of a text_classification-based machine_learning approach to predict mrs using their program documentation as the sole input . we compare our method to our previously_developed graph kernelbased machine_learning approach and demonstrate that textual_features extracted from program documentation are highly effective for predicting metamorphic relations for matrix calculation programs .
title : prediction of iso 9001:2015 audit reports according to its major clauses using recurrent_neural_networks ; abstract : the quality_assurance department of the educational sectors is rapidly generating digital documents . the continuous_increase of digital documents may become a risk and challenge in the future . interpreting and analyzing those digital data in a short period of time is very critical and crucial for the top management to support their decisions . by this purpose , this paper explored the possibility of machine_learning and data_mining process to improve the quality_assurance management system process , specifically in the quality audit procedures and generation of management reports . the researchers developed a machine_learning model that predicts an audit report according to the major clauses of the iso 9001:2015 quality_management system ( qms ) requirements . the proposed data_mining process helps the top management to identify which principles of the iso 9001:2015 qms requirements they are lacking . the authors used four different recurrent_neural_networks ( rnns ) as a classifier ; ( 1 ) long short_term-memory ( lstm ) , ( 2 ) bidirectional-lstm , ( 3 ) deep-lstm and a ( 4 ) deep-bidirectional-lstm recurrent_neural_networks with a combine word_representation models ( word_encoding plus an embedding dimension layer ) . the deep-bidirectional-lstm outperformed the other three rnn models . where it achieved an average classification_accuracy of 91.10 % .
title : similarity-based techniques for text document_classification ; abstract : with large_scale text_classification labeling a large number of documents for training poses a considerable burden on human experts who need to read each document and assign it to appropriate categories . with this problem in mind , our goal was to develop a text_categorization system that uses fewer labeled_examples for training to achieve a given level of performance using a similarity-based learning algorithm and thresholding strategies . experimental results show that the proposed model is quite useful to build document_categorization systems . this has been designed for a small level implementation considering the size of the corpus being used . this can be enhanced for a larger data set and the efficiency can be proved against the performance of the presently available methods like svm , naive_bayes etc . this approach on the whole concentrates on categorizing small level documents and does the assigned task with completeness . © medwell journals , 2008 .
title : learning to classify utterances in a task-oriented_dialogue ; abstract :
title : aspect-based sentiment_analysis of user created game reviews ; abstract : there is an increase in the selling of games through an online market such as steam and with it the opinions of customers on the products . using python , this study developed a method that gathers reviews written on the steam website and classifies them to specific aspects such as audio , gameplay , and graphics and sub-categorizes them into positive , neutral , and negative_sentiments . the study used support_vector_machine ( svm ) classifiers for both the aspect classifier and the polarity classifiers . the aspect classifier which classifies any combination of the three aspects yielded the highest_accuracy when classifying gameplay with graphics at 97 % and the highest precision and recall when classifying gameplay with 86 % and 83 % respectively . while the polarity classifiers for audio , gameplay , and graphics yielded an accuracy of 91 % , 72 % , and 84 % respectively .
title : a two-stage biomedical event_trigger detection method based on hybrid neural_network and sentence_embeddings ; abstract : biomedical_event_extraction is a challenging task in biomedical_text_mining , which plays an important role in improving biomedical_research and disease prevention . as the crucial and prerequisite step in event_extraction , biomedical trigger_detection has attracted much attention . previous_approaches usually depended on feature_engineering with unbalanced_data . in this paper , we propose a two-stage method based on hybrid neural_network for trigger_detection , which divides trigger_detection into recognition stage and classification stage . in the first stage , we build a bilstm based recognition model integrating attention_mechanism ( att-bilstm ) . in the second stage , the classification model based on passive-aggressive online_algorithm is constructed . furthermore , to enrich sentence-level features , we establish sentence_embeddings and add reading gate . on the multi-level event_extraction ( mlee ) corpus test dataset , our method_achieves an f-score of 80.26 % , which achieves the state-of-the-art systems .
title : improving features extraction for supervised invoice classification ; abstract : an essential step in the understanding of printed_documents is the classification of such documents based on their class , i.e. , on the nature of information they contain and their layout . in this work we are concerned with automatic classification of such documents . this task is usually accomplished by extracting a suitable set of low-level features from each document which are then fed to a classifier . the quality of the results depends primarily on the classifier , but they are also heavily influenced by the specific features used . in this work we focus on the feature_extraction part and propose a method that characterizes each document based on the spatial density of black pixels and of image edges . we assess our proposal on a real-world dataset composed of 560 invoices belonging to 68 different classes . these documents have been digitalized after their printed counterparts have been handled by a corporate environment , thus they contain a substantial amount of noise - big stamps and handwritten signatures at unfortunate positions and so on . we show that our proposal is accurate , even a with very small learning set .
title : a study of fasttext word_embedding effects in document_classification in bangla_language ; abstract : natural_language_processing is the current topic due to many important tasks like document_classification , named_entity_recognition , opinion_mining , sentiment_analysis , textual_entailment , etc . such types of task in the bangla_language is also important . this research work endeavored to find out the word_embedding of the bengali_language . leveraging the fasttext word_embedding , it has shown significant performance in bangla document_classification without any prepossessing like lemmatization , stemming , and others . for the extrinsic_evaluation of our word_vectors , a classification problem-solving strategy has been used which showed an outstanding result . in the classification module , attempts have been made to classify 40 thousand news samples into 12 categories . for this purpose , three deep_learning techniques have been used : convolutional_neural_network ( cnn ) , bi-directional_lstm ( blstm ) and convolutional bi-directional_lstm ( cblstm ) alongside fasttext . from the analogous study of all the parameters of every classifier implemented here , we found that the blstm technique is the most promising technique for this task . this technique achieved 91.49 % , 87.87 % , and 85.5 % accuracies for training , testing , and validation_set , respectively .
title : extended category learning with spiking nets and spike timing dependent plasticity ; abstract : neuroscience makes use of models of neurons , synapases , and learning rules that modify the efficiency of synapses in stimulating neurons . these models can be used to simulate spiking neural_networks , and the standard learning rule is based on the timing of the spikes of the pre and post-synaptic neurons . this paper describes the use of these models to categorise documents by translating this spike timing dependent plasticity into an unsupervised_learning rule by representing documents and categories in neurons and presenting them in specific fashion for learning and categorisation . the resulting system is comparable to other unsupervised machine_learning systems . this presentation mechanism is extended to combine input feature value pairs to resolve the exclusive or problem . it is further refined to approximate co-variance of features to an arbitrary degree of precision .
title : security-level classification for confidential documents by using adaptive_neuro-fuzzy inference systems ; abstract : the security-level detection of a confidential document is a vital task for organizations to protect their confidential_information . diverse classification rules and techniques are being applied by human experts . increasing number of confidential_information in organizations is making difficult to classify all the documents carefully with human effort . the recommended frameworks in this study classify the internal documents of tubitak uekae ( national research institute of electronics and cryptology of turkey ) by using classification_algorithms naïve_bayes , support_vector_machines ( svms ) and adaptive_neuro-fuzzy inference systems ( anfiss ) . a hybrid approach involving support_vector classifiers and adaptive_neuro-fuzzy classifiers exposes the most successful accuracy_rates of expert system classification . this study also states preprocessing_tasks required for document_classification with natural_language_processing . to represent term-document relations , a recommended metric tf-idf was chosen to construct a weight_matrix . agglutinative nature of turkish documents is handled by turkish stemming algorithms . at the end of the article , some experimental results and success metrics are projected with accuracy_rates and receiver operating characteristic ( roc ) curves . © 2012 wiley publishing ltd .
title : opinionated_text classification for hindi tweets using deep_learning ; abstract : the recent_years have witnessed a significant_growth in the data collected from the reviews_posted on various websites . reviews are a direct way of getting the response of the customers and clients of any business , making it a convenient way for getting feedback for marketing , performance and other such characteristics in association with any product or service . the opinions mined from these collections of data can provide strategies to improve the sales based on how well a product is received . this is done in two steps , first being the subjectivity detection followed by sentiment_analysis . for this process , various methods have been already introduced in this field . these vary from svms , naive-bayesian , deep_learning etc . since , english is the most commonly used language in the world , it is not surprising that most work done in this field focuses on the same . but it is already known that there are roughly around 6500 languages used around the world . india alone has 447 languages which ranks it fourth on the list of countries with the greatest number of languages . the proposed research work focuses on sentiment_classification in hindi_language text . the proposed research work has attempted to experiment with a method that does not rely on availability language dictionaries . this is done by creating a completely numerical data corresponding to the text . the model proposed in this paper will use a combination of recurrent_neural_network and convolutional_neural_network model to extract the subjective data form the given dataset of movie reviews .
title : hsd shared_task in vlsp campaign 2019 : hate_speech detection for social good ; abstract : the paper describes the organisation of the `` hatespeech detection '' ( hsd ) task at the vlsp workshop 2019 on detecting the fine-grained presence of hate_speech in vietnamese textual items ( i.e. , messages ) extracted from facebook , which is the most popular social_network site ( sns ) in vietnam . the task is organised as a multi-class classification_task and based on a large-scale dataset containing 25,431 vietnamese textual items from facebook . the task participants were challenged to build a classification model that is capable of classifying an item to one of 3 classes , i.e. , `` hate '' , `` offensive '' and `` clean '' . hsd attracted a large number of participants and was a popular task at vlsp 2019 . in particular , there were 71 teams signed up for the task , 14 of them submitted results with 380 valid submissions from 20th september 2019 to 4th october 2019 .
title : a post-processing method for detecting unknown intent of dialogue system via pre-trained deep_neural_network classifier ; abstract : with the maturity and popularity of dialogue_systems , detecting user 's unknown intent in dialogue_systems has become an important task . it is also one of the most challenging tasks since we can hardly get examples , prior_knowledge or the exact numbers of unknown intents . in this paper , we propose softermax and deep novelty_detection ( smdn ) , a simple yet effective post-processing method for detecting unknown intent in dialogue_systems based on pre-trained deep_neural_network classifiers . our method can be flexibly applied on top of any classifiers trained in deep_neural_networks without changing the model architecture . we calibrate the confidence of the softmax outputs to compute the calibrated confidence_score ( i.e. , softermax ) and use it to calculate the decision_boundary for unknown intent_detection . furthermore , we feed the feature_representations learned by the deep_neural_networks into traditional novelty_detection algorithm to detect unknown intents from different perspectives . finally , we combine the methods above to perform the joint prediction . our method classifies examples that differ from known intents as unknown and does not require any examples or prior_knowledge of it . we have conducted extensive_experiments on three benchmark dialogue datasets . the results show that our method can yield significant_improvements compared with the state-of-the-art baselines
title : comparative_analysis of binary classifiers on an array of scientific_publications ; abstract : binary classifiers are studies on balanced text samples . the samplings are formed from scientific_publications in the field of computer science ( computer science ) . the first class contains articles on “ text data_mining ” ( the “ tdm ” class ) , the second one contains works on other topics of computer science ( the “ non-tdm ” class ) . all the main_stages of preliminary processing of text documents are considered , models of their presentation are analyzed . the problem of binary classification is formulated and the quality indicators used in the study are given . a method of sampling from the russian digital_library ( elibrary ) is proposed . the generated sampling consists of bibliographic descriptions of documents ( title , abstract and keywords ) . an exploratory_analysis was carried out and the sampling structure was studied . “ term clouds ” for two classes are constructed and analyzed , documents are visualized using the method of stochastic embedding of neighbors with t-distribution ( t-sne ) . based on the review and analysis of known classifiers , the following methods were selected for the study : the k-nearest_neighbor method , random_forest , gradient boosting , logistic_regression , and the support_vector method . profile methods based on the construction of a vector ( profile ) of the most informative terms determined by the frequency of occurrence of terms and classes are also used in the study . the parameters of the methods were configured using a five-fold_cross-validation . the best quality of classification in our sampling demonstrated the methods using the ensemble ( collective ) decision-making principle ( random_forest , gradient boosting ) , as well as the support_vector method . the best classifier , gradient boosting , had the proportion of correct answers ( accuracy ) about 0.98 , recall and precision about 0.99 . the other ( simpler ) methods used in the study also generally showed rather good quality of classification ( for the least accurate k-nearest_neighbor method accuracy , recall and precision were 0.90 , 0.81 , and 0.91 , respectively ) .
title : genetic optimization of big_data sentiment_analysis ; abstract : this paper deals with opinion_mining from unstructured textual_documents . the proposed method focuses on approach with minimum preliminary requirements about the knowledge of the analysed language and thus it can be deployed to any language . the proposed method builds on artificial_intelligence , which consists of support_vector_machines classifier , big_data analysis and genetic_algorithm optimization . to make the optimization feasible together with big_data approach we have proposed ga operators , which significantly accelerate conversion to the accurate solutions . in this work we outperformed the traditional_approaches ( which use language dependent text_preprocessing ) for text valence classification with the highest achieved accuracy 90.09 % . the data set for validation was czech texts .
title : multi-scale self-attention for text_classification ; abstract : in this paper , we introduce the prior_knowledge , multi-scale structure , into self-attention modules . we propose a multi-scale transformer which uses multi-scale multi-head_self-attention to capture features from different scales . based on the linguistic perspective and the analysis of pre-trained_transformer ( bert ) on a huge corpus , we further design a strategy to control the scale distribution for each layer . results of three different kinds of tasks ( 21 datasets ) show our multi-scale transformer outperforms the standard transformer consistently and significantly on small and moderate size datasets .
title : on the use of emojis to train emotion classifiers ; abstract : nowadays , the automatic detection of emotions is employed by many applications in different fields like security informatics , e-learning , humor detection , targeted_advertising , etc . many of these applications focus on social_media and treat this problem as a classification problem , which requires preparing training_data . the typical method for annotating the training_data by human experts is considered time consuming , labor intensive and sometimes prone to error . moreover , such an approach is not easily extensible to new domains/languages since such extensions require annotating new training_data . in this study , we propose a distant_supervised learning approach where the training sentences are automatically annotated based on the emojis they have . such training_data would be very cheap to produce compared with the manually_created training_data , thus , much larger training_data can be easily obtained . on the other hand , this training_data would naturally have lower quality as it may contain some errors in the annotation . nonetheless , we experimentally show that training classifiers on cheap , large and possibly erroneous data annotated using this approach leads to more accurate results compared with training the same classifiers on the more expensive , much smaller and error-free manually_annotated training_data . our experiments are conducted on an in-house dataset of emotional arabic tweets and the classifiers we consider are : support_vector_machine ( svm ) , multinomial_naive_bayes ( mnb ) and random_forest ( rf ) . in addition to experimenting with single classifiers , we also consider using an ensemble of classifiers . the results show that using an automatically annotated training_data ( that is only one order of magnitude larger than the manually_annotated one ) gives better results in almost all settings considered .
title : noise audits improve moral foundation classification ; abstract : morality plays an important role in culture , identity , and emotion . recent advances in natural_language_processing have shown that it is possible to classify moral values expressed in text at scale . morality classification relies on human annotators to label the moral expressions in text , which provides training_data to achieve state-of-the-art performance . however , these annotations are inherently subjective and some of the instances are hard to classify , resulting in noisy annotations due to error or lack of agreement . the presence of noise in training_data harms the classifier 's ability to accurately recognize moral foundations from text . we propose two metrics to audit the noise of annotations . the first metric is entropy of instance labels , which is a proxy measure of annotator disagreement about how the instance should be labeled . the second metric is the silhouette coefficient of a label assigned by an annotator to an instance . this metric leverages the idea that instances with the same label should have similar latent representations , and deviations from collective judgments are indicative of errors . our experiments on three widely used moral foundations datasets show that removing noisy annotations based on the proposed metrics improves classification performance .
title : keyword_extraction using graph based supervised_term_weighting ; abstract : text_classification of massive textual_data has become an integral part and necessity of most computer applications . eventually , term_weighting becomes an important aspect as it directly_affects the classification quality of document_classification . the challenge of effective term_weighting followed by the deficiency of traditional_methods such as term_frequency-inverse_document_frequency ( tf-idf ) method has encouraged numerous researchers to propose different algorithms . term_frequency-inverse gravity moment ( tf-igm ) in vector_space_model is one such example which precisely measure the class distinguishing power of terms . in graph based model textrank has become quite popular for term_weighting . however , no algorithm has been proposed yet in graph based model which directly influences the class distinguishing power of a term . hence , we propose a novel graph based supervised_term_weighting ( gbstw ) model which modifies the textrank_algorithm to incorporate class_label information . the model has been validated using three mobile review data sets and has been compared with the pre-existing modified textrank model . results show that gbstw model outperforms the pre-existing modified textrank_algorithm in terms of precision , recall and f-measure .
title : quality_estimation of english-hindi machine_translation systems ; abstract : quality_estimation is a new research area in natural_language_processing where machine_learning techniques are used to estimate the quality of machine_translation outputs . in this paper we have discussed our experience of performing quality_estimation for english-hindi_language pair . we have shown the use of some seventeen language independent features and then added some linguistic features to the feature_set and analyzed the performance of the system by training two different classifiers on distinct sets . the results of these classifiers are compared with the results of human evaluators . moreover we have also compared the results of these classifiers with some of the popular evaluation_metrics .
title : eliciting semantic types of legal norms in korean legislation with deep_learning ; abstract : automating information_extraction from legal_documents and formalising them into a machine_understandable format has long been an integral challenge to legal reasoning . most approaches in the past consist of highly complex solutions that use annotated syntactic structures and grammar to distil rules . the current_research trend is to utilise state-of-the-art natural_language_processing ( nlp ) approaches to automate these tasks , with minimum human interference . in this paper , based on its functional features , we propose a taxonomy of semantic type in korean legislation , such as obligations , rights , permissions , penalties , etc . based on this , we performed automatic classification of legal norms with a rule-based classifier using a manually_labelled dataset formed by three korean acts , i.e. , insurance business act , banking act and financial holding_companies act , of the korean legislation ( n= 1237 ) and a performance of f1= 0.97 was reached . in contrast , several supervised_machine_learning based classifiers were implemented and a performance of f-measure = 0.99 was achieved .
title : online biterm_topic_model based short text stream classification using short text expansion and concept drifting detection ; abstract : short text stream classification suffers from enormous challenges , due to the sparsity , high dimension and rapid variability of the short text stream . in this paper , we present a short text stream classification approach refined from online biterm_topic_model ( btm ) using short text expansion and concept drifting detection . specifically , in our method , we firstly extend short_text streams from an external resource to make up for the sparsity of data , and use online btm to select representative topics instead of the word_vector to represent the feature of short texts . secondly , we propose a concept_drift detection method based on the topic_model to detect the hidden concept_drifts in short_text streams . thirdly , we build an ensemble model using several data chunks and update with the newest data chunk and results of the concept_drift detection . finally , extensive experimental results demonstrate that compared to well-known baselines , our approach achieves a better performance in the classification and concept drifting detection .
title : dimensionality_reduction with category information fusion and non-negative_matrix_factorization for text_categorization ; abstract : dimensionality_reduction can efficiently improve computing performance of classifiers in text_categorization , and non-negative_matrix_factorization could map the high dimensional term space into a low_dimensional semantic subspace easily . meanwhile , the non-negative of the basis vectors could provide a meaningful explanation for the semantic subspace . however , it usually could not achieve a satisfied classification performance because it is sensitive to the noise , data missing and outlier as a linear reconstruction method . this paper proposes a novel approach in which the train text and its category information are fused and a transformation_matrix that maps the term space into a semantic subspace is obtained by a basis orthogonality non-negative_matrix_factorization and truncation . finally , the dimensionality can be reduced aggressively with these transformations . experimental results show that the proposed approach remains a good classification performance in a very low_dimensional case . ©_2011_springer-verlag .
title : improve vsm text_classification by title vector based document_representation method ; abstract : text_classification is a daunting task because it is difficult to extract the semantics of natural_language texts . many problems must be resolved before natural-language_processing techniques can be effectively applied to a large collection of texts . a significant one is to extract semantic information from corpus in plan text . in vector_space_model , a document is conceptually represented by a vector of terms extracted from each document , with associated weights representing the importance of each term in the document and within the whole document_collection . likewise , an unclassified document is also modeled as a list of terms with associated weights representing the importance of the terms in it . many techniques introduces much statistical information of terms to represent their semantic information . however , as always , document title is not taken into special consideration , while it obviously contains much semantic information . this paper proposes title vector to address this issue . © 2011 ieee .
title : a big_data approach to sentiment_analysis using greedy feature_selection with cat swarm_optimization-based long short-term_memory neural_networks ; abstract : sentiment_analysis is crucial in various systems such as opinion_mining and predicting . considerable research has been done to analyze sentiment using various machine_learning techniques . however , the high error_rates in these studies can reduce the entire system ’ s efficiency . we introduce a novel big_data and machine_learning technique for evaluating sentiment_analysis processes to overcome this problem . the data are collected from a huge volume of datasets , helpful in the effective analysis of systems . the noise in the data is eliminated using a preprocessing data_mining concept . from the cleaned sentiment data , effective features are selected using a greedy approach that selects optimal features processed by an optimal classifier called cat swarm_optimization-based long short-term_memory neural_network ( cso-lstmnn ) . the classifiers analyze sentiment-related features according to cat behavior , minimizing error_rate while examining features . this technique helps improve system efficiency , analyzed using experimental results of error_rate , precision , recall , and accuracy . the results obtained by implementing the greedy feature and cso-lstmnn algorithm and the particle_swarm_optimization ( pso ) algorithm are compared ; cso-lstmnn outperforms pso in terms of increasing accuracy and decreasing error_rate .
title : an ensemble model for stance_detection in social_media texts ; abstract : the aim of this paper is to develop a model to classify the stance expressed in social_media texts . more specifically , the work presented focuses on tweets . in stance_detection ( sd ) tasks , the objective is to identify the stance of a person towards a target of interest . in this paper , a model for sd is established and its variations are evaluated using different classifiers . the single models differ based on the pre-processing and the combination of features . to reduce the dimensionality of the feature_space , analysis of variance ( anova ) test is used . then , two classifiers are employed as base_learners including random_forests ( rf ) and support_vector_machines ( svm ) . experimental analyses are conducted on semeval dataset that is used as a benchmark for sd . finally , the base_learners that resulted from different design alternatives , are combined into three ensemble_models . experimental results show the significance of the used features and the effectiveness of a manually built dictionary that is used in the pre-processing stage . moreover , the proposed ensembles outperform the state-of-the-art models in the overall test score , which suggests that ensemble_learning is the best tool for effective sd in tweets .
title : several alternative term_weighting methods for text_representation and classification ; abstract : text_representation is one kind of hot_topics which support text_classification ( tc ) tasks . it has a substantial impact on the performance of tc . although the most famous tf–idf is specially designed for information_retrieval rather than tc tasks , it is highly useful in the field of tc as a term_weighting_method to represent text contents . inspired by the idf part of tf–idf which is defined as the logarithmic transformation , we proposed several alternative methods in this study to generate unsupervised term_weighting_schemes that can offset the drawback confronting tf–idf.​ moreover , owing to tc tasks are different from information_retrieval , representing test texts as a vector in an appropriate way is also essential for tc tasks , especially for supervised_term_weighting approaches ( e.g. , tf–rf ) , mainly due to these methods need to use category information when weighting the terms . but most of current schemes do not clearly explain how to represent test texts with their schemes . to explore this problem and seek a reasonable solution to these schemes , we analyzed a classic unsupervised term_weighting_method and three typical supervised_term_weighting methods in depth to illustrate how to represent test texts . to investigate the effectiveness of our work , three sets of experiments are designed to compare their performance . comparisons show that our proposed methods can indeed enhance the performance of tc , and sometimes even outperform existing supervised_term_weighting methods .
title : automatic classification of government texts based on improved cnn and skip-gram models ; abstract : aiming at the characteristics of government texts with rich_semantic information , the impact of convolutional_neural_network models on the classification and recognition of government text features is studied , and an optimization method to assist in the classification of official government information resources is proposed so that government staff can better investigate public opinion . based on machine_learning and deep_learning theories , this paper conducts an experimental study on automatic classification and recognition of texts using convolutional_neural_network models and conducts a comprehensive comparative_analysis from the perspective of information details and core topic sentences . after data cleaning , word2vec is used to construct a word_vector matrix , and this information is integrated into the convolutional_neural_network model for comparison and optimization . and based on the use of a convolutional_neural_network based on the improved convolutional_layer technology , combined with a skip-gram word_vector model , the text is converted into low-latitude feature_vectors as the input of the improved convolutional_neural_network model to achieve the automatic classification of government texts . the experimental results show that the model of this study achieves 92.7 % fl mean and 93.4 % acc in the evaluation of automatic classification of government texts , which not only solves the problem of excessive dimensionality but also significantly_improves the execution performance and classification_accuracy compared with the traditional_methods , and effectively solves the automatic classification of information contents and labels .
title : efficient categorization of web_documents using enhanced self-organizing_map ; abstract : the growth of internet has made clear that efficient methods for searching and exploring vast amount of data are essential . document_categorization has become important for handling large amount of documents available on world_wide_web . however , the challenges associated with categorization of documents are semantics and high dimensionality . this paper proposes a method latent_semantic_indexing of 1d-som for web document_categorization . latent_semantic_indexing is a successful technology in information_retrieval which attempts to explore the latent_semantics implied by a query or a document through representing them in a dimension-reduced space . som is used for both clustering and projection . it projects onto a 2d-grid . various methods were developed for the automatic clustering of worldwide web_documents according to the user_requirements . the objective of this paper is to reduce the time and effort the user has to spend to find the information sought after . the method 1d-som provides topographical visualization of categorized topics . the proposed method is tested on documents of different topics collected from different websites . categorization of documents is obtained from the som clustering of the latent_semantic_indexing representation of document terms . the proposed method which uses 1d-som with lsi is found to be efficient in terms of computational time , visualization , and obtaining meaningful related topics . it can be easily_adapted for large data set . the execution time and number of clusters are validated . it shows that proposed method lsi-1dsom provides better results as compared with the methods k-means , 2dsom , backpropagation , lsi-2dsom , 1dsom . © eurojournals_publishing , inc. 2012 .
title : polyglot prompt : multilingual multitask promptraining ; abstract : this paper aims for a potential architectural improvement for multilingual learning and asks : can different tasks from different languages be modeled in a monolithic framework , i.e . without any task/language-specific module ? the benefit of achieving this could open new doors for future multilingual research , including allowing systems trained on low resources to be further assisted by other languages as well as other tasks . we approach this goal by developing a learning framework_named polyglot prompting to exploit prompting methods for learning a unified semantic space for different languages and tasks with multilingual prompt engineering . we performed a comprehensive evaluation of 6 tasks , namely topic classification , sentiment_classification , named_entity_recognition , question_answering , natural_language inference , and summarization , covering 24 datasets and 49 languages . the experimental results demonstrated the efficacy of multilingual multitask prompt-based learning and led to inspiring observations . we also present an interpretable multilingual evaluation methodology and show how the proposed framework , multilingual multitask prompt training , works . we release all datasets prompted in the best setting and code .
title : on the influence of machine_translation on language origin obfuscation ; abstract : in the last decade , machine_translation has become a popular means to deal with multilingual digital content . by providing higher_quality translations , obfuscating the source_language of a text becomes more attractive . in this paper , we analyze the ability to detect the source_language from the translated output of two widely used commercial machine_translation systems by utilizing machine-learning algorithms with basic textual_features like n-grams . evaluations show that the source_language can be reconstructed with high accuracy for documents that contain a sufficient amount of translated text . in addition , we analyze how the document size influences the performance of the prediction , as well as how limiting the set of possible source languages improves the classification_accuracy .
title : infosuggest : a system for automated information gathering : with a real-world case_study ; abstract : departments of many organizations treat the world_wide_web as an important information source . they have a need to keep themselves up-to-date with current information in their domain . such information gathering is a time consuming process due to overload of available information and there are dedicated teams in many organizations for this task . in this paper , we present info suggest , a system for end-to-end information gathering from the web . info suggest improves efficiency of such focused information gathering process with the use of machine_learning . we employ a semi-supervised document_classification method called transductive support_vector_machines ( tsvms ) for learning user_preferences based on example articles provided by them . we also devise a strategy for unlabeled_data selection tsvm-meta that is applicable for an information gathering setting . in the paper , we discuss the system architecture and also present a case_study for information gathering for food_safety in an environmental_health department of a government_agency . we conduct_experiments and demonstrate that our system results in improving the efficiency by as much as 35 % by making it easier to find relevant content . © 2014 ieee .
title : deep bi-directional_long_short-term_memory neural_networks for sentiment_analysis of social data ; abstract : sentiment_analysis ( sa ) has been attracting a lot of studies in the field of natural_language_processing and text_mining . recently , there are many algorithm ’ s enhancements in various sa applications are investigated and introduced . deep_convolutional_neural_networks ( dcnns ) have recently been shown to give the state-of-the-art performance on sentiment_classification of social data . although , these solutions effectively address issues of multi-levels features presentation but having some limitations of temporal modeling . in addition , the bidirectional_long_short-term_memory ( bltsm ) conventional models have encountered some limitations in presentation with multi-level features but can keep track of the temporal_information while enabling deep representations in the data . in this paper , we propose to use deep bi-directional_long_short-term_memory ( dblstm ) architecture with multi-levels feature presentation for sentiment_polarity classification ( spc ) on social data . by using dblstm , we can exploit more level features than bltsm and inherit temporal modeling in bltsm . moreover , the language of social data is very informal with misspellings and abbreviations . one word can be appeared in multiple formalities , which is a challenge in word-level models . we use character-level as input of dblstm neural_network ( called character dblstm - cdblstm ) for learning sentence_level presentation . the experimental results show that the performance of our model is competitive with state-of-the-art of spc on twitter ’ s data . our model achieves 85.86 % accuracy on stanford twitter sentiment corpus ( sts ) and 84.82 % accuracy on the subtasks b of semeval-2016 task 4 corpus .
title : common terms for rare epilepsies : synonyms , associated terms , and links to structured vocabularies ; abstract : identifying individuals with rare epilepsy syndromes in electronic data sources is difficult , in part because of missing codes in the international_classification_of_diseases ( icd ) system . our objectives were the following : ( 1 ) to describe the representation of rare epilepsies in other medical vocabularies , to identify gaps ; and ( 2 ) to compile synonyms and associated terms for rare epilepsies , to facilitate text and natural_language_processing tools for cohort identification and population-based surveillance . we describe the representation of 33 epilepsies in 3 vocabularies : orphanet , snomed-ct , and umls-metathesaurus . we compiled terms via 2 surveys , correspondence with parent advocates , and review of web_resources and standard vocabularies . umls-metathesaurus had entries for all 33 epilepsies , orphanet 32 , and snomed-ct 25 . the vocabularies had redundancies and missing phenotypes . emerging epilepsies ( scn2a- , scn8a- , kcnq2- , slc13a5- , and syngap-related epilepsies ) were underrepresented . survey and correspondence respondents included 160 providers , 375 caregivers , and 11 advocacy_group leaders . each epilepsy syndrome had a median of 15 ( range 6–28 ) synonyms . nineteen had associated terms , with a median of 4 ( range 1–41 ) . we conclude that medical vocabularies should fill gaps in representation of rare epilepsies to improve their value for epilepsy research . we encourage epilepsy researchers to use this resource to develop tools to identify individuals with rare epilepsies in electronic data sources .
title : multi-grained attention representation with albert for aspect-level_sentiment_classification ; abstract : aspect-level_sentiment_classification aims to solve the problem , which is to judge the sentiment_tendency of each aspect in a sentence with multiple_aspects . previous_works mainly employed long short-term_memory ( lstm ) and attention_mechanisms to fuse information between aspects and sentences , or to improve large language models such as bert to adapt aspect-level_sentiment_classification tasks . the former methods either did not integrate the interactive information of related aspects and sentences , or ignored the feature_extraction of sentences . this paper proposes a novel multi-grained attention representation with albert ( mgar-albert ) . it can learn the representation that contains the relevant_information of the sentence and the aspect , while integrating it into the process of sentence modeling with multi_granularity , and finally get a comprehensive sentence_representation . in masked lm ( mlm ) task , in order to avoid the influence of aspect words being masked in the initial stage of the pre-training , the noise linear cosine decay is introduced into n-gram . we implemented a series of comparative_experiments to verify the effectiveness of the method . the experimental results show that our model can achieve excellent_results on restaurant dataset with numerous number of parameters reduced , and it is not inferior to other models on laptop dataset .
title : sentiment_analysis on twitter data based on spider_monkey optimization and deep_learning for future prediction of the brands ; abstract : in this manuscript , a deep_neural_network is proposed by integrating improved adaptive-network-based fuzzy inference system ( ianfis ) for branding online products to overcome these issues . here , the sentiment_analysis ( sa ) and prediction on future branding of products that are extracted from the twitter data is carried out . after the review process classifying the products as positive , negative , and neutral assessments completely concentrated in three folds , prediction of a future brand that is carried out by ianfis for weighting the products finally classify them . this scheme helps the respective retailers/retail brands with their digital_marketing team to understand their brand perception as opposed to others . the performance of the proposed method is compared with the existing_methods , such as sentiment_analysis on twitter data based on particle_swarm_optimization and genetic_algorithm , sentiment_analysis on twitter data based on particle_swarm_optimization and convolutional_neural_network , sentiment_analysis on twitter data based on whale_optimization_algorithm and support_vector_machine , and sentiment_analysis on twitter data based on convolutional_neural_network and long short_term_memory . the simulation results show that the proposed method_outperforms the state of art methods .
title : determining the fitness of a document model by using conflict instances ; abstract : documents can not be automatically classified unless they have been represented as a collection of computable features . a model is a representation of a document with computable features . however , a model may not be sufficient to express a document , especially when two documents have the same features , they might not be necessarily classified into the same category . we propose a method for determining the fitness of a document model by using conflict instances . conflict instances are instances with exactly same features , but with different category_labels given by human expert in an interactive document labelling process for training of the classifier . in our paper , we do not treat conflict instances as noises , but as the evidences that can reveal a distribution of positive_instances . we develop an approach to the representation of this distribution information as a hy-perplane , namely distribution hyperplane . then the fitness problem becomes a problem of computing the distribution hyperplane . besides determining the fitness of a model , distribution hyperplane can also be used for : 1 ) acting as classifier itself ; and 2 ) being a membership_function of fuzzy_sets . in this paper , we also propose the selection_criteria of effectiveness measuring for a model in a process of fitness computations . © 2005 , australian computer society , inc .
title : enabling digital health by automatic classification of short messages ; abstract : in response to the growing hiv/aids and other health-related issues , unicef through their u-report platform receives thousands of messages ( sms ) every day to pro-vide prevention strategies , health case advice , and counsel-ing support to vulnerable population . due to a rapid in-crease in u-report usage ( up to 300 % in last 3 years ) , plus approximately 1,000 new registrations each day , the volume of messages has thus continued to increase , which made it impossible for the team at unicef to process them in a timely manner . in this paper , we present a platform de-signed to perform automatic classification of short messages ( sms ) in real-time to help unicef categorize and prior-itize health-related messages as they arrive . we employ a hybrid approach , which combines human and machine intel-ligence that seeks to resolve the information_overload issue by introducing processing of large-scale data at high-speed while maintaining a high classification_accuracy . the sys-tem has recently been tested in conjunction with unicef in zambia to classify short messages received via the u-report platform on various health related issues . the system is designed to enable unicef make sense of a large_volume of short messages in a timely manner . in terms of evalua-tion , we report design choices , challenges , and performance of the system observed during the deployment to validate its effectiveness .
title : intelligent document analysis in the insurance market ; abstract : we address the problem of classifying documents into categories for the insurance market . one of the most arduous tasks in any business is to sort through piles of documents . often this is done using a large amount of the time of employees the most basic , and time consuming task is the visual inspection of each document in order to discover its type . our work aimed to address the problem of examining each document submitted by a customer to discover whether it is the one that was requested . we addressed this problem with supervised_machine_learning methods . in particular we have followed two approaches , in the first approach the documents are handled as images and we use convolutional_neural_networks ( cnn ) . in the second approach we extract the text from images , and then we apply classification the experimental evaluation has been performed on documents that were provided by the generali insurance_company , and they were subsequently annotated the documents were in various formats ( pdfs , jpgs , tiffs , etc . ) , and of varying quality regarding the illumination , rotation etc. , while containing a combination of greek and english text .
title : big social data and customer decision_making in vegetarian restaurants : a combined machine_learning method ; abstract : customers increasingly use various social_media to share their opinion about restaurants service_quality . big_data collected from social_media provides a data platform to improve the service_quality of restaurants through customers ' online_reviews , where online_reviews are a trustworthy and reliable source that helps consumers to evaluate food_quality . developing methods for effective evaluation of customer-generated reviews of restaurant services is important . this study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants . the method is developed through text_mining ( latent_dirichlet_allocation ) , cluster_analysis ( self_organizing_map ) and predictive learning technique ( classification and regression trees ) to reveal the customer ’ satisfaction levels from the service_quality in vegetarian friendly restaurants . based on the obtained results of our experiments on the data vegetarian friendly restaurants in bangkok , the models constructed by classification and regression trees were able to give an accurate prediction of customers ' preferences on the basis of restaurants ' quality factors . the results showed that customers ’ online_reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service_quality improvements .
title : application of the deep pretrained_language_model processing method in social_network sentiment_analysis ; abstract : in social_network , users can manage their social_network and social identity , publish information on various topics , and obtain information published by other users through friend relationship . the resulting large amount of text data attract more and more scholars to study it . text sentiment_analysis has become a hot spot in social_network data analysis and has important application value in academic field , social field , and business field . based on the idea of pre-training , this paper improves the random word masking algorithm of deep pretraining task in the bert ( bidirectional_encoder_representation ) model to improve the efficiency and stability of model pretraining . second , a new pretraining task of original sentence judgment is designed to enable the model to measure the degree of sentence smoothness , so that the bert model can better understand the semantics of context . by referring to the idea of attention_mechanism , a deep_learning framework with attention_weight added into gated convolution is constructed and the special attention_weight method is adopted to enhance semantic information . second , gating convolution and attention_mechanism are combined to model aspect-related semantic information and text complete semantic information . finally , classify the emotion classifier layer of social_network , use softmax_function to complete negative , positive , and neutral multiple classifications and calculate the result of emotion classification . by applying the optimized convolutional_neural_network cyclic optimization network to single task and multitask in practice , the feasibility of applying the optimized convolutional_neural_network and cyclic_neural_network to social_network sentiment_analysis is verified .
title : multimodal_sentiment_analysis via rnn variants ; abstract : multimodal_sentiment_analysis involves the classification of sentiment by using different forms of data together , namely , text , audio , and video . previously sentiment_analysis was implemented only on textual_data . multimodal_sentiment_analysis relies mainly on identifying the utterances present in the video and using them as a basis for sentiment_classification . in this paper , we proposed four different variants of rnn , namely , grnn , lrnn , glrnn and ugrnn for analyzing the utterances of the speakers from the videos . experimental results on cmi-mosi dataset demonstrates that our approach is able to achieve better sentiment_classification accuracy on individual modality ( text , audio , and video ) than existing_approaches on the same dataset . moreover , our method also gave decent results after fusing the individual modality using attention networks for multimodal_sentiment_analysis .
title : lexicalization of the motion event of `` going '' in the holy_quran : a cognitive approach ; abstract : motion is basic and universal concept and among the major life experiences of human beings . studies by talmy marked the beginning of cognitive research into motion ; he delineated the systematic relations among surface and deep structures and , based on the representation of the motion event , classified languages into two groups of satellite-framed and verb-framed . the present study examined the representation of the motion event in the verb `` to go '' via corpus-based analysis . the corpus comprised texts from the holy_quran , consisting of 88 verbs extracted from 114 quranic verbs meaning `` to go '' after evaluating the surface structures used for concepts related to this verb of motion . in collecting said corpus , the verbs were extracted from al-maány website and , based on the author 's linguistic intuition with regard to the motion concept of going in the quran , the relevant sample was collected and analyzed . this study aimed to examine manner- and path-lexicalization verbs , and the representation of information on manner and path based on the lexicalization patterns of talmy , and eventually investigate the consistency between the results and talmy 's theory about the classification of language in terms of motion representation . the results showed that , in the quran , of 88 verbs of motion with the deep structure of `` to go '' , 22 cases lexicalized the path such as zahaba ( going ) , araja ( ascending ) , 40 cases the manner such as haraba ( escaping ) and farra ( running away ) , and 26 cases a combination of the two such as jaraya ( flowing down ) and jarra ( pulling on the ground ) representing both elements of manner and path in the verb stem . of these , the highest frequency in the manner of motion belonged to transition motion ( 80 cases ) , while only 8 cases were of the rotatory type . another result was the inconclusiveness of talmy 's theory about the classifications of languages in terms of the motion concept of going .
title : persica : a persian corpus for multi-purpose text_mining and natural_language_processing ; abstract : lack of multi-application text_corpus despite of the surging text data is a serious bottleneck in the text_mining and natural_language_processing especially in persian_language . this paper presents a new corpus for news articles analysis in persian called persica . news analysis includes news classification , topic_discovery and classification , trend discovery , category classification and many more procedures . dealing with news has special requirements . first of all it needs a valid and news-content-enriched corpus to perform the experiments . our approach is based on a modified category classification and data normalization over persian news articles which has led to creation of a multipurpose persian corpus which shows reasonable results in text_mining outcomes . in the literature , regarding to our knowledge there are few persian corpuses but none of them have persian news time trend characteristics . empirical results on our benchmark indicate that in addition to reducing the problem dimensions and useless content , persica keeps admissible validity and reliability in comparison with standard corpuses in the literature . © 2012 ieee .
title : a comparative study of text_classification and missing word prediction using bert and ulmfit ; abstract : we perform a comparative study on the two types of emerging nlp models , ulmfit and bert . to gain_insights on the suitability of these models to industry-relevant tasks , we use text_classification and missing word prediction and emphasize how these two tasks can cover most of the prime industry use cases . we systematically frame the performance of the above two models by using selective metrics and train them with various configurations and inputs . this paper is intended to assist the industry researchers on the pros_and_cons of fine-tuning the industry data with these two pre-trained_language_models for obtaining the best possible state-of-the-art results .
title : an improved personalized recommender system with optimized abc neural_network ; abstract : the recommender system plays a vital role in the field of online business . to improve the quality of the recommender system , in this research work we focus on enhancingthesentiment analysis process to generate the most efficient recommendations to the users . e-commerce website has given consumers a venue to share their opinion on different entities . these opinions have a high impact on other customers to decide whether to purchase a product or not . by taking advantage of this , some users follow unethical ways to promote their products . analyzing these opinions manually is difficult , and thus attention-based bi-lstm convolutionneural network ( abc ) with rmsprop optimizer is proposed to extract the genuine opinion level of reviews automatically . the contributions are : ( i ) development of the abc model for text_classification with fake_review_detection , ( ii ) evaluation of the results in cross-domain datasets , ( iii ) analyzing the impact of different optimization_algorithms in the proposed neural networkthat can improve the classification_task , ( iv ) finally , sentiment_analysis module is passed to recommender module to produce top n recommendations.extensive experiments are carried out on three different domain datasets and the results clearly indicate abcwith rmsprop optimizer outperforms better than state-of-art text_classification methods .
title : research on xml text_categorization based on bayesian classifier ; abstract : according to the tree_structure of xml text , three kinds of feature_items are used to represent xml text , and the three kinds of feature_items are list , support branch structure and text word . in this way , both the content characteristics and structural characteristics are considered . in the aspect of dimension reduction , based on the three kinds of feature_items extracted , `` three-stage dimension reduction strategy '' is proposed . according to the structural characteristics of each feature item , we propose the method for calculating the weights of feature_items , and then integrate the weights into naϊve bayesian classifier , and thus , we get the structure-weighting based naϊve bayesian xml text_categorization . the feedback is applied to the classifier ; we make use of the results of the classifier to modify the weights of various feature_items in order to improve categorization results . © icic_international 2011 .
title : classification of german scripts by adjacent local_binary_pattern analysis of the coded text ; abstract : the paper proposes a script classification method which is based on textural analysis of the script types . in the first stage , each letter is coded by the equivalent script type , which is defined by its baseline position . obtained coded text is subjected to the adjacent local_binary_pattern analysis to extract the features . the result_shows the diversity of the extracted features between scripts , which makes the feature classification easier . it is the basis for decision-making process of the script_identification by automatic classification . the proposed method is tested on an example of synthetic and historical german printed_documents written in antiqua and fraktur scripts . the experiment_shows very positive results , which proved the correctness of the proposed algorithm .
title : effectiveness of a pse system evaluates internet literacy level by using naive_bayes_classifier ; abstract : in order to educate teenager internet literacy on social_network_service , we have developed a problem_solving environment to evaluate the literacy-level of their messages on twitter for their teachers and them . we propose a method the system provides effective recognition for their risks . and we adapt the naive_bayes_classifier to evaluation for tweets on twitter based on pattern-based classifier . in this result , the classification_accuracy for word patterns increases from 39.6-57.6 % to 68.0-79.9 % using naive_bayes_classifier on a set of 3000 training_data sets , and users obtain internet literacy skills base on this system .
title : density-based cluster algorithms in low-dimensional and high-dimensional applications ; abstract : cluster_analysis is the art of detecting groups of similar objects in large data sets- without having specified these groups by means of explicit features . among the various cluster algorithms that have been developed so far the density-based algorithms count to the most advanced and robust approaches . however , this paper shows that density-based cluster_analysis embodies no principle with clearly defined algorithmic properties . we contrast the density-based cluster algorithms dbscan and majorclust , which have been developed having different clustering tasks in mind , and whose strengths_and_weaknesses can be explained against the background of the dimensionality of the data to be clustered . our motivation for this analysis comes from the field of information_retrieval , where cluster_analysis plays a key role in solving the document_categorization problem . the paper is organized as follows : section 1 recapitulates the important principles of cluster algorithms , section 2 discusses the density-based algorithms dbscan and majorclust , and section 3 illustrates the strengths_and_weaknesses of both algorithms on the basis of geometric data analysis and document_categorization problems .
title : enhancement of arabic text_classification using semantic relations of arabic wordnet ; abstract : arabic text_classification methods have emerged as a natural result of the existence of a massive amount of varied textual_information ( written in arabic_language ) on the web . in most text_classification processes , feature_selection is crucial task since it highly affects the classification_accuracy . generally , two types of features could be used : statistical based features and semantic and concept features . the main interest of this paper is to specify the most effective semantic and concept features on arabic text_classification process . in this study , two novel features that use lexical , semantic and lexico-semantic relations of arabic wordnet ( awn ) ontology are suggested . the first feature_set is list of pertinent synsets ( lops ) , which is list of synsets that have a specific relation with the original terms . the second feature_set is list of pertinent words ( lopw ) , which is list of words that have a specific relation with the original terms . fifteen different relations ( defined in awn ontology ) are used with both proposed features . naïve_bayes classifier is used to perform the classification process . the experimental results , which are conducted on bbc_arabic dataset , show that using lops feature_set improves the accuracy of arabic text_classification compared with the well-known bag-of-word feature and the recent bag-of-concept ( synset ) features . also , it was found that lopw ( especially with related-to relation ) improves the classification_accuracy compared with lops , bagof- word and bag-of-concept .
title : research on the classification modeling for the natural_language texts with subjectivity characteristic ; abstract : the methods of natural_language text_classification have the characteristic of diversification , and the text characteristics are the basis of the method effectiveness ; this paper takes the car service complaint data as an example to study the classification modeling for the texts with subjectivity characteristic . the effective handling of car service complaints is important for improving user_experience and maintaining brand reputation ; manual classification commonly has the disadvantages of experience dependence , prone to error , heavy workload and so on ; corresponding automatic classification modeling research is of great practical_significance . the core links of the research method in this study include word_segmentation , text_vectorization , feature_selection and dimensionality_reduction based on correlation , classification modeling based on progressive method and random_forest , and model reliability analysis ; the research results show that the car service complaint_texts could be effectively classified based on the method in this study , which could provide a reference for related further research and application .
title : news text_classification model based on topic_model ; abstract : in modern_society , some famous news websites such as sina and times server to provide information every day for millions of users . but with the continuous development of information_technology , the amount of disorder data is increasing . how to organize the text and make automatically text_classification has become a challenge . the traditional manual classification of news text not only consumes a lot of human and financial resources , but also hardly achieved classification_task quickly . in this paper , the paper mainly makes a research about the news text_classification . it proposes a news text_classification model based on latent_dirichlet_allocation ( lda ) . due to the dimension of the news texts is too high , this model uses topic_model to make text dimension reduced and get features . at the same time , the paper also makes a research on softmax regression algorithm to solve multi-class of text problems in our life and make it as model 's classifier . the paper evaluates proposed model on a real_news dataset and the result of the experiment_shows the improved model performs relatively well . the model can effectively_reduce the features dimension of the news text and get good classification results .
title : joint image-text_representation by gaussian visual-semantic embedding ; abstract : how to jointly represent images and texts is important for tasks involving both modalities . visual-semantic embedding models have been recently proposed and shown to be effective . the key idea is that by learning a mapping from images into a semantic text space , the algorithm is able to learn a compact and effective joint_representation . however , existing_approaches simply map each text concept to a single point in the semantic space . mapping instead to a density distribution provides many interesting advantages , including better capturing uncertainty about each text concept , and enabling better geometric interpretation of concepts such as inclusion , intersection , etc . in this work , we present a novel gaussian visual-semantic embedding ( gvse ) model , which leverages the visual information to model text concepts as gaussian distributions in semantic space . experiments in two tasks , image_classification and text-based image_retrieval on the large_scale mit places205 dataset , have demonstrated the superiority of our method over existing_approaches , with higher_accuracy and better robustness .
title : sg < inf > ++ < /inf > : word_representation with sentiment and negation for twitter sentiment_classification ; abstract : here we propose an advance skip-gram model to incorporate both word sentiment and negation information . in particular , there is aa softmax_layer for the word sentiment_polarity upon the skip-gram model . then , two paralleled embedding layers are set up in the same embedding_space , one for the affirmative context and the other for the negated context , followed by their loss_functions . we evaluate our proposed model on the 2013 and 2014 semeval data sets . the experimental results show that the proposed approach achieves better performance and learns higher dimensional word_embedding informatively on the large-scale data .
title : cross-domain multi-task_learning for sequential sentence_classification in research papers ; abstract : sequential sentence_classification deals with the categorisation of sentences based on their content and context . applied to scientific_texts , it enables the automatic structuring of research papers and the improvement of academic_search engines . however , previous work has not investigated the potential of transfer_learning for sentence_classification across different scientific_domains and the issue of different text structure of full papers and abstracts . in this paper , we derive seven related research questions and present several contributions to address them : first , we suggest a novel uniform deep_learning architecture and multi-task_learning for cross-domain sequential sentence_classification in scientific_texts . second , we tailor two common transfer_learning methods , sequential transfer_learning and multi-task_learning , to deal with the challenges of the given task . semantic relatedness of tasks is a prerequisite for successful transfer_learning of neural_models . consequently , our third contribution is an approach to semi-automatically identify semantically_related classes from different annotation schemes and we present an analysis of four annotation schemes . comprehensive experimental results indicate that models , which are trained on datasets from different scientific_domains , benefit from one another when using the proposed multi-task_learning architecture . we also report comparisons with several state-of-the-art approaches . our approach_outperforms the state of the art on full paper datasets significantly while being on par for datasets consisting of abstracts .
title : generating hierarchical explanations on text_classification via feature interaction detection ; abstract : generating explanations for neural_networks has become crucial for their applications in real-world with respect to reliability and trustworthiness . in natural_language_processing , existing_methods usually provide important features which are words or phrases selected from an input text as an explanation , but ignore the interactions between them . it poses challenges for humans to interpret an explanation and connect it to model prediction . in this work , we build hierarchical explanations by detecting feature interactions . such explanations visualize how words and phrases are combined at different levels of the hierarchy , which can help users understand the decision-making of black-box models . the proposed method is evaluated with three neural text classifiers ( lstm , cnn , and bert ) on two benchmark_datasets , via both automatic and human evaluations . experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans .
title : classifying natural_language sentences for policy ; abstract : organizations derive policies from a wide variety of sources , such business plans , laws , regulations , and contracts . however , an efficient process does not yet exist for quickly finding or automatically deriving policies from uncontrolled natural_language sources . the goal of our research is to assure compliance with established policies by ensuring policies in existing natural_language texts are discovered , appropriately represented , and implemented . we propose a tool-based process to parse natural_language documents , learn which statements signify policy , and then generate appropriate policy representations . to evaluate the initial work on our process , we analyze four data use agreements for a particular project and classify sentences as to whether or not they pertain to policy , requirements , or neither . our k-nearest_neighbor classifier with a unique distance_metric had a precision of 0.82 and a recall of 0.81 , outperforming weighted random guess , which had a precision of 0.44 and a recall of 0.46 . the initial results demonstrate the feasibility of classifying sentences for policy and we plan to continue this work to derive policy elements from the natural_language text . © 2012 ieee .
title : enhancing aspect-based sentiment_analysis of arabic hotels ’ reviews using morphological , syntactic and semantic features ; abstract : this research presents an enhanced approach for aspect-based sentiment_analysis ( absa ) of hotels ’ arabic reviews using supervised_machine_learning . the proposed approach employs a state-of-the-art research of training a set of classifiers with morphological , syntactic , and semantic features to address the research tasks namely : ( a ) t1 : aspect category identification , ( b ) t2 : opinion_target expression ( ote ) extraction , and ( c ) t3 : sentiment_polarity identification . employed classifiers include naïve_bayes , bayes networks , decision_tree , k-nearest_neighbor ( k-nn ) , and support-vector_machine ( svm ) .the approach was evaluated using a reference dataset based on semantic evaluation 2016 workshop ( semeval-2016 : task-5 ) . results show that the supervised_learning approach_outperforms related work evaluated using the same dataset . more precisely , evaluation results show that all classifiers in the proposed approach outperform the baseline approach , and the overall enhancement for the best performing classifier ( svm ) is around 53 % for t1 , around 59 % for t2 , and around 19 % in t3 .
title : pre-consulting dialogue_systems for telemedicine : yes/no intent_classification ; abstract : telemedicine is an emerging challenge for the shortage of qualified professionals , particularly in under-resourced regions . physical assessment by a non-medical doctor is a practice in telemedicine which discovers essential symptom of a patient who needs to consult a doctor . we aim at facilitating this stage with a conversational chatbot which identifies the patient by conversation . adopting the procedures of physical assessment one critical types of conversation involves in the self-diagnosis . further , it turned out that useful kinds of questions in chatbot at this stage are related to yes/no questions . we discovered that particular difficulties lie in the ambiguous replies by the patients : a patient modifies a question which makes them answer yes or no , a response does not the corresponding reply to the question , a reply involves some part yes and some part no , and so on . focusing on this particular type of question we introduce a text classifier using long short-term_memory ( lstm ) and build a corpus using twitter .
title : an on-device machine_reading_comprehension model with adaptive fast inference ; abstract : pretrained_language_models ( prlms ) have been widely used in machine_reading_comprehension ( mrc ) tasks . although prlms can learn powerful representations from large-scale corpora , it introduces the problem of model redundancy , which severely restricts the application of mobile devices . recent_studies have suggested the use of early exit ( ee ) methods for the fast inference of prlms . however , there are two limitations to the application of existing ee methods . first , different transformer layers have different capabilities for learning semantic features . thus , the lower layers may behave differently from the higher layers . as a result , the earlier the exit from the model , the lower its performance . second , a greedy search is applied to the classifiers exiting either the start or the end position . however , the start and end positions may also be related . choosing only one best candidate might be suitable for the current step , but it may be a suboptimal choice in global inference . to address these issues , this study proposes an on-device mrc model with fast inference based on an ee strategy . to ensure that the lower layers have similar abilities as the higher layers , we propose the use of self-distillation to transfer knowledge from the higher layers to the lower ones . furthermore , a dynamic_programming strategy was applied to choose the optimal confidence combination of both the start and end classifiers to better select the exit layers in the inference stage . experimental results on both squad 1.1 and 2.0 show that the inference of the proposed model is accelerated at the minimum cost of performance loss , thereby outperforming previous ee models .
title : centrality measures for text clustering ; abstract : text clustering is an unsupervised process of classifying texts and words into different groups . in literature , many algorithms use a bag of words model to represent texts and classify contents . the bag of words model assumes that word_order has no signicance . the aim of this article is to propose a new method of text clustering , considering links between terms and documents . we use centrality measures to assess word/text importance in a corpus and to sequentially classify documents . copyright © taylor_&_francis_group , llc .
title : rift : a rule_induction framework for twitter sentiment_analysis ; abstract : the rapid evolution of microblogging and the emergence of sites such as twitter have propelled online_communities to flourish by enabling people to create , share and disseminate free-flowing messages and information globally . the exponential_growth of product-based user_reviews has become an ever-increasing resource playing a key role in emerging twitter-based sentiment_analysis ( sa ) techniques and applications to collect and analyse customer trends and reviews . existing_studies on supervised black-box sentiment_analysis systems do not provide adequate information , regarding rules as to why a certain review was classified to a class or classification . the accuracy in some ways is less than our personal judgement . to address these shortcomings , alternative approaches , such as supervised white-box classification_algorithms , need to be developed to improve the classification of twitter-based microblogs . the purpose of this study was to develop a supervised white-box microblogging sa system to analyse user_reviews on certain products using rough_set_theory ( rst ) -based rule_induction algorithms . rst classifies microblogging reviews of products into positive , negative , or neutral class using different rules extracted from training decision tables using rst-centric rule_induction algorithms . the primary_focus of this study is also to perform sentiment_classification of microblogs ( i.e . also known as tweets ) of product_reviews using conventional , and rst-based rule_induction algorithms . the proposed rst-centric rule_induction algorithm , namely learning from examples module version : 2 , and lem2 + corpus-based rules ( lem2 + cbr ) , which is an extension of the traditional lem2 algorithm , are used . corpus-based rules are generated from tweets , which are unclassified using other conventional lem2 algorithm rules . experimental results show the proposed method , when compared with baseline_methods , is excellent , with regard to accuracy , coverage and the number of rules employed . the approach using this method_achieves an average accuracy of 92.57 % and an average coverage of 100 % , with an average number of rules of 19.14 .
title : classification of text documents using b-tree ; abstract : in this paper , we propose an unconventional method of representing and classifying text documents , which preserves the sequence of term_occurrence in a test document . the term sequence is effectively preserved with the help of a novel datastructure called 'status matrix ' . in addition , in order to avoid sequential matching during classification , we propose to index the terms in b-tree , an efficient index scheme . each term in b-tree is associated with a list of class_labels of those documents which contain the term . further the corresponding classification technique has been proposed . to corroborate the efficacy of the proposed representation and status matrix based classification , we have conducted extensive_experiments on various datasets . © institute for computer sciences , social informatics and telecommunications_engineering 2012 .
title : multi-level feature_fusion method for long text_classification ; abstract : news classification_task is essentially long text_classification in the field of nlp ( natural_language_processing ) . long text contains a lot of hidden or topic-independent information . moreover , bert ( bidirectional_encoder_representations_from_transformer ) can only process the text with a character sequence length of 512 at most , which may lose the key information and reduce the classification effectiveness . to solve above problems , the paper puts_forward a model of mutli-level feature_fusion based on bert , which is suitable for the bert through the hierarchical decomposition of long text . then cnn ( convolutional_neural_networks ) and stacked bilstm ( bidirectional_long_short-term_memory ) based on attention_mechanism are used to capture local and contextual features of text respectively . finally , various features are spliced for classification_task . the experimental results show that the model achieves 97.4 % accuracy and 97.2 % f1 score on thucnews , 1.2 % accuracy and 1.6 % f1 score higher than that of bert-cnn , 1.8 % accuracy and 1.4 % f1 score higher than that of bert-bilstm , indicating that our model can significantly_improve the effectiveness of news classification .
title : automatic verbal analysis of interviews with schizophrenic patients ; abstract : schizophrenia is a long-term mental disease associated with language impairments that affect about one percent of the population . traditional assessment of schizophrenic patients is conducted by trained professionals , which requires tremendous resources of time and effort . this study is part of a larger research objective committed to creating automated platforms to aid clinical_diagnosis and understanding of schizophrenia . we have analyzed non-verbal cues and movement signals in our previous work . in this study , we explore the feasibility of using automatic transcriptions of interviews to classify patients and predict the observability of negative symptoms in schizophrenic patients . interview recordings of 50 schizophrenia patients and 25 age-matched healthy_controls were automatically transcribed by a speech_recognition toolkit . after which , natural_language_processing techniques were applied to automatically_extract the lexical_features and document_vectors of transcriptions . using these features , we applied ensemble machine_learning algorithm ( by leave-one-out cross-validation ) to predict the negative symptom assessment subject ratings of schizophrenic patients , and to classify patients from controls , achieving a maximum accuracy of 78.7 % . these results indicate that schizophrenic patients exhibit significant differences in lexical usage compared with healthy_controls , and the possibility of using these lexical_features in the understanding and diagnosis of schizophrenia .
title : aspect extraction and classification for sentiment_analysis in drug reviews ; abstract : aspect-based sentiment_analysis ( absa ) of patients ’ opinions_expressed in drug reviews can extract valuable_information about specific aspects of a particular drug such as effectiveness , side effects and patient conditions . one of the most important and challenging tasks of absa is to extract the implicit and explicit aspects from a text , and to classify the extracted aspects into predetermined classes . supervised_learning algorithms possess high accuracy in extracting and classifying aspects ; however , they require annotated_datasets whose manual construction is time-consuming and costly . in this paper , first a new method was introduced for identifying expressions that indicate an aspect in user_reviews about drugs in english . then , distant_supervision was adopted to automate the construction of a training_set using sentences and phrases that are annotated as aspect classes in the drug domain . the results of the experiments showed that the proposed method is able to identify various aspects of the test set with 74.4 % f-measure , and outperforms the existing aspect extraction methods . also , training the random_forest classifier on the dataset that was constructed via distant_supervision obtained the f-measure of 73.96 % , and employing this dataset to fine-tune bert for aspect classification yielded better f-measure ( 78.05 % ) in comparison to an existing method in which the random_forest classifier trained on an accurate manually constructed dataset .
title : investigating the dynamics of religious conflicts by mining public opinions on social_media ; abstract : the powerful emergence of religious faith and beliefs within political and social groups , now leading to discrimination and violence against other communities has become an important problem for the government and law enforcement agencies . in this paper , we address the challenges and gaps of offline surveys by mining the public opinions , sentiments and beliefs shared about various religions and communities . due to the presence of descriptive posts , we conduct our experiments on tumblr website- the second most popular microblogging service . based on our survey among 3 different groups of 60 people , we define 11 dimensions of public opinion and beliefs that can identify the contrast of conflict in religious posts . we identify various linguistic features of tumblr posts using topic_modeling and linguistic_inquiry_and_word_count . we investigate the efficiency of dimensionality_reduction techniques and semi-supervised classification methods for classifying the posts into various dimensions of conflicts . our results reveal that linguistic features such as emotions , language variables , personality_traits , social process , and informal_language are the discriminatory features for identifying the dynamics of conflict in religious posts .
title : assertion modeling and its role in clinical phenotype identification ; abstract : this paper describes an approach to assertion_classification and an empirical_study on the impact this task has on phenotype identification , a real_world application in the clinical domain . the task of assertion_classification is to assign to each medical concept mentioned in a clinical report ( e.g. , pneumonia , chest_pain ) a specific assertion category ( e.g. , present , absent , and possible ) . to improve the classification of medical assertions , we propose several new features that capture the semantic properties of special cue words highly indicative of a specific assertion category . the results obtained outperform the current state-of-the-art results for this task . furthermore , we confirm the intuition that assertion_classification contributes in significantly improving the results of phenotype identification from free-text clinical_records . © 2012 elsevier inc .
title : beyond a binary of ( non ) racist tweets : a four-dimensional categorical detection and analysis of racist and xenophobic opinions on twitter in early covid-19 ; abstract : transcending the binary categorization of racist and xenophobic texts , this research takes cues from social_science theories to develop a four dimensional category for racism and xenophobia detection , namely stigmatization , offensiveness , blame , and exclusion . with the aid of deep_learning techniques , this categorical detection enables insights into the nuances of emergent topics reflected in racist and xenophobic expression on twitter . moreover , a stage wise analysis is applied to capture the dynamic changes of the topics across the stages of early development of covid-19 from a domestic epidemic to an international public_health emergency , and later to a global_pandemic . the main contributions of this research include , first the methodological advancement . by bridging the state-of-the-art computational methods with social_science perspective , this research provides a meaningful approach for future_research to gain_insight into the underlying subtlety of racist and xenophobic discussion on digital platforms . second , by enabling a more accurate comprehension and even prediction of public opinions and actions , this research paves the way for the enactment of effective intervention policies to combat racist crimes and social_exclusion under covid-19 .
title : cwc : a clustering-based feature_weighting approach for text_classification ; abstract : most existing text_classification methods use the vector_space_model to represent documents , and the document_vectors are evaluated by the tf-idf method . however , tf-idf weighting does not take into account the fact that the weight of a feature in a document is related not only to the document , but also to the class that document belongs to . in this paper , we present a clustering-based feature_weighting approach for text_classification , or cwc for short . cwc takes each class in the training collection as a known cluster , and searches for feature_weights iteratively to optimize the clustering objective_function , so the best clustering result is achieved , and documents in different classes can be best distinguished by using the resulting feature_weights . performance of cwc is validated by conducting classification over two real text_collections , and experimental results show that cwc outperforms the traditional knn . ©_springer-verlag_berlin_heidelberg 2007 .
title : emotion classification from speech and text in videos using a multimodal_approach ; abstract : emotion classification is a research area in which there has been very intensive literature production concerning natural_language_processing , multimedia data , semantic knowledge discovery , social_network mining , and text and multimedia data_mining . this paper addresses the issue of emotion classification and proposes a method for classifying the emotions expressed in multimodal data extracted from videos . the proposed method models multimodal data as a sequence of features extracted from facial_expressions , speech , gestures , and text , using a linguistic approach . each sequence of multimodal data is correctly associated with the emotion by a method that models each emotion using a hidden_markov_model . the trained model is evaluated on samples of multimodal sentences associated with seven basic emotions . the experimental results demonstrate a good classification rate for emotions .
title : an improved knn text_classification algorithm based on density ; abstract : text_classification has gained booming interest over the past few years . as a simple , effective and nonparametric classification method , knn method is widely used in document_classification . however , the uneven distribution in training_set will affect the knn classified result negatively . moreover , the uneven distribution phenomenon of text is very common in documents on the web . to tackling on this , this paper proposes an improved knn method denoted by dbknn . experimental results show that the dbknn algorithm can better serve classification requests for large sets of unevenly distributed documents . © 2011 ieee .
title : evaluating pre-trained_transformers on italian administrative texts ; abstract : in recent_years , transformer-based models have been widely used in nlp for various downstream_tasks and in different domains . however , a language model explicitly built for the italian administrative language is still lacking . therefore , in this paper , we decided to compare the performance of five different transformer models , pre-trained on general purpose texts , on two main tasks in the italian administrative domain : name entity_recognition and multi-label document_classification on public administration ( pa ) documents . we evaluate the performance of each model on both tasks to identify the best model in this particular domain . we also discuss the effect of model size and pre-training data on the performances on domain data . our evaluation identifies umberto as the best-performing model , with an accuracy of 0.71 , an f1 score of 0.89 for multi-label document_classification , and an f1 score of 0.87 for ner-pa .
title : classification of personality_traits on facebook using key_phrase_extraction , language models and machine_learning ; abstract : the study of automated personality_trait detection has become increasingly prevalent in the field of affective_computing and sentiment_analysis . psycholinguistic databases and linguistic styles are found to have correlations with personality_traits . natural_language_processing has improved thanks to transfer_learning . with pre-trained_language_models as features , this paper presents a machine_learning model that benefits from transfer_learning . the proposed method using key_phrase_extraction and sentencetransformer through decision_tree classifier surpasses the xgboost by 26.79 % in terms of accuracy and exhibits 97.79 % on mypersonality dataset .
title : aspect_term_extraction and categorization for chinese mooc reviews ; abstract : sentiment_analysis has become one of the most active topics in education research . so far , however , there has been little discussion about the recent application of sentiment_analysis for chinese mooc reviews . therefore , this paper sheds_light on some fine-grained sentiment_analysis technology to benefit the current students and education practitioners . firstly , we focus on extracting aspect terms associated with the course via dependency_parsing and sentiment word lexicons . secondly , we categorize the aspect terms with the naive_bayes . experimental results effectively demonstrate that the proposed approach and refine the granularity of sentiment categories in higher_education . this paper makes sentiment_analysis possible to increase students ' learning retention and improve teachers ' performance in online teaching .
title : research on intelligent_question_answering system under operation and maintenance knowledge_graph ; abstract : at present , many intelligent_question_answering based on knowledge graphs can answer simple questions , but there is no good solution to complex multi-hop problems . in response to this , relying on the knowledge_graph of the operation and maintenance domain , an intelligent_question_answering system based on the classification strategy and subgraph matching idea is proposed , which can better solve the complex question_answering under the knowledge_graph intelligent_question_answering ; especially for multi-hop questions.firstly , the multi-hop question is divided through the classification model , and then the corresponding subgraph is recalled through the entity_mention and the multi-hop category , and finally the original question and the candidate subgraph are matched for similarity to obtain the answer to the question . the experimental results show that the system has achieved high accuracy and application value in map question and answer in the field of operation and maintenance .
title : deep graph neural_networks for text_classification task ; abstract : text_classification is to organizing documents into predetermined_categories , usually by machinery learn algorithms . it is a significant ways to organize and utilize the large amount of information that exists in unstructured_text format . text_classification is an important module in text processing , and its applications are also very extensive , such as garbage filtering , news classification , part-of-speech_tagging , and so on . with the continuous development of deep_learning in recent_years ! its applications are also very extensive , such as : garbage filtering , news classification , part-of-speech_tagging , and so on . but the text also has its own characteristics . according to the characteristics of the text , the general process of text_classification is : 1 . preprocessing ; 2 . text_representation and feature_selection ; 3 . construction of a classifier ; 4 . the task of text_classification refers to classifying texts into only single or many types in tc system . some researchers are beginning to apply deep_neural_networks to tasks such as the text_classification we mentioned above . although the research around the task has made great_progress , the review of this task is very scarce , and there is a lack of a comprehensive review of the development of the task in recent_years . therefore , we present a survey of research in text_classification to create taxonomies . finally , it is by giving vital effects , the direction of future_research , and those challenges that may counter in the research field .
title : # brexit : leave or remain ? the role of user 's community and diachronic evolution on stance_detection ; abstract : interest has grown around the classification of stance that users assume within online debates in recent_years . stance has been usually addressed by considering users posts in isolation , while social_studies highlight that social communities may contribute to influence users ' opinion . furthermore , stance should be studied in a diachronic perspective , since it could help to shed light on users ' opinion shift dynamics that can be recorded during the debate . we analyzed the political discussion in uk about the brexit referendum on twitter , proposing a novel approach and annotation schema for stance_detection , with the main aim of investigating the role of features related to social_network community and diachronic stance evolution . classification experiments show that such features provide very useful clues for detecting stance .
title : a survey on text_classification techniques for sentiment_polarity detection ; abstract : with the increasing growth and availability of resources , there arises a difficulty in gaining relevant_information . text_classification is a mining method to classify each document into a fixed number of predefined_classes in order to reduce the length of the text without losing significant information . opinion_mining identifies and extracts subjective_information from various sources using techniques such as natural_language_processing , text analysis and computational_linguistics . opinions provided by individuals and organizations can be utilized for improving market_trends and decision_making . this paper discusses various text_classification techniques for opinion_mining such as bayesian classification , latent_dirichlet_allocation ( lda ) classification , dynamic ontology classification , novel algorithm and genetic_algorithm .
title : referemo : a referential quasi-multimodal model for multilabel emotion classification ; abstract : textual emotion classification is a task in affective ai that branches from sentiment_analysis and focuses on identifying emotions expressed in a given text excerpt . it has a wide variety of applications that improve human-computer interactions , particularly to empower computers to understand subjective human language better . significant research has been done on this task , but very little of that research leverages one of the most emotion-bearing symbols we have used in modern communication : emojis . in this research , we propose referemo , a model that processes emojis as textual inputs and leverages deepmoji to generate affective feature_vectors used as reference when aggregating different modalities of text encoding . to evaluate referemo , we experimented on two benchmark_datasets : semeval ’ 18 and goemotions for emotion classification , and achieved competitive_performance compared to state-of-the-art models tested on these datasets . notably , our model performs better on the underrepresented classes of each dataset . the source_code of referemo is available on github ( https : //github.com/alvarosness/referemo ) .
title : question_answering system over knowledge_graph of weapon field ; abstract : question_answering system in the weapon field not only enables users to obtain information on weapons quickly and accurately , but also provides smarter question_answering . with military weapons as the research direction , an svm question_classification method based on chinese character algorithm is proposed , and a question_answering system over knowledge_graph of weapons is established . the domain word_segmentation is used in this system to analyze user questions , extract question features for classification , intelligently generate sparql , query rdf to get answers , and return them to the user . experiments show that the intelligent_question_answering system with the multi-question_classification is effective and highly_accurate in answering_questions in the field of weapon .
title : deck : behavioral tests to improve interpretability and generalizability of bert models detecting depression from text ; abstract : models that accurately detect depression from text are important tools for addressing the post-pandemic mental_health crisis . bert-based classifiers ' promising performance and the off-the-shelf availability make them great candidates for this task . however , these models are known to suffer from performance inconsistencies and poor generalization . in this paper , we introduce the deck ( depression checklist ) , depression-specific model behavioural tests that allow better interpretability and improve generalizability of bert classifiers in depression domain . we create 23 tests to evaluate bert , roberta and albert depression classifiers on three datasets , two twitter-based and one clinical interview-based . our evaluation shows that these models : 1 ) are robust to certain gender-sensitive variations in text ; 2 ) rely on the important depressive language marker of the increased use of first person pronouns ; 3 ) fail to detect some other depression symptoms like suicidal_ideation . we also demonstrate that deck tests can be used to incorporate symptom-specific information in the training_data and consistently improve generalizability of all three bert models , with an out-of-distribution f1-score increase of up to 53.93 % .
title : identifying finding sentences in conclusion subsections of biomedical_abstracts ; abstract : segmenting scientific_abstracts and full-text based on their rhetorical function is an essential task in text_classification . small rhetorical segments can be useful for fine-grained literature search , summarization , and comparison . current effort has been focusing on segmenting documents into general sections such as introduction , method , and conclusion , and much less on the roles of individual sentences within the segments . for example , not all sentences in the conclusion section are describing research findings . in this work , we developed rule-based and machine_learning methods and compared their performance in identifying the finding sentences in conclusion subsections of biomedical_abstracts . 1100 conclusion subsections with observational and randomized clinical_trials study designs covering five common health topics were sampled from pubmed to develop and evaluate the methods . the rule-based method and the bag-of-words based machine_learning method both achieved high accuracy . the better performance by the simple rule-based approach shows that although advanced machine_learning approaches could capture the main patterns , human expert may still outperform on such a specialized task .
title : a cnn-based born-again tsk_fuzzy classifier integrating soft_label information and knowledge distillation ; abstract : this paper proposes a cnn-based born-again takagi-sugeno-kang ( tsk ) fuzzy classifier denoted as cnnbatsk . cnnbatsk achieves the following distinctive characteristics : 1 ) cnnbatsk provides a new perspective of knowledge distillation with a non-iterative learning method ( least learning machine with knowledge distillation , llm-kd ) to solve the consequent parameters of fuzzy_rule , where consequent parameters are trained jointly on the ground-truth label loss , knowledge distillation loss , and regularization term ; 2 ) with the inherent advantage of the fuzzy_rule , cnnbatsk has the capability to express the dark knowledge acquired from the cnn in an interpretable manner . specifically , the dark knowledge ( soft_label information ) is partitioned into five fixed antecedent fuzzy spaces . the centers of each soft_label information in different fuzzy_rules are { 0 , 0.25 , 0.5 , 0.75 , 1 } , which may have corresponding linguistic explanations : { < italic > very low , low , medium , high , very high < /italic > } . for the consequent part of the fuzzy_rule , the original features are employed to train the consequent parameters that ensure the direct interpretability in the original feature_space . the experimental results on the benchmark_datasets and the chb-mit eeg dataset demonstrate that cnnbatsk can simultaneously improve the classification performance and model interpretability .
title : analyzing employee voice using real-time feedback ; abstract : people nowadays tend to use social_media as a platform to share their reviews , emotions , and opinions , including about their jobs . thus , a lot of data is available on the web . therefore , a rapid response is needed to analyze and interpret the data . unfortunately , many organizations still use annual surveys to assess satisfaction , engagement , and culture in the workplace . compared to other conventional datasets such as company survey and questionnaire , decision-makers could make decision effectively and efficiently by using the interpreted data . this may be done with the help of sentiment_analysis method . in this research , we classify the feedback based on its category and sentiment . several classification_algorithms are used in opinion_mining ; two of them are naive_bayes_classifier ( nbc ) and support_vector_machine ( svm ) . this paper aims to classify feedback based on sentiments using nbc and svm .
title : a svm based speech to text converter for turkish_language ; abstract : in proposed speech to text conversion , a support_vector_machines ( svm ) based turkish speech to text converter system has been developed . in the recognition system , mel_frequency_cepstral_coefficients ( mfcc ) has been applied to extract features of turkish speech and svm based classifier has been used to classify the phonemes . the morphological structure of turkish , a language based on phonemes , has been taken into consideration in the devoloped person-dependent voice_recognition system . unlike the multiclass classifiers which are used in the svm-mfcc based voice_recognition system , a new svm classifier system has been developed that uses fewer classes in layers , increasing the number of multiclass layers . a new text comparison algorithm is proposed , which also uses phoneme sequence to measure similarity in word_similarity measurement . along with these enhancements , as the training period becomes higher , performance of voice_recognition is improved and word recognition performance is increased . the performance of the proposed structure is compared with similar systems .
title : order-preserving abstractive_summarization for spoken content based on connectionist temporal classification ; abstract : connectionist temporal classification ( ctc ) is a powerful approach for sequence-to-sequence learning , and has been popularly used in speech_recognition . the central ideas of ctc include adding a label `` blank '' during training . with this mechanism , ctc eliminates the need of segment alignment , and hence has been applied to various sequence-to-sequence learning problems . in this work , we applied ctc to abstractive_summarization for spoken content . the `` blank '' in this case implies the corresponding input data are less important or noisy ; thus it can be ignored . this approach was shown to outperform the existing_methods in term of rouge scores over chinese gigaword and matbn corpora . this approach also has the nice property that the ordering of words or characters in the input documents can be better preserved in the generated_summaries .
title : meta_learning for few-shot joint intent_detection_and_slot-filling ; abstract : intent_detection_and_slot_filling are the two main tasks in natural_language_understanding module in goal_oriented conversational_agents . models which optimize these two objectives simultaneously within a single network ( joint models ) have proven themselves to be superior to mono-objective networks . however , these data-intensive deep_learning approaches have not been successful in catering the demand of the industry for adaptable , multilingual dialogue_systems . to this end , we cast joint intent_detection as an n-way k-shot classification problem and establish it within meta_learning setup . our approach is motivated by the success of meta_learning on few-shot image_classification tasks . we empirically_demonstrate that , our approach can meta-learn a prior from similar tasks under highly resource_constrained settings which enable rapid inference on target tasks . first , we show the adaptability of proposed approach by meta_learning n-way k-shot joint intent_detection using set of intents and evaluating on a completely new set of intents . second , we exemplify the cross-lingual adaptability by learning a prior , utilizing english utterances and evaluating on spanish and thai utterances . compared to random initialization , our method significantly_improves the accuracy in both intent_detection_and_slot-filling .
title : classification of summarized sensor data using sampling and clustering : a performance analysis ; abstract : as humans and machines generate a tremendous amount of digital data in their daily life , we are in the era of big_data which poses unique_challenges of storing , processing and analyzing this voluminous data . sensors which continuously generate data are one important source of big_data and have innumerous applications in real_life scenario . as storing the entire data becomes expensive , summarization is the need of the hour . data summarization is a compact representation of the entire data which can reduce storage and processing requirements . in this work , we try to effectively summarize sensor data using simple but effective techniques such as sampling and clustering and analyze the performance of the summarized data in comparison to the complete dataset . popular classification techniques like knn , svm and naive_bayes are used to evaluate the efficiency of the summarization_techniques by training the classifiers using the summarized data and testing with the test data set . the performance of the summarized dataset and the complete dataset are compared . the experimental results show that summarized data set performs almost equally well as the complete data set .
title : simultaneous learning of trees and representations for extreme classification and density_estimation ; abstract : we consider multi-class classification where the predictor has a hierarchical_structure that allows for a very large number of labels both at train and test time . the predictive_power of such models can heavily depend on the structure of the tree , and although past work showed how to learn the tree_structure , it expected that the feature_vectors remained static . we provide a novel algorithm to simultaneously perform representation_learning for the input data and learning of the hierarchi- cal predictor . our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions . we theoret- ically analyze this objective , showing that it gives rise to a boosting style property and a bound on classification error . we next show how to extend the algorithm to conditional density_estimation . we empirically validate both variants of the al- gorithm on text_classification and language mod- eling , respectively , and show that they compare favorably to common baselines in terms of accu- racy and running time .
title : an association_rule_mining method based on named_entity_recognition and text_classification ; abstract : using massive text data , building a knowledge_graph to implement in-depth association analysis and mining can help identify entities and make decisions . the accuracy of traditional_chinese named_entity_recognition methods is low , and traditional frequent itemset mining methods are also difficult to obtain different types of categories , and their novelty is not high . in this paper , we propose an association_rule_mining method based on named_entity_recognition and text_classification ( armtner ) . first , the textcnn_model is used to extract the word_vector information of the text data ; secondly , bidirectional_lstm is used the model extracts the contextual features of the text ; then the neural_network model is used to automatically_extract the word features and the global features of the text for text_classification ; finally , the text sequence labeling and entity_recognition are performed . then the association_rules be used to mine frequent itemsets through two-level classification of text_classification and entity classification . the experimental results showed that , our method can achieve an f1-score of 97.3 % on the public data set in chinese named_entity_recognition , and the novelty of frequent itemsets increased by 0.279 % .
title : domain_adaptive model for sentiment_classification using deep_learning approach ; abstract : this paper discusses a domain_adaptive model for opinion_mining and sentiment_classification of unstructured_text reviews_posted in social_media site or web_forums . the amount of online textual reviews generated in social_media websites and blogs is growing exponentially and can source from numerous domains and is difficult to maintain labeled_training_data for all of them . model proposed in the paper utilizes deep_learning techniques for feature_extraction in an unsupervised manner and neural_network for sentiment_classification of textual reviews . the goal is to build a domain_adaptive model which can automatically capture the shared sentiment features across domains .
title : classification of chinese text in the power field using fasttext with electric_power keywords ; abstract : text_classification has always been a hot research topic in the field of natural_language_processing . fasttext is a model which is simple in structure and has the advantage of fast and efficient . in order to analyze the situation of electric_power intellectual_property in recent_years , this paper use the model of fasttext to process chinese electric_power core journals and patents in recent five years . the text information in the company 's intellectual_property_rights is divided into a series of phrases representing individual meanings though word_segmentation basing on the jieba word_segmentation tool . the stop_words unrelated to the power_industry in the phrases are removed , and the word related the power_industry frequency corresponding to the phrases and the frequency of occurrence in the text are counted . according to the characteristics of power_industry word aggregation , the phrase can be divided into 26 categories . the text_classification model based on keywords of power_industry was designed , and the machine_learning algorithm fasttext is used to complete the text_classification . based on each subdivision field of power , the publication situation of intellectual_property is analyzed and counted , and the results of power system intellectual_property are visualized .
title : a two-level deep_learning approach for emotion recognition in arabic news headlines ; abstract : going online has created more opportunities for newspapers to present breaking_news in a timely manner . concentrating in spreading more bad news increases the feeling of danger and depression in the society . some authors believe on tendency of some media to be focused on sharing the bad events in life rather than the good ones because of the impact and the attraction over the audience is more significant . sentiment_analysis work has not been recognized , proposed , or documented on arabic news because of the challenges that arabic raises as a language including the different arabs ' dialects and its complex grammatical_structure . with the emerging flow of news that cause a global panic and anxiety worldwide , this study focuses on the serious need to identify what effective role could machine_learning classifiers have in the early detection process of the psychology impact on the readers by the daily news headlines . in this work , a dataset of news headlines were gathered from top popular online arabic news sites and were annotated to seven emotional_categories : anger , disgust , fear , happiness , neutral , sadness , and surprise . a convolutional_neural_network-based two-level approach was proposed for sentiment_classification . the performance of the proposed approach was compared to six machine_learning classifiers ( zeror , k-nearest_neighbor , decision_trees , naïve_bayes , random_forest and support_vector_machine ) and showed better accuracy , precision , and recall .
title : organisation , classification and analysis of online_reviews directed to retail in the municipality of porto ; abstract : web_2.0 has allowed collaboration , interaction and sharing of information online , such as online_review platforms . consequently , these short and straightforward opinions have increasingly proved to be essential sources of information not only for consumers but also for companies , as they represent the consumer 's sincere evaluation , free from any kind of bias . in this sense , there should be an interest in the analysis and monitoring of online_reviews by companies , as the result of these actions may provide guidelines to readjust their strategy , support decision-making and ensure the satisfaction of their consumers . to generate useful information to assist decision-making and strategies ’ implementation by retailers in the municipality of porto , online_reviews from the googlemybusiness platform were organised , classified , and analysed . 9945 online_reviews were extracted , directed to 246 retail adaptations of the municipality of porto , from 2017 to 2020 , which were later classified by the polarity of sentiment ( positive , negative , neutral , or mixed ) . sentiment_analysis was conducted , combined with statistical_tests and frequency_distribution tables to discover relevant_information for retailers . with sentiment_analysis , retailers can understand their consumers and their behaviour to adapt their strategies and make the right decisions to ensure their customers ’ satisfaction . with the results obtained , this study proves that it is possible to extract useful information from online_reviews and reveals that it is still an area of little interest for retailers in the municipality of porto .
title : bag-of-words vs. graph vs. sequence in text_classification : questioning the necessity of text-graphs and the surprising strength of a wide mlp ; abstract : graph neural_networks have triggered a resurgence of graph-based text_classification methods , defining today 's state of the art . we show that a wide multi-layer_perceptron ( mlp ) using a bag-of-words ( bow ) outperforms the recent graph-based models textgcn and hetegcn in an inductive text_classification setting and is comparable with hypergat . moreover , we fine-tune a sequence-based bert and a lightweight distilbert model , which both outperform all state-of-the-art models . these results question the importance of synthetic graphs used in modern text classifiers . in terms of efficiency , distilbert is still twice as large as our bow-based wide mlp , while graph-based models like textgcn require setting up an $ \mathcal { o } ( n^2 ) $ graph , where $ n $ is the vocabulary plus corpus size . finally , since transformers need to compute $ \mathcal { o } ( l^2 ) $ attention_weights with sequence length $ l $ , the mlp models show higher training and inference speeds on datasets with long sequences .
title : interactional style detection for versatile dialogue response using prosodic and semantic features ; abstract : this work presents an approach to interactional style ( is ) detection for versatile responses in spoken_dialogue_systems ( sdss ) . since speakers generally express their intents in different styles , the responses of an sds should be versatile instead of invariable , planned responses . moreover , the is of dialogue turns can be affected by dialogue topics and speakers ' emotional_states . in this study , three base-level classifiers are employed for preliminary detection , including latent_dirichlet_allocation for dialogue topic categorization , support_vector_machine for prosody-based emotional_state identification and maximum_entropy for semantic label-based emotional_state identification . finally , an artificial_neural_network is adopted for is detection considering the scores estimated from the aforementioned classifiers . to evaluate the proposed approach , an sds in a chatting domain was constructed for evaluation . the performance of is detection can achieve 82.67 % . copyright © 2011 isca .
title : lstm network learning for sentiment_analysis ; abstract : the strong economic issues ( e-reputation , buzz detection ... ) and political ( opinion_leaders identification ... ) explain the rapid rise of scientists on the topic of sentiment_classification . sentiment_analysis focuses on the orientation of an opinion on an entity or its aspects . it determines its polarity which can be positive , neutral , or negative . sentiment_analysis is associated with texts classification problems . deep_learning ( machine_learning technique ) is based on multi-layer artificial_neural_networks . this technology has allowed scientists to make significant_progress in data recognition and classification . what makes deep_learning different from traditional_machine_learning_methods is that during complex analyses , the basic features of the treatment will no longer be identified by human treatment in a previous algorithm , but directly by the deep_learning . in this article we propose a twitter sentiment_analysis application using a deep_learning algorithm with lstm units adapted for natural_language_processing .
title : big_data , big noise : the challenge of finding issue networks on the web ; abstract : in this article , we focus on noise in the sense of irrelevant_information in a data set as a specific methodological challenge of web research in the era of big_data . we empirically_evaluate several methods for filtering hyperlink networks in order to reconstruct networks that contain only webpages that deal with a particular issue . the test corpus of webpages was collected from hyperlink networks on the issue of food_safety in the united_states and germany . we applied three filtering strategies and evaluated their performance to exclude irrelevant content from the networks : keyword filtering , automated document_classification with a machine-learning algorithm , and extraction of core networks with network-analytical measures . keyword filtering and automated classification of webpages were the most effective methods for reducing noise , whereas extracting a core_network did not yield satisfying results for this case .
title : sentiment mapping : point pattern analysis of sentiment classified twitter data ; abstract : detecting and monitoring collective public opinion via social_media platforms can provide real-time information to researchers and policymakers . human emotions , culture , and opinions can be tracked over time to understand where different sentiments manifest themselves geographically . expanding on existing methodology , the present study draws from sentiment_analysis and point pattern analysis to categorize and analyze sentiment toward natural_gas across the united_states as a means of applying these techniques together . three methods of machine_learning were used to classify collected_tweets into positive and negative categories : naïve_bayes , support_vector_machine , and logistic_regression . spatial clustering methods and spatial scan statistics were then applied to geocoded tweets to examine the distribution of sentiment about natural_gas . in this analysis , the logistic_regression and support_vector_machine methods outperformed naïve_bayes in classifying sentiment . the different methods produced not rather different classification results but also produced varying geographic results . the spatial analyses successfully indicated persistent patterns of negative and positive tweeting about natural_gas that correlate with expectations given the physical and cultural environment of various regions . further , the temporal variation of geographic hotspots of sentiment was readily apparent , suggesting that these approaches can reveal dynamic sentiment landscapes .
title : opinion-driven communities ' detection ; abstract : purpose - the purpose of this paper is to address the challenge of opinion_mining in text documents to perform further analysis such as community detection and consistency control . more specifically , we aim to identify and extract opinions from natural_language documents and to represent them in a structured manner to identify communities of opinion holders based on their common opinions . another goal is to rapidly identify similar or contradictory opinions on a target issued by different holders . design/methodology/approach - for the opinion_extraction problem we opted for a supervised approach focusing on the feature_selection problem to improve our classification results . on the community detection problem , we rely on the infomap community detection algorithm and the multi-scale community detection framework used on a graph representation based on the available opinions and social data . findings - the classification performance in terms of precision and recall was significantly_improved by adding a set of `` meta-features '' based on grouping rules of certain part of speech ( pos ) instead of the actual words . concerning the evaluation of the community detection feature , we have used two quality metrics : the network modularity and the normalized mutual_information ( nmi ) . we evaluated seven one-target similarity_functions and ten multi-target aggregation functions and concluded that linear functions perform_poorly for data sets with multiple targets , while functions that calculate the average similarity have greater resilience to noise . originality/value - although our solution relies on existing_approaches , we managed to adapt and integrate them in an efficient_manner . based on the initial experimental results obtained , we managed to integrate original enhancements to improve the performance of the obtained results .
title : a memory based approach to word_sense_disambiguation in bengali using k-nn method ; abstract : word_sense_disambiguation ( wsd ) is an important and challenging task in the area of natural_language_processing ( nlp ) where the task is to find the correct sense of an ambiguous_word given its context . there have been very few attempts on wsd in bengali or in indian languages . the k-nearest-neighbor ( k-nn ) algorithm is a very well known and popular method for text_classification . the k-nn algorithm determines the classification of a new sample from its k_nearest_neighbors . in this paper , we present how k-nn algorithm can be effectively applied to the task of wsd in bengali . the k-nn algorithm achieved an accuracy of over 71 % in a wsd task in bengali reported in this paper .
title : arabie text_classification using learning vector_quantization ; abstract : one of the several benefits of text_classification is to automatically assign document in predefined_category . researchers using lvq algorithm in english and persian [ 1 , 2 ] and do n't be attention for arabic_language . so in our research , we used neural_network approach for classify arabic text by using learning vector_quantization ( lvq ) algorithm . this algorithm is based on kohonen self_organizing_map ( som ) that is able to organize big-size document_collections according to textual similarities . also , lvq algorithm requires less training_examples and its faster than other classification methods . we select arabic documents from different domains . after that we select suitable preprocessing_methods such as term_weighting_schemes , and arabic morphological_analysis ( stemming and light stemming ) , these preprocessing prepared dataset that need for classification . then , we compared the results obtained from different lvq improvement versions ( lvq2.1 , lvq3 , olvq1 and olvq3 ) . the results showed that the lvq 's algorithms especially lvq2.1 algorithm achieved high accuracy and less time compared to other lvq 's algorithms . © 2012 cairo_university .
title : emotion based social_media text_classification using optimized improved id3 classifier ; abstract : emotion sensing is a complicated task of machine_learning technology . that belongs to the natural_language_processing ( nlp ) branch of artificial_intelligence ( ai ) . it is a broad domain of learning and analysis , the text_classification and their emotional orientation discovery is also part of this domain . in this work the emotion based text_classification is introduced . thus previously_developed data classification techniques are evaluated and a decision_tree based text_classification model is proposed . the key_issues in traditional id3 and its variants are to select optimal attribute from the training dataset for optimized learning . also the performances of the previously_developed algorithms are based on the size of training_set which is another key_issue . in this work the algorithm is designed such that its performance is almost independent of the size of the training_data ( size with minimum limit ) as well as it tries to select more optimal attribute for decision_tree . therefore to improve learning ability modification on the attribute_selection process is performed first . after improving the classifier 's learning ability , the modified model is used with the nlp parsing tool for analyzing the text sentiments . here the classification nature is binary classification . additionally the comparative performance is also measured with the traditional id3 algorithm and its variant classification algorithm . the experimental result based on parameters accuracy , error_rate shows that the proposed technique outperforms as compared to its similar techniques implemented and gives more accurate sentiment_orientations .
title : document class recognition using a support_vector_machine approach ; abstract : in most document archiving systems , one of the main fields is to identify the category of documents . in most case , determination of the document category in archiving tasks requires the application of classification model , which have had successes in improving documents processing . however , concerns exploding the frequency of use of documents in many office managers have driven increasing interests in document_image analysis ( dai ) system . an automated tool that can distinguish photographs , textual and mixed documents in heterogeneous dataset can be effective to reduce the complexity measure in archiving process . otherwise , instead of applying the same archiving strategy on the all documents dataset , a machine_vision system can be used for identifying only the archiving process which can be used for each input document . this paper examines the use of support_vector_machine ( svm ) algorithm for the helpful classification of documents in heterogeneous digital documents dataset . our purpose is to investigate if a sufficient classification rate can be achieved when svm is employed as a classification model in an automated document archiving system . in our experiments , a mixture of low-level features that characterizes documents in dataset was tested to build the aforementioned classification model . the obtained results reveal that the proposed svm model yields 96 % accuracy over a set of 250 test documents .
title : political leaning categorization by exploring subjectivities in political blogs ; abstract : this paper addresses a relatively new text_categorization problem : classifying a political blog as either 'liberal ' or 'conservative ' , based on its political leaning . instead of simply using `` bag of words '' features ( bow ) as in previous work , we have explored subjectivity manifested in blogs and used subjectivity information thus found to help build political leaning classifiers . specifically , our subjectivity based approach is two fold : 1 ) we identify subjective_sentences that contain at least two strong subjective clues based on the general inquirer dictionary ; 2 ) from subjective_sentences identified , we extract opinion expressions and bow features to build political leaning classifiers . experiments with a political blog corpus we built show that by using features from subjective_sentences can significantly_improve the classification performance . in addition , by extracting opinion expressions from subjective_sentences , we are able to reveal opinions that are characteristic of a specific political orientation to some extent .
title : predicting software defect severity_level using sentence_embedding and ensemble_learning ; abstract : bug_tracking is one of the prominent activities during the maintenance phase of software_development . the severity of the bug acts as a key indicator of its criticality and impact towards planning evolution and maintenance of various types of software products . this indicator measures how negatively the bug may affect the system functionality . this helps in determining how quickly the development teams need to address the bug for successful execution of the software system . due to a large number of bugs reported every day , the developers find it really difficult to assign the severity_level to bugs accurately . assigning incorrect severity_level results in delaying the bug resolution process . thus automated systems were developed which will assign a severity_level using various machine_learning techniques . in this work , five different types of sentence_embedding techniques have been applied on bugs description to convert the description comments to an n-dimensional_vector . these computed vectors are used as an input of the software defect severity_level prediction models and ensemble_techniques like bagging , random_forest classifier , extra_trees classifier , adaboost and gradient boosting have been used to train these models . we have also considered different variants of the synthetic_minority_oversampling_technique ( smote ) to handle the class_imbalance problem as the considered datasets are not evenly_distributed . the experimental results on six projects highlight that the usage of sentence_embedding , ensemble_techniques , and different variants of smote techniques helps in improving the predictive ability of defect severity_level prediction models .
title : explainable zero-shot topic extraction using a common-sense_knowledge graph ; abstract : pre-trained_word_embeddings constitute an essential building_block for many nlp systems and applications , notably when labeled_data is scarce . however , since they compress word_meanings into a fixed-dimensional representation , their use usually lack interpretability beyond a measure of similarity and linear analogies that do not always reflect real-world word relatedness , which can be important for many nlp applications . in this paper , we propose a model which extracts topics from text documents based on the common-sense_knowledge available in conceptnet [ 24 ] - a semantic concept graph that explicitly encodes real-world relations between words - and without any human supervision . when combining both conceptnet 's knowledge_graph and graph embeddings , our approach_outperforms other baselines in the zero-shot setting , while generating a human-understandable explanation for its predictions through the knowledge_graph . we study the importance of some modeling choices and criteria for designing the model , and we demonstrate that it can be used to label data for a supervised classifier to achieve an even better performance without relying on any humanly-annotated training_data . we publish the code of our approach at https : //github.com/d2klab/zeste and we provide a user_friendly demo at https : //zeste.tools.eurecom.fr/ .
title : source_code assessment and classification based on estimated error probability using attentive lstm language model and its application in programming education ; abstract : the rate of software_development has increased_dramatically . conventional compilers can not assess and detect all source_code errors . software may thus contain errors , negatively affecting end-users . it is also difficult to assess and detect source_code logic errors using traditional compilers , resulting in software that contains errors . amethod that utilizes artificial_intelligence for assessing and detecting errors and classifying source_code as correct ( error-free ) or incorrect is thus required . here , we propose a sequential language model that uses an attention-mechanism-based long short-term_memory ( lstm ) neural_network to assess and classify source_code based on the estimated error probability . the attentive mechanism enhances the accuracy of the proposed language model for error assessment and classification . we trained the proposed model using correct source_code and then evaluated its performance . the experimental results show that the proposed model has logic and syntax error_detection accuracies of 92.2 % and 94.8 % , respectively , outperforming state-of-the-art models . we also applied the proposed model to the classification of source_code with logic and syntax errors . the average precision , recall , and f-measure values for such classification are much better than those of benchmark models . to strengthen the proposed model , we combined the attention_mechanism with lstm to enhance the results of error assessment and detection as well as source_code classification . finally , our proposed model can be effective in programming education and software_engineering by improving code writing , debugging , error-correction , and reasoning .
title : a text feature_selection method using the improved mutual_information and information entropy ; abstract : mutual_information is a generally used evaluation function for feature_selection . but research showed that it might lead to the low classification_accuracy . to overcome the problem that the mutual_information is incline to selecting low-frequency words , this paper proposes a new feature evaluation function tfmiie for feature_selection , which combines the information entropy with the improved mutual_information . the improved mutual_information avoids to selecting the low-frequency unfamiliar words , and the entropy of feature favors to removing the feature words with unclear class properties . experimental results show that using tfmiie to select feature , and to repesent text and build classifiers can achieve better precision ratio and recall_ratio of text_classification with about 40 % increasing , which validated the proposed text feature_selection method using the improved mutual_information and the information entropy .
title : document_classification method with small training_data ; abstract : document_classification is one of important topics in the field of nlp ( natural_language_processing ) . in our previous_research we 've proposed a document_classification method which minimizes an error_rate with reference to a bayes criterion . but when the number of documents in training_data is small , the accuracy of the previous method is low . so in this research we propose a document_classification method whose accuracy is higher than the previous method when the number of documents in training_data is small . © 2009 sice .
title : generating american_sign_language animation : overcoming misconceptions and technical challenges ; abstract : misconceptions about the english literacy rates of deaf americans , the linguistic structure of american_sign_language ( asl ) , and the suitability of traditional machine_translation ( mt ) technology to asl have slowed the development of english-to-asl mt systems for use in accessibility applications . this article traces the progress of a new english-to-asl mt project targeted to translating texts important for literacy and user-interface applications . these texts include asl phenomena called `` classifier predicates . '' challenges in producing classifier predicates , novel solutions to these challenges , and applications of this technology to the design of user-interfaces accessible to deaf users will be discussed . ©_springer-verlag 2007 .
title : bi-directional recurrent neural ordinary_differential_equations for social_media text_classification ; abstract : classification of posts in social_media such as twitter is difficult due to the noisy and short nature of texts . sequence classification models based on recurrent_neural_networks ( rnn ) are popular for classifying posts that are sequential in nature . rnns assume the hidden_representation dynamics to evolve in a discrete manner and do not consider the exact time of the posting . in this work , we propose to use recurrent neural ordinary_differential_equations ( rnode ) for social_media post classification which consider the time of posting and allow the computation of hidden_representation to evolve in a time-sensitive continuous manner . in addition , we propose a novel model , bi-directional rnode ( bi-rnode ) , which can consider the information flow in both the forward and backward directions of posting times to predict the post label . our experiments demonstrate that rnode and bi-rnode are effective for the problem of stance_classification of rumours in social_media .
title : an improved knn_algorithm for text_classification ; abstract : this paper analyzes the advantages and disadvantages of knn alogrithm and introduces an improved knn alogrithm ( wpsokn ) for text c1assification.it is based on particle_swarm_optimization which has the ability of random and directed global search within training document set.during the procedure for searching k_nearest_neighbors of the test sample , those document_vectors that are impossible to be the k closest vectors are kicked out quickly.besides it reduces the impact of individual particles from the overall.moreover , the interference factor is introduced to avoid premature to find the k_nearest_neighbors of test samples quickly.we conducted an extensive experimental study using real datasets , and the results show that the wpsoknn algorithm is more efficient than other knn_algorithm . © 2010 ieee .
title : evaluation of different machine_learning approaches and input text_representations for multilingual classification of tweets for disease_surveillance in the social_web ; abstract : twitter and social_media as a whole have great_potential as a source of disease_surveillance data however the general messiness of tweets presents several challenges for standard information_extraction methods . most deployed systems employ approaches that rely on simple keyword_matching and do not distinguish between relevant and irrelevant keyword mentions making them susceptible to false_positives as a result of the fact that keyword volume can be influenced by several social phenomena that may be unrelated to disease occurrence . furthermore , most solutions are intended for a single language and those meant for multilingual scenarios do not incorporate semantic context . in this paper we experimentally examine different approaches for classifying text for epidemiological surveillance on the social_web in addition we offer a systematic comparison of the impact of different input representations on performance . specifically we compare continuous representations against one-hot encoding for word-based , class-based ( ontology-based ) and subword units in the form of byte pair encodings . we also go on to establish the desirable performance characteristics for multi-lingual semantic filtering approaches and offer an in-depth discussion of the implications for end-to-end surveillance .
title : improved language identification through cross-lingual self-supervised_learning ; abstract : language identification greatly impacts the success of downstream_tasks such as automatic_speech_recognition . recently , self-supervised speech representations learned by wav2vec_2.0 have been shown to be very effective for a range of speech tasks . we extend previous self-supervised work on language identification by experimenting with pre-trained models which were learned on real-world unconstrained speech in multiple_languages and not just on english . we show that models pre-trained on many languages perform better and enable language identification systems that require very little labeled_data to perform well . results on a 26 languages setup show that with only 10 minutes of labeled_data per language , a cross-lingually pre-trained model can achieve over 89.2 % accuracy .
title : sarcasm detection in twitter using sentiment_analysis ; abstract : designing efficient and robust algorithms for detection of sarcasm on twitter is the exciting challenge in opinion_mining field . sarcasm means the person speaks the contradictory of what the individual means , expressing gloomy feelings applying positive words . it helps the retailers to know the opinions of the customers . sarcasm is widely used in many social_networking and micro-blogging websites where people invade others which makes problematic for the individuals to say what it means . in the existing systems , logistic_regression technique is used to detect these sarcastic tweets , it has a drawback as it can not predict for continuous variables . in the proposed methodology sentiment_analysis , naive_bayes classification and adaboost algorithms are used to detect sarcasm on twitter . by using naive_bayes classification , the tweets are categorized into sarcastic and non-sarcastic . the adaboost_algorithm is used to make the weak statement to strong statements by iteratively considering the subset of training_data . sentiment_analysis is used to mine the opinions of customers to identify and extract information from the text . by using these two techniques , sarcastic statements can be easily classified and identified from twitter .
title : deep tensor evidence fusion network for sentiment_classification ; abstract : recently , a multimodal_sentiment_analysis of social_media has attracted_increasing_attention , and its core idea is to discovery heuristic fusion strategy to analyze the sentiment_orientations over heterogeneous multimodal source from a learned compact multimodal representation . the existing multimodal_fusion techniques not only struggle to achieve full heterogeneous data interaction , but also they are unable to dynamically assess the quality of various modal data to determine predictability . in this article , we present a novel deep tensor evidence fusion ( dtef ) network for multimodal_sentiment_classification . first , we propose a common view evaluation network that uses a long short-term_memory ( lstm ) network and a tensor-based neural_network to extract rich intermodal and intramodal information . then , we propose a unique time cue evaluation network that takes advantage of the temporal granularity associated with numerous pattern sequences . to make reliable decisions , we finally incorporate uncertainty through the trusted fusion layer , which improves the accuracy and robustness of sentimental classification . our model is validated using the cmu multimodal opinion sentiment and emotion intensity ( cmu-mosei ) and cmu multimodal corpus of sentiment_intensity ( cmu-mosi ) datasets , and the experimental findings demonstrate the superior performance of the proposed network in terms of accuracy compared with the state-of-the-art methods .
title : a novel method for fine-grained geolocation of wildlife activities by integrating geographical information and land use/cover in texts ; abstract : text data contain rich geographic_information . how to mine and spatialize the geographic_information embedded in text data through linking the geographic_location text with its spatial location in the real_world is fundamental for utilizing geographic_information . however , as the semantic granularity of geographic_location in texts is too raw to be directly used in most cases . it becomes a major challenge for geographic knowledge services to achieve fine-_grained geolocation of texts by effectively integrating geographical information with other related features such as land use/cover . the existing geolocation methods , including geocoding , place name retrieval , and fuzzy area modeling , have been widely used to decode non-urban geographic_location texts without considering land use/cover information . these methods usually failed to precisely extract geolocation in texts on wildlife activities . in this study , we proposed a fine-grained geolocation method through the inclusion of land use/ cover information in texts on wildlife activities . this method employed a natural_language spatial relationship approximation conversion model to determine a fine-grained geolocation domain by integrating geographically relevant_information ( including geographic_location entities , land use/cover entities , and spatial_relationships ) , fine classification of land use/cover , and coarse-grained matching geolocation . the coordinates of fine-grained geolocation were determined by iteratively searching and matching within the fine-grained geolocation domain by combining the natural_language form of land use/cover entities and fine land use/cover classification map . our experiments were conducted using texts information of the wild asian_elephants ' activities/accidents occurred in southern yunnan_province of china . the quality of geolocation in the experiments was evaluated using matching level and location accuracy . the results show that the method proposed here can soundly mine fine-grained geolocation of texts on wild asian_elephants ' activities/accidents . by mining and analyzing the texts on asian_elephants ' activities/accidents in an area with frequent human-elephant conflict in 2020 , fine-_grained geolocation of the examined asian_elephants ' activities/accidents was accurately extracted . compared with the domestic mainstream online geocoding and place name retrieval services with or without considering spatial_relationships , the proposed method greatly_improved the quality of fine-grained geolocation . the exact matching ratio of experimental location points reached to 81.51 % , and the mean value of the location error distance between location points and real points was 65.97 m , with a proportion of the location error distance below 50 m of 70.50 % . the significant outperformance of this method in mining and spatializing geographic_information of texts on asian_elephants ' activities/accidents sheds new light on wildlife monitoring and early_warning , and human-wildlife conflict emergency_management based on the fine-grained geolocation derived from multi-media texts on wildlife activities .
title : interpretable semantic textual similarity of sentences using alignment of chunks with classification and regression ; abstract : the proposed work is focused on establishing an interpretable semantic textual similarity ( ists ) method for a pair of sentences , which can clarify why two sentences are completely or partially similar or have some variations . this proposed interpretable approach is a pipeline of five modules that begins with the pre-processing and chunking of text . further chunks of two sentences are aligned using a one–to–multi ( 1 : m ) chunk aligner . thereafter , support_vector , gaussian_naive_bayes and k–nearest_neighbours classifiers are then used to create a multiclass_classification algorithm , and different class_labels are used to define an alignment type . at last , a multivariate regression algorithm is developed to find the semantic equivalence of an alignment with a score ( that ranges from 0 to 5 ) . the efficiency of the proposed method is verified on three different datasets and also compared to other state–of–the–art interpretable sts ( ists ) methods . the evaluated results show that the proposed method performs better than other ists methods . most importantly , the modules of the proposed ists method are used to develop a textual_entailment ( te ) method . it is found that , when we combined chunk level , alignment , and sentence_level features the entailment results significantly_improves .
title : self-attention-based convolutional_neural_networks for sentence_classification ; abstract : sentence_classification is a challenging task . the research on convolutional_neural_networks combined with the attention_mechanism for sentence_classification is not yet complete , especially the performance of multi-classification_tasks needs to be improved . in this paper , we propose a self-attention-based convolutional_neural_network ( sacnn ) for sentence_classification , which consists of two self-attention layers and a convolutional_neural_network . we conducted multiple experiments on seven benchmark_datasets . experimental results show that the proposed model can achieve up to 0.4 % -1.4 % higher_accuracy than other cnn-based models , and outperform other cnn-based models on five out of seven tasks .
title : detection of text quality flaws as a one-class classification problem ; abstract : for web_applications that are based on user_generated content the detection of text quality flaws is a key concern . our research contributes to automatic quality flaw detection . in particular , we propose to cast the detection of text quality flaws as a one-class classification problem : we are given only positive_examples ( = texts containing a particular quality flaw ) and decide whether or not an unseen text suffers from this flaw . we argue that common binary or multiclass_classification approaches are ineffective in here , and we underpin our approach by a real-world application : we employ a dedicated one-class learning approach to determine whether a given wikipedia article suffers from certain quality flaws . since in the wikipedia setting the acquisition of sensible test data is quite intricate , we analyze the effects of a biased sample selection . in addition , we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown ( real-world ) flaw-specific class imbalances . altogether , provided test data with little noise , four from ten important quality flaws in wikipedia can be detected with a precision close to 1 . © 2011 acm .
title : a novel feature_selection and extraction technique for classification ; abstract : pattern_recognition is a vast field which has seen significant advances over the years . as the datasets under consideration grow larger and more comprehensive , using efficient techniques to process them becomes increasingly important . we present a versatile technique for the purpose of feature_selection and extraction - class dependent features ( cdfs ) . cdfs identify the features innate to a class and extract them accordingly . the features thus extracted are relevant to the entire class and not just to the individual data item . this paper focuses on using cdfs to improve the accuracy of classification and at the same time control computational expense by tackling the curse of dimensionality . in order to demonstrate the generality of this technique , it is applied to two problem statements which have very little in common with each other - handwritten_digit_recognition and text_categorization . it is found that for both problem statements , the accuracy is comparable to state-of-the-art results and the speed of the operation is considerably greater . results are presented for reuters-21578 and web-kb datasets relating to text_categorization and the mnist and usps datasets for handwritten_digit_recognition .
title : automatic semantic text tagging on historical lexica by combining ocr and typography classification a case_study on daniel sander ’ s wörterbuch der deutschen sprache ; abstract : when converting historical lexica into electronic form the goal is not only to obtain a high quality ocr result for the text but also to perform a precise automatic recognition of typographical attributes in order to capture the logical structure . for that purpose , we present a method that enables a fine-grained typography classification by training an open_source ocr engine both on traditional ocr and typography recognition and show how to map the obtained typography information to the ocr recognized text output . as a test_case , we used a german dictionary ( sander ’ s wörterbuch der deutschen sprache ) from the 19th century , which comprises a particularly complex semantic function of typography . despite the very challenging material , we achieved a character_error_rate below 0.4 % and a typography recognition that assigns the correct label to close to 99 % of the words . in contrast to many existing_methods , our novel approach works with real historical data and can deal with frequent typography changes even within lines .
title : a tensor space model-based deep_neural_network for text_classification ; abstract : most text_classification systems use machine_learning algorithms ; among these , naïve_bayes and support_vector_machine algorithms adapted to handle text data afford reasonable_performance . recently , given developments in deep_learning technology , several scholars have used deep_neural_networks ( recurrent and convolutional_neural_networks ) to improve text_classification . however , deep_learning-based text_classification has not greatly_improved performance compared to that of conventional algorithms . this is because a textual document is essentially expressed as a vector ( only ) , albeit with word dimensions , which compromises the inherent semantic information , even if the vector is ( appropriately ) transformed to add conceptual information . to solve this ‘ loss of term senses ’ problem , we develop a concept-driven deep_neural_network based upon our semantic tensor space model . the semantic tensor used for text_representation features a dependency between the term and the concept ; we use this to develop three deep_neural_networks for text_classification . we perform experiments using three standard document corpora , and we show that our proposed methods are superior to both traditional and more recent learning methods .
title : an ordinal multi-class classification method for readability_assessment of chinese documents ; abstract : readability_assessment is worthwhile in recommending suitable documents for the readers . in this paper , we propose an ordinal multi-class classification with voting ( omcv ) method for estimating the reading levels of chinese documents . based on current achievements of natural_language_processing , we also design five groups of text features to explore the peculiarities of chinese . we collect the chinese primary_school language textbook dataset , and conduct_experiments to demonstrate the effectiveness of both the method and the features . experimental results show that our method has potential in improving the performance of the state-of-the-art classification and regression models , and the designed features are valuable in readability_assessment of chinese documents .
title : on the road to speed-reading and fast learning with conceptum ; abstract : this work introduces conceptum , an advanced knowledge discovery system for speed-reading natural_language texts and allowing faster and more effective learning . conceptum sports a huge plethora of features , ranging from language detection and conceptualization , up to semantic categorization , named_entity_recognition and automatic ontology building , effectively turning an unstructured textual source into concepts , topics , relationships and summaries to quickly and easily browse it and classify it . the system does not require any training or configuration and at present can be applied as-is on general-purpose english and italian texts , providing disparate kinds of users with a powerful means to significantly speed up and improve their learning and research activities . in this work , a challenging experimentation on the biochemistry field is reported to highlight and discuss the arising critical issues in the application of the system on a highly-technical domain .
title : korean text_categorization using the character tv-gram ; abstract : we previously proposed the accumulation method , a language-independent text_classification method that is based on the character n-gram , and classified english and japanese text documents . the accumulation method does not depend on the language structure , because it uses the character n-gram to form index_terms . if text documents are expressed in unicode , the accumulation method can classify the documents using the same algorithm . in the present paper , we improve the proposed method and classify korean text documents , which are newspaper articles from the korean hankyoreh 2008 data set . as a result , the highest macro-averaged f-measure of the proposed method is 90.2 % for the korean hankyoreh 2008 data set . in this way , we obtain good results for korean . in addition , we demonstrate the improvement in classification_accuracy for english . finally , we consider points of qualitative meaning of the accumulation method .
title : dual-state capsule_networks for text_classification ; abstract : text_classification systems based on contextual_embeddings are not viable options for many of the low_resource_languages . on the other hand , recently introduced capsule_networks have shown performance in par with these text_classification models . thus , they could be considered as a viable alternative for text_classification for languages that do not have pre-trained contextual embedding models . however , current capsule_networks depend upon spatial patterns without considering the sequential features of the text . they are also sub-optimal in capturing the context-level information in longer sequences . this paper presents a novel dual-state capsule ( ds-caps ) network-based technique for text_classification , which is optimized to mitigate these issues . two varieties of states , namely sentence-level and word-level , are integrated with capsule layers to capture deeper context-level information for language modeling . the dynamic_routing process among capsules was also optimized using the context-level information obtained through sentence-level states . the ds-caps networks outperform the existing capsule_network architectures for multiple datasets , particularly for tasks with longer sequences of text . we also demonstrate the superiority of ds-caps in text_classification for a low_resource_language .
title : empirical_study to evaluate the performance of classification_algorithms on public datasets ; abstract : in today ’ s world , a huge amount of data is stored in the form of electronic documents in the world_wide_web . text_classification algorithms have been widely used for classifying those text documents into a fixed number of predefined_classes . the applicable scopes and their performances of these algorithms are different . therefore , finding an appropriate algorithm for a dataset is becoming a significant emphasis for researchers to solve practical problems quickly . this paper puts_forward an experimental evaluation of five significant text_classification algorithms with each other and with tf and tf-idf feature_selection methods built using decision_tree ( c5.0 ) , support_vector_machine , k-nearest_neighbor , naïve_bayes , and neural_network on four public datasets , namely 20news-bydate , ohsumed-first-20000-docs , reuters 21578-apte-90 cat , and 20_newsgroup . the experimental results are examined from multiple perspectives and summarized to provide usefulness of different algorithms on different datasets .
title : multi-part representation_learning for cross-domain web_content classification using neural_networks ; abstract : owing to the tremendous_increase in the volume and variety of user_generated content , train-once-apply-forever models are insufficient for supervised_learning tasks . the need is to develop algorithms that can adapt across domains by leveraging labeled_data from source_domain ( s ) and efficiently perform the task in the unlabeled_target_domain . towards this , we present a novel two-stage neural_network learning algorithm for domain_adaptation which learns a multi-part hidden_layer where individual parts contribute differently to the tasks in source and target domains . the multiple parts of the representation ( i.e . hidden_layer ) are learned while being cognizant of what characteristics to transfer across domains and what to preserve within domains for enhanced performance . the first stage embroils around learning a two-part representation i.e . source specific and common representations in a manner such that the former do not detract the ability of the later to represent the target domain . in the second stage , the generalized common representation is further iteratively extended with discriminating target specific characteristics to adapt to the target domain . we empirically_demonstrate that the learned representations , in difierent arrangements , outperform existing domain_adaptation algorithms in the source classification as well as the cross-domain classification_tasks on the user_generated content from different domains on the web .
title : a comparative_analysis on medical article classification using text_mining & amp ; machine_learning algorithms ; abstract : the document_classification task is one of the widely_studied research fields on multiple_domains . the core motivation of the classification_task is that the manual classification efforts are impractical due to the exponentially growing document volumes . thus , we densely need to exploit automated computational_approaches , such as machine_learning models along with data & text_mining techniques . in this study , we concentrated on the classification of medical articles specifically on common cancer types , due to the significance of the field and the decent number of available documents of interest . we deliberately targeted medline articles about common cancer types because most cancer types share a similar literature composition . therefore , this situation makes the classification effort relatively more complicated . to this end , we built multiple machine_learning models , including both traditional and deep_learning architectures . we achieved the best performance ( r¿82 % f score ) by the lstm model . overall , our results demonstrate a strong effect of exploiting both text_mining and machine_learning methods to distinguish medical articles on common cancer types .
title : solving stance_detection on tweets as multi-domain and multi-task text_classification ; abstract : stance_detection on tweets aims at classifying the attitude of tweets towards given targets . existing work leverage attention-based models to learn target-aware stance representations . while those methods achieve substantial success , most of them usually train a model for each target separately despite the scarcity of annotated_data for each target . to alleviate limitation of annotated_data , some methods turn to external linguistic resources , additional sentiment annotations or target-aware data_augmentation techniques for better detection results . we argue that the sharedness of stance-related features across targets in the existing stance_detection dataset is not fully exploited . however , directly training on mixed examples of all targets may confuse the model in learning best features for each target . to this end , we borrow the idea from transfer_learning and multi-task_learning , and formulate stance_detection on tweets as a multi-domain multi-task classification problem . we apply the target adversarial_learning to capture stance-related features shared by all targets and target descriptors for learning stance-informative_features correlating to specific targets . experimental results on the benchmark semeval_2016 dataset demonstrate the effectiveness of our model , which outperforms bert model by over 2 % on macro_average_f1 and achieves superior performance than many recent methods utilizing external_resources . we further provide detailed analyses to illustrate the superiority of fully utilizing features shared by different targets .
title : enhance multiword_expressions extraction and description with transformational rules ; abstract : this paper presents a methodology to extract and describe verbal multiword_expressions using their transformational behavior . several objectives are targeted : 1 ) automatically extracting mwe and especially frozen expression , 2 ) describing linguistically their mwe behavior , 3 ) comparing statistical_methods and our approach , and finally 4 ) showing the importance of mwe in a text_classification tool .
title : refining word_embeddings using domain-specific knowledge for drug reviews sentiment_classification ; abstract : with the development of internet technologies , more users are sharing and seeking health-related information in online medical forums about medications and treatments and their contents are usually in subjective nature . patient-generated drug reviews are valuable and useful textual_contents which have not been researched largely by researchers in the natural_language_processing area . analyzing drug reviews can assist to improve the pharmacovigilance systems . we propose a refining technique for traditional word_embedding using domain-specific knowledge for the drug reviews sentiment_classification . the domain-specific lexicon generated from the drug reviews corpus is applied to refine traditional word_embeddings in the feature_extraction process of sentiment_classification . we evaluate our proposed method on the publicly available drug review datasets . according to the experimental results , the proposed method outperformed refined word_embeddings using domain-independent lexicon in terms of accuracy in sentiment_classification of drug reviews . it indicates the significance of domain_knowledge in sentiment_analysis of medical domain .
title : latent target-opinion as prior for document-level sentiment_classification : a variational approach from fine-grained perspective ; abstract : existing_works for document-level sentiment_classification task treat the review document as an overall text unit , performing feature_extraction with various sophisticated model architectures . in this paper , we draw inspiration from fine-grained sentiment_analysis , proposing to first learn the latent target-opinion distribution behind the documents , and then leverage such fine-grained prior_knowledge into the classification process . we model the latent target-opinion distribution as hierarchical variables , where global-level variable captures the overall target and opinion , and local-level variables retrieve the detailed opinion clues at the word_level . the proposed method consists of two main parts : a variational module and a classification module . we employ the conditional variational_autoencoder to make reconstructions of the document , during which the user and product information can be integrated . in the classification module , we build a hierarchical model based on transformer encoders , where the local-level and global-level prior distribution representations induced from the variational module are injected into the word-level and sentence-level transformers , respectively . experimental results on benchmark_datasets show that the proposed method significantly_outperforms strong_baselines , achieving the state-of-the-art performance . further analysis shows that our model is capable of capturing the latent fine-grained target and opinion prior information , which is highly effective for improving the task performance .
title : software functional and non-function requirement classification using word-embedding ; abstract : the classification of software requirements is an essential task in software_engineering . manual classification requires a large amount of efforts , time and cost . hence , automated techniques are required to classify software requirements . this work aims to develop requirement classification models based on extraction of relevant features from requirement documents and thereafter classifying requirement into functional and non-function requirements . in this paper , different word-_embedding techniques to extract numerical_features , feature_selection to remove irrelevant feature , smote to balance data , and six different classifiers for models training . the experiments have been conducted on promise software_engineering dataset . the experimental finding indicate that word2vec is best way to extracting numerical_features from requirement documents , rank-sum test is best way to find important features , and svm-r was found as the best classifier .
title : effect of training_set size on svm and naïve_bayes for twitter sentiment_analysis ; abstract : twitter sentiment_analysis has become an effective way in measuring public sentiment about a certain topic or product . thus , researchers have worked extensively in recent_years to build efficient models for sentiment_classification . in this paper , we will measure the effect of varying the training_set size on the classification_accuracy and f-score of svm and naive_bayes classifiers . we will expand our study even further by forming two ensembles : ensemble 1 and ensemble 2 . both ensembles include a single naive_bayes and svm classifier , but the ensembles differ in terms of the decision fusion technique utilized . ensemble 1 uses 'and-type ' fusion while ensemble 2 uses 'or-type ' fusion . in this paper , we measure the effect of training_set size on each ensemble configuration type by measuring their f-scores and classification_accuracies while varying the training_set size .
title : gl-lstm model for multi-label_text_classification of cardiovascular_disease reports ; abstract : in recent_years , the rapid_growth of electronic data and information has gotten a lot of attention to find relevant knowledge such as textual_information . the goal of automatic text_classification is to automatically predict textual articles classes , especially in the medical domain . however , for some applications , the used data must inherently be described by more than one label . in this research , a new scheme of medical multi-label_text_classification is investigated which is based on intelligent engineering features using glove technique and lstm classifier . the main particularity of glove permits the extraction of informative_features to the word_level automatically and capture the global and local textual semantics . the choice of the lstm model is motivated by the success that has been achieved by taking into account the very long-term dependencies between words . the experiment of our approach named gl-lstm based on ohsumed cardiovascular text dataset has produced impressive_results with an overall accuracy of 0.927 compared with related_works existing in the literature
title : sentiment_analysis for informal malay text in social_commerce ; abstract : sentiment_analysis ( sa ) is opinion_mining which often defines as the study of emotions , opinions , or feedback that relates to the usage of computational_linguistics , text_analytics , and natural_language_processing . with the rise of social_media posts , it is becoming more challenging to evaluate brief , casual , and non-structured texts to optimize consumer feedback and spot patterns . meanwhile , social_commerce involves social_media for social_interaction in assisting customers and merchants to do business transactions . from a social_media perspective , the informal malay text is less explored by the researchers . thus , it will directly yield difficulties in conducting and preparing the sa processes . cross-industry standard process for data_mining ( crisp-dm ) was adapted as a reference model for the methodology of this work with machine_learning approaches in classifying the informal malay textual_data based on sentiment . the dataset was extracted from the facebook platform of pos laju malaysia pages . the comparison of the classification technique performances was analyzed in identifying the most accurate classifier for sa , within three different machine_learning classifiers was experimented by using 1200 instances from an informal malay textual dataset . the results of decision_tree ( j48 ) , support_vector_machine ( svm ) , and naïve_bayes ( nb ) were analyzed and discussed . the result of the highest_accuracy of ten-fold_cross-validation is 69.7 % and meanwhile , for the percentage split method , the highest_accuracy result is 70.9 % . it shows that support_vector_machine ( svm ) is the best classifier compared to other classifiers of text_classification based on sentiment .
title : learning and knowledge-based sentiment_analysis in movie review key excerpts ; abstract : we propose a data-driven approach based on back-off n-grams and support_vector_machines , which have recently become popular in the fields of sentiment and emotion recognition . in addition , we introduce a novel valence classifier based on linguistic analysis and the on-line knowledge sources conceptnet , general inquirer , and wordnet . as special benefit , this approach does not demand labeled_training_data . moreover , we show how such knowledge sources can be leveraged to reduce out-of-vocabulary events in learning-based processing . to profit from both of the two generally different concepts and independent knowledge sources , we employ information fusion techniques to combine their strengths , which ultimately leads to better overall performance . finally , we extend the data-driven classifier to solve a regression problem in order to obtain a more fine-grained resolution of valence . © 2011 springer .
title : research on text_categorization based on the fusion of k-nn and svm ; abstract : this paper presents a new fusion algorithm for automatic_text_categorization based on the combination of the improved k-nn and svm . the new algorithm analyses the reliability of classifier with cla ( classifier 's local accuracy ) technique for the combination of these two methods . the improved k-nn introduces text cluster to describe inside text structure and the improved svm uses sigmoid_function to transform the output of svm to a probability value . the experimental results prove that the new algorithm combines the merits of the two algorithms and performs better with fewer candidates and higher_precision .
title : bootstrapped multi-level distant_supervision for relation_extraction ; abstract : distant_supervised relation_extraction has been widely used to identify new relation_facts from free text . however , relying on a single-node categorization model to identify relation_facts for thousands of relations simultaneously inevitably accompanies with serious false categorization problem . also to the best of our knowledge , no previous efforts has yet considered to update the categorization model with the new identified relation_facts , which wastes the chance to further improsve the extraction precision and recall . in this paper , we novelly propose a multi-level distant_supervision model for relation_extraction , which divides the original categorization task into a number of sub-tasks in multiple levels of a constructed tree-like categorization structure . with the tree-like structure , an unlabelled relation instance would be categorized step by step along a path from the root node to a leaf node . beyond that , we propose to do bootstrapped distant_supervision to update the distant_supervision model with new learned relation_facts iteratively to further improve the extraction precision and recall . experimental results conducted on two real datasets prove that our approach_outperforms state-of-the-art approaches by reaching more than 10 % better extraction quality .
title : research on the comment text_classification based on transfer_learning ; abstract : in the traditional review text_classification method , in order to realize the high accuracy of classification model , there are two basic premises : ( 1 ) training_data and test data must be distributed independently and uniformly ; ( 2 ) there must be enough training_data to learn a good classification model . however , in many cases , these two premises are not true . if a classification model already exists and classifies data from a domain well , then a classification_task for a related domain exists , but only data from the source_domain , then it may violate this assumption . the comment text_classification method based on transfer_learning refers to applying the classification knowledge learned in the source_domain to the new classification_task in the relevant field by using the transfer_learning method in the process of classifying the comment text . therefore , after constructing the isomorphic feature_space of source_domain and target domain , the tradaboost migration learning framework was used to train the classification model . this model allows users to leverage old data with a small amount of new markup data to build a high-quality classification model for new data . experimental results show that the model can effectively transfer classification knowledge from source_domain to target domain .
title : research on chinese text_classification based on word2vec ; abstract : the set of features which the traditional feature_selection algorithm of chi-square selected is not complete . this causes the low performance for the final text_classification . therefore , this paper proposes a method . the method utilizes word2vec to generate word_vector to improve feature_selection algorithm of the chi_square . the algorithm applies the word_vector generated by word2vec to the process of the traditional feature_selection and uses these words to supplement the set of features as appropriate . ultimately , the set of features obtained by this method has better discriminatory power . because , the feature words with the better discriminatory power has the strong ability of distinguishing categories as its semantically_similar words . on this base , multiple experiments have been carried out in this paper . the experimental results show that the performance of text_classification can increase after extension of feature words .
title : prosoul : a framework to identify propaganda from online urdu content ; abstract : today , the rapid dissemination of information on digital platforms has seen the emergence of information pollution such as misinformation , disinformation , fake_news , and different types of propaganda . information pollution has become a serious threat to the online digital world and has posed several challenges to social_media platforms and governments around theworld . in this article , we propose propaganda spotting in online urdu_language ( prosoul ) - a framework to identify content and sources of propaganda spread in the urdu_language . first , we develop a labelled_dataset of 11,574 urdu news to train the machine_learning classifiers . next , we develop the linguistic_inquiry_and_word_count ( liwc ) dictionary to extract psycho-linguistic features of urdu text . we evaluate the performance of different classifiers by varying ngram , news landscape ( nela ) , word2vec , and bidirectional_encoder_representations_from_transformers ( bert ) features . our results show that the combination of nela , word n-gram , and character n-gram features outperform with 0.91 accuracy for urdu text_classification . in addition , word2vec_embedding outperforms bert features in classification of the urdu text with 0.87 accuracy . moreover , we develop and classify large_scale urdu content repositories to identify web_sources spreading propaganda . our results show that prosoul framework performs best for propaganda detection in the online urdu news content compared to the general web_content . to the best of our knowledge , this is the first study on the detection of propaganda content in the urdu_language .
title : myanmar to english verb translation disambiguation approach based on naïve_bayesian classifier ; abstract : natural_language_processing ( nlp ) is a field of computer science and linguistics concerned with the interactions between computers and human ( natural ) languages . ambiguity is one of these problems which have been a great challenge for computational linguists . this paper concentrates on the problem of target word selection in myanmar to english machine_translation , for which the approach is directly applicable . however this system can only solve the ambiguities of verbs in myanmar-english translations . this paper presents a corpus-based approach to word_sense_disambiguation that builds an ensemble of naïve_bayesian classifiers , each of which based on lexical_features . moreover nouns are detail classified in our system . in this paper , we propose a framework to solve ambiguous verb problems . our system will support to improve the accuracy of the myanmar to english translation . © 2011 ieee .
title : commodity text_classification based e-commerce category and attribute mining ; abstract : this paper proposes a commodity text_classification-based e-commerce category and attribute mining method . the fasttext is utilized to classify the commodity text data and establish the first-level and leaf category mapping between two different e-commerce platforms . based on the category mapping , the system can mine the attributes and attributes ' values that the target leaf category does not have , while its corresponding leaf category from another platform does . additionally , the longest_common subsequence ( lcs ) algorithm is utilized to calculate the attribute similarity . the experiments on two different e-commerce platforms show that the proposed method not only can help the e-commerce platform establish and improve the category architecture but also enhances user_experience .
title : deep_learning methods with pre-trained_word_embeddings and pre-trained_transformers for extreme_multi-label_text_classification ; abstract : in recent_years , there has been a considerable increase in textual_documents online . this increase requires the creation of highly improved machine_learning methods to classify text in many different domains . the effectiveness of these machine_learning methods depends on the model capacity to understand the complex nature of the unstructured_data and the relations of features that exist . many different machine_learning methods were proposed for a long time to solve text_classification problems , such as svm , knn , and rocchio classification . these shallow_learning methods have achieved doubtless success in many different domains . for big and unstructured_data like text , deep_learning methods which can learn representations and features from the input data wtihout using any feature_extraction methods have shown to be one of the major solutions . in this study , we explore the accuracy of recent recommended deep_learning methods for multi-label_text_classification starting with simple rnn , cnn models to pretrained_transformer models . we evaluated these methods ' performances by computing multi-label evaluation_metrics and compared the results with the previous_studies .
title : margin maximization with feed-forward neural_networks : a comparative study with svm and adaboost ; abstract : feed-forward neural_networks ( fnn ) and support_vector_machines ( svm ) are two machine_learning frameworks developed from very different starting points of view . in this work a new learning model for fnn is proposed such that , in the linearly_separable case , it tends to obtain the same solution as svm . the key idea of the model is a_weighting of the sum-of-squares error_function , which is inspired by the adaboost_algorithm . as in svm , the hardness of the margin can be controlled , so that this model can be also used for the non-linearly_separable case . in addition , it is not restricted to the use of kernel_functions , and it allows to deal with multiclass and multilabel problems as fnn usually do . finally , it is independent of the particular algorithm used to minimize the error_function . theoretic and experimental results on synthetic and real-world problems are shown to confirm these claims . several empirical comparisons among this new model , svm , and adaboost have been made in order to study the agreement between the predictions made by the respective classifiers . additionally , the results obtained show that similar performance does not imply similar predictions , suggesting that different models can be combined leading to better performance . © 2003 elsevier b.v. all rights_reserved .
title : bert-based nlp techniques for classification and severity modeling in basic warranty data study ; abstract : this paper is to explore data-driven models based on a newly_developed natural_language_processing ( nlp ) tool called bidirectional_encoder_representations_from_transformer ( bert ) to incorporate textural data information for group classification and loss amount prediction on truck 's basic warranty claims . in group classification modeling , multiple-class logistic_regression is compared with bert-based back-propagation neural_networks ( nn ) . in group loss severity modeling , direct nn regression is compared with bert-based nn regression prediction . furthermore , based on the results from a so-called optimal bin-width algorithm , the severity distribution is fitted in gamma and its parameters are then estimated using maximum_likelihood_estimation ( mle ) . the data experiments show that the bert framework for nlp improves the classification of the warranty claims as well as the loss severity prediction both in accuracy and stability .
title : region-based discriminative feature pooling for scene_text_recognition ; abstract : we present a new feature_representation method for scene_text_recognition problem , particularly focusing on improving scene character_recognition . many existing_methods rely on histogram of oriented gradient ( hog ) or part-based models , which do not span the feature_space well for characters in natural_scene_images , especially given large variation in fonts with cluttered backgrounds . in this work , we propose a discriminative feature pooling method that automatically learns the most informative sub-regions of each scene character within a multi-class classification framework , whereas each sub-region seamlessly integrates a set of low-level image features through integral images . the proposed feature_representation is compact , computationally_efficient , and able to effectively model distinctive spatial structures of each individual character_class . extensive_experiments conducted on challenging datasets ( chars74k , icdar'03 , icdar'11 , svt ) show that our method significantly_outperforms existing_methods on scene character classification and scene_text_recognition tasks .
title : real-time investors ’ sentiment_analysis from newspaper articles ; abstract : recently , investor_sentiment measures have become one of the more widely examined areas in behavioral_finance . they are capable of both explaining and forecasting stock returns . the purpose of this paper is to present a method , based on a combination of a naïve_bayes classifier and the n-gram probabilistic language model , which can create a sentiment index for specific stocks and indices of the new york stock_exchange . an economic useful proxy for investor_sentiment is constructed from u.s. news articles mainly provided by the new york times . initially , a large amount of articles for ten big companies and indices is collected and processed , in order to be able to extract a sentiment_score from each one of them . then , the classifier is trained from the positive , negative and neutral articles , so that it is possible afterwards to examine the sentiment of any unseen newspaper article , for any company or index . subsequently , the classification_task is tested and validated for its accuracy and efficiency . the widely used baker and wurgler sentiment index [ 2 ] is used as a comparison measure for predicting stock returns . in a sample of s_&_p 500 index from 2004 to 2010 on monthly basis , it is shown that the new sentiment index created has , on average , twice the predictive ability of baker and wurgler ’ s index , for the existing time frame .
title : compositional sentence_representation from character within large context text ; abstract : this paper describes a hierarchical composition recurrent_network ( hcrn ) consisting of a 3-level hierarchy of compositional models : character , word and sentence . this model is designed to overcome two problems of representing a sentence on the basis of a constituent word_sequence . the first is a data-sparsity problem in word_embedding , and the other is a no usage of inter-sentence dependency . in the hcrn , word_representations are built from characters , thus resolving the data-sparsity problem , and inter-sentence dependency is embedded into sentence_representation at the level of sentence composition . we adopt a hierarchy-wise learning scheme in order to alleviate the optimization difficulties of learning deep hierarchical recurrent_network in end-to-end fashion . the hcrn was quantitatively and qualitatively evaluated on a dialogue_act_classification task . especially , sentence_representations with an inter-sentence dependency are able to capture both implicit and explicit semantics of sentence , significantly improving performance . in the end , the hcrn achieved state-of-the-art performance with a test error_rate of 22.7 % for dialogue_act_classification on the swbd-damsl database .
title : sentiment_analysis of serious suicide references in twitter social_network ; abstract : sentiment_analysis analyzes people emotions , attitudes , and opinion towards organizations , services , issues , and individuals . opinions are the core of almost all human activities because they consider a significant influencers of our behaviors . with the growing popularity of social_media applications ( micro-blogs , twitter , comments , etc ) , users of these platforms express their emotions through their posts and comments . suicide is one of these dangerous emotions that threaten the public_health of canadians , and mortality form suicide is the third leading cause of death in teenage . in this paper , we propose a suicide classifier system called auto twitter suicide detector system ( atsds ) that provides support to authorities to take appropriate actions in order to protect communities from such kind of thoughts . the proposed twitter suicide detector system is a classifier system using data gathered from twitter to detect those related to suicide . our system is built using deep_neural_network on multi-purpose cluster computing system called spark . in order to asses the system performance , in terms of accuracy , we have conducted several experiments and tuned neural_network parameters to achieve higher performance . the results returned are very promising .
title : debiasing embeddings for reduced gender bias in text_classification ; abstract : ( bolukbasi et al. , 2016 ) demonstrated that pretrained_word_embeddings can inherit gender bias from the data they were trained on . we investigate how this bias affects downstream_classification_tasks , using the case_study of occupation classification ( de-arteaga et al. , 2019 ) . we show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information . with a relatively minor adjustment , however , we show how these same techniques can be used to simultaneously reduce bias and maintain high classification_accuracy .
title : multi-resolution interpretation and diagnostics tool for natural_language classifiers ; abstract : developing explainability methods for natural_language_processing ( nlp ) models is a challenging task , for two main reasons . first , the high dimensionality of the data ( large number of tokens ) results in low coverage and in turn small contributions for the top tokens , compared to the overall model performance . second , owing to their textual nature , the input variables , after appropriate transformations , are effectively binary ( presence or absence of a token in an observation ) , making the input-output relationship difficult to understand . common nlp interpretation techniques do not have flexibility in resolution , because they usually operate at word-level and provide fully local ( message_level ) or fully global ( over all messages ) summaries . the goal of this paper is to create more flexible model explainability summaries by segments of observation or clusters of words that are semantically_related to each other . in addition , we introduce a root cause analysis method for nlp models , by analyzing representative false_positive and false_negative examples from different segments . at the end , we illustrate , using a yelp review data set with three segments ( restaurant , hotel , and beauty ) , that exploiting group/cluster structures in words and/or messages can aid in the interpretation of decisions made by nlp models and can be utilized to assess the model 's sensitivity or bias towards gender , syntax , and word_meanings .
title : sentiment_analysis during the covid-19_pandemic as a tool for exploring psychosocial expression in cyberspace ; abstract : this work presents the brazilian population ’ s sentiment_analysis during the covid-19_pandemic , using robert plutchik ’ s wheel of emotions classifications . the study ended by checking the application feasibility regarding the monitoring of social_media while exploring the psychosocial expression of cyberspace . after data preparation , there were assessed the best results and the supervised_learning algorithms . real data was the source for this work , with near 150,000 tweets , between march and october 2020 . a total of 4491 tweets were manually_labeled for training and test classification , in a proportion of 75 % being used for training and 25 % for testing . the results showed a precision of up to 0.78 , having a recall between 0.56 and 0.82 and f1-score ranging from 0.57 to 0.80 . considering the results of populational sentiments , varying minimally on tweets , given the presence of covid , while citing the brazilian_army , no causal relationship has been demonstrated between the general sentiments of the public in brazil and that related to the brazilian_army .
title : a comparison of stylometric and lexical_features for web genre classification and emotion classification in blogs ; abstract : in the blogosphere , the amount of digital content is expanding and for search_engines , new challenges have been imposed . due to the changing information need , automatic methods are needed to support blog search users to filter information by different facets . in our work , we aim to support blog search with genre and facet information . since we focus on the news genre , our approach is to classify blogs into news versus_rest . also , we assess the emotionality facet in news related blogs to enable users to identify people 's feelings towards specific events . our approach is to evaluate the performance of text classifiers with lexical and stylometric_features to determine the best performing combination for our tasks . our experiments on a subset of the trec blogs08 dataset reveal that classifiers trained on lexical_features perform consistently better than classifiers trained on the best stylometric_features . © 2010 ieee .
title : attend , copy , parse -- end-to-end information_extraction from documents ; abstract : document information_extraction tasks performed by humans create data consisting of a pdf or document_image input , and extracted string outputs . this end-to-end data is naturally consumed and produced when performing the task because it is valuable in and of itself . it is naturally available , at no additional cost . unfortunately , state-of-the-art word classification methods for information_extraction can not use this data , instead requiring word-level labels which are expensive to create and consequently not available for many real_life tasks . in this paper we propose the attend , copy , parse architecture , a deep_neural_network model that can be trained directly on end-to-end data , bypassing the need for word-level labels . we evaluate the proposed architecture on a large diverse set of invoices , and outperform a state-of-the-art production system based on word classification . we believe our proposed architecture can be used on many real_life information_extraction tasks where word classification can not be used due to a lack of the required word-level labels .
title : feature_selection method based on simplified information gain of id3 ; abstract : at present , feature_selection is a core research topic in text_categorization . it firstly analyzed several classic_feature_selection methods and summarized their deficiencies , and combined word_frequency with document_frequency and presented an optimal document_frequency method . and then it analyzed shortcomings of information gain in id3 and introduced attribute dependence to improve information gain . next , according to the character of information gain , it simplified information gain to reduce the computing complexity by the convex_function . finally , it combined the simplified information gain with the optimal document_frequency method and proposed a comprehensive feature_selection method . the comprehensive method firstly uses the optimal document_frequency method to select features to reduce the sparsity of feature_spaces , and then uses to the simplified information gain to select features again , so can acquire the feature_subsets which are more representative . the experimental results show that the combined method is promising .
title : evolving dictionary based sentiment_scoring framework for patient authored text ; abstract : in recent days , the government and other organizations are focusing on providing better health_care to people . understanding the patients experience of care-received is key for providing better health_care . with prevailing usage of social_media applications , patients are expressing their experience over social_media . this patient authored text is a free-unstructured_data which is available over social_media in large chunks . to extract the sentiments from this huge data , a domain-specific dictionary is required to get better accuracy . the proposed approach defines a new domain-specific dictionary and uses this in sentiment_scoring to enhance the overall sentiment_classification on patient authored text . we conducted experiments on the proposed approach using nhs choices dataset and compared it with popular classifiers like linear_regression , stochastic_gradient_descent , dictionary-based approaches : vader and afinn . the results prove that the proposed approach is an effective strategy for sentiment_analysis over patient authored text which helps in improving the classification_accuracy .
title : relational teacher student learning with neural label embedding for device adaptation in acoustic_scene_classification ; abstract : in this paper , we propose a domain_adaptation framework to address the device mismatch issue in acoustic_scene_classification leveraging upon neural label embedding ( nle ) and relational teacher student learning ( rtsl ) . taking into account the structural relationships between acoustic scene classes , our proposed framework captures such relationships which are intrinsically device-independent . in the training stage , transferable knowledge is condensed in nle from the source_domain . next in the adaptation stage , a novel rtsl strategy is adopted to learn adapted target models without using paired source-target data often required in conventional teacher student learning . the proposed framework is evaluated on the dcase 2018 task1b data set . experimental results based on alexnet-l deep classification models confirm the effectiveness of our proposed approach for mismatch situations . nle-alone adaptation compares favourably with the conventional device adaptation and teacher student based adaptation techniques . nle with rtsl further improves the classification_accuracy .
title : bag-of-concepts document_representation for bayesian text_classification ; abstract : the classification of text documents into a number of pre-defined categories has many application scenarios , for example the classification of news items into thematic sections . documents to be classified are commonly represented by a bag-of-words feature_vector . the bag-of-words model can not handle two language phenomena : synonymy_and_polysemy , besides , dimensions of feature_vectors are orthogonal . in order to effectively address those problems , some researchers adopt a bag-of-concepts representation of documents - understanding concept as 'unit of meaning '' . this paper reports a comprehensive experimental evaluation of the efficiency of a bag-of-concepts representation for bayesian text_classification , tackling synonymy_and_polysemy , and exploiting semantic relatedness between concepts to alleviate the problem of orthogonality - following an approach that we call semantic expansion . results of experiments performed on three corpora widely used as benchmarks - reuters , ohsumed , and 20newsgroups - show that : the efficiency of the bag-of-concepts approach is very dependent on the capacity of the semantic annotator for extracting concepts and on the characteristics of particular corpora , peaking on ohsumed , and that it performs especially well when the number of training_samples is small . in particular , for the shorter training sequence bag-of-concepts outperforms bag-of-words by 43.67 % in ohsumed and 22.44 % in 20newsgroups . this work provides useful insights to researchers that aim at applying bag-of-concepts representations , for example for organizing scientific_articles in accordance with their thematic .
title : phrase2vec : phrase embedding based on parsing ; abstract : text is one of the most common unstructured_data , and usually , the most primary task in text_mining is to transfer the text into a structured_representation . however , the existing text_representation models split the complete semantic unit and neglect the order of words , finally lead to understanding bias . in this paper , we propose a novel phrase-based text_representation method that takes into account the integrity of semantic units and utilizes vectors to represent the similarity relationship between texts . first , we propose hpmbp ( hierarchical phrase mining based on parsing ) which mines hierarchical phrases by parsing and uses bop ( bag of phrases ) to represent text . then , we put forward three phrase embedding models , called phrase2vec , including skip-phrase , cbop ( continuous bag of phrases ) , and glovefp ( global_vectors for phrase representation ) . they learn the phrase vector with semantic similarity , further obtain the vector representation of the text . based on phrase2vec , we propose petc ( phrase embedding based text_classification ) and petclu ( phrase embedding based text clustering ) . petc utilizes the phrase embedding to get the text vector , which is fed to a neural_network for text_classification . petclu gets the vectorization expression of text and cluster_center by phrase2vec , furthermore extends the k-means model for text clustering . to the best of our knowledge , it is the first work that focuses on the phrase-based english text_representation . experiments show that the introduced phrase2vec outperforms state-of-the-art phrase embedding models in the similarity task and the analogical reasoning task on enwiki , dblp , and yelp dataset . petc is superior to the baseline text_classification methods in the f1-value index by about 4 % . petclu is also ahead of the prevalent text clustering methods in entropy and purity indicators . in summary , phrase2vec is a promising approach to text_mining .
title : batterydataextractor : battery-aware text-mining software embedded with bert models ; abstract : due to the massive_growth of scientific_publications , literature mining is becoming increasingly_popular for researchers to thoroughly explore scientific text and extract such data to create new databases or augment existing databases . efforts in literature-mining software_design and implementation have improved text-mining productivity , but most of the toolkits that mine text are based on traditional_machine-learning-algorithms which hinder the performance of downstream text-mining tasks . natural-language_processing ( nlp ) and text-mining technologies have seen a rapid development since the release of transformer models , such as bidirectional_encoder_representations_from_transformers ( bert ) . upgrading rule-based or machine-learning-based literature-mining toolkits by embedding transformer models into the software is therefore likely to improve their text-mining performance . to this end , we release a python-based literature-mining toolkit for the field of battery materials , batterydataextractor , which involves the embedding of batterybert models in its automated data-extraction pipeline . this pipeline employs bert models for token-classification tasks , such as abbreviation detection , part-of-speech_tagging , and chemical-named-entity_recognition , as well as new double-turn question-answering data-extraction models for auto-generating repositories of inter-related material and property data as well as general information . we demonstrate that batterydataextractor exhibits state-of-the-art performance on the evaluation data sets for both token_classification and automated data extraction . to aid the use of batterydataextractor , its code is provided as open-source_software , with associated documentation to serve as a user guide .
title : a novel statistical feature_selection approach for text_categorization ; abstract : for text_categorization task , distinctive text features selection is important due to feature_space high dimensionality . it is important to decrease the feature_space dimension to decrease processing time and increase accuracy . in the current study , for text_categorization task , we introduce a novel statistical feature_selection approach . this approach measures the term_distribution in all collection documents , the term_distribution in a certain category and the term_distribution in a certain class relative to other classes . the proposed method results show its superiority over the traditional feature_selection methods .
title : parameter-efficient tuning by manipulating hidden_states of pretrained_language_models for classification_tasks ; abstract : parameter-efficient tuning aims to distill knowledge for downstream_tasks by optimizing a few introduced parameters while freezing the pretrained_language_models ( plms ) . continuous prompt_tuning which prepends a few trainable vectors to the embeddings of input is one of these methods and has drawn much attention due to its effectiveness and efficiency . this family of methods can be illustrated as exerting nonlinear transformations of hidden_states inside plms . however , a natural question is ignored : can the hidden_states be directly used for classification without changing them ? in this paper , we aim to answer this question by proposing a simple tuning method which only introduces three trainable vectors . firstly , we integrate all layers hidden_states using the introduced vectors . and then , we input the integrated hidden_state ( s ) to a task-specific linear_classifier to predict categories . this scheme is similar to the way elmo utilises hidden_states except that they feed the hidden_states to lstm-based models . although our proposed tuning scheme is simple , it achieves comparable performance with prompt_tuning methods like p-tuning and p-tuning v2 , verifying that original hidden_states do contain useful information for classification_tasks . moreover , our method has an advantage over prompt_tuning in terms of time and the number of parameters .
title : deep_learning-based resolution prediction of software enhancement_reports ; abstract : the automatic resolution prediction of newly submitted enhancement_reports is an important task during the bug_triage process . it can help developers automatically predict the resolution status of enhancement_reports . the resolution prediction is still a manual process which is very time-consuming , costly , and laborious . to help software applications for the timely implementation of enhancement_reports , we introduce a deep_learning-based technique to predict the resolution of newly submitted enhancement_reports automatically by using a summary and description of enhancement_reports . we use word2vec and a deep-learning-based classifier that can learn the deep syntactical and semantical relationship between the words of enhancement_reports . we use additional novel features from enhancement_reports and customized tokenizer to save useful features . experimental results show the proposed approach enhances the performance as compared to state-of-the-art approaches in resolution prediction and has an effective ability to predict the resolution status of enhancement_reports .
title : same side stance_classification task : facilitating argument stance_classification by fine-tuning a bert model ; abstract : research on computational argumentation is currently being intensively investigated . the goal of this community is to find the best pro and con arguments for a user given topic either to form an opinion for oneself , or to persuade others to adopt a certain standpoint . while existing argument_mining methods can find appropriate arguments for a topic , a correct classification into pro and con is not yet reliable . the same side stance_classification task provides a dataset of argument pairs classified by whether or not both arguments share the same stance and does not need to distinguish between topic-specific pro and con vocabulary but only the argument similarity within a stance needs to be assessed . the results of our contribution to the task are build on a setup based on the bert architecture . we fine-tuned a pre-trained bert model for three epochs and used the first 512 tokens of each argument to predict if two arguments share the same stance .
title : semantic partitioning and machine_learning in sentiment_analysis ; abstract : this paper investigates sentiment_analysis in arabic tweets that have the presence of jordanian dialect . a new dataset was collected during the coronavirus disease ( covid-19 ) pandemic . we demonstrate two models : the traditional arabic_language ( tal ) model and the semantic partitioning arabic_language ( spal ) model to envisage the polarity of the collected_tweets by invoking several , well-known classifiers . the extraction and allocation of numerous arabic features , such as lexical_features , writing_style features , grammatical_features , and emotional features , have been used to analyze and classify the collected_tweets semantically . the partitioning concept was performed on the original dataset by utilizing the hidden semantic meaning between tweets in the spal model before invoking various classifiers . the experimentation reveals that the overall performance of the spal model competes over and better than the performance of the tal model due to imposing the genuine idea of semantic partitioning on the collected dataset .
title : a component clustering_algorithm based on semantic similarity and optimization ; abstract : to overcome the subjective factors of faceted_classification representation , the method combined the faceted_classification with text retrieval is used to describe the components . meanwhile , from the semantic view and combined optimization techniques , a component clustering_algorithm based on semantic similarity and optimization is proposed . this algorithm can reduce the subjective factors of faceted_classification , and further improve the efficiency and accuracy of component search . and compared with component clustering effect based on vector_space_model , the experiments_prove that this component clustering_algorithm based on semantic similarity and optimization is effective which can improve the result of component clustering and raise the clustering quality . © 2010 ieee .
title : relation_classification with entity_type restriction ; abstract : relation_classification aims to predict a relation between two entities in a sentence . the existing_methods regard all relations as the candidate relations for the two entities in a sentence . these methods neglect the restrictions on candidate relations by entity_types , which leads to some inappropriate relations being candidate relations . in this paper , we propose a novel paradigm , relation_classification with entity_type restriction ( recent ) , which exploits entity_types to restrict candidate relations . specially , the mutual restrictions of relations and entity_types are formalized and introduced into relation_classification . besides , the proposed paradigm , recent , is model-agnostic . based on two representative models gcn and spanbert respectively , recent_gcn and recent_spanbert are trained in recent . experimental results on a standard dataset indicate that recent improves the performance of gcn and spanbert by 6.9 and 4.4 f1 points , respectively . especially , recent_spanbert achieves a new state-of-the-art on tacred .
title : a classical approach to handcrafted_feature extraction techniques for bangla handwritten_digit_recognition ; abstract : bangla handwritten_digit_recognition is a significant step forward in the development of bangla ocr . however , intricate shape , structural likeness and distinctive composition style of bangla digits makes it relatively challenging to distinguish . thus , in this paper , we benchmarked four rigorous classifiers to recognize bangla handwritten_digit : k-nearest_neighbor ( knn ) , support_vector_machine ( svm ) , random_forest ( rf ) , and gradient-boosted decision_trees ( gbdt ) based on three handcrafted_feature extraction techniques : histogram of oriented gradients ( hog ) , local_binary_pattern ( lbp ) , and gabor filter on four publicly available bangla handwriting digits datasets : numtadb , cmartdb , ekush and bdrw . here , handcrafted_feature extraction methods are used to extract features from the dataset image , which are then utilized to train machine_learning classifiers to identify bangla handwritten_digits . we further fine-tuned the hyperparameters of the classification_algorithms in order to acquire the finest bangla handwritten_digits recognition performance from these algorithms , and among all the models we employed , the hog features combined with svm model ( hog+svm ) attained the best performance_metrics across all datasets . the recognition_accuracy of the hog+svm method on the numtadb , cmartdb , ekush and bdrw datasets reached 93.32 % , 98.08 % , 95.68 % and 89.68 % , respectively as well as we compared the model performance with recent state-of-art methods .
title : sentiment_analysis on bengali text using lexicon based approach ; abstract : in this modern_era , we daily involve in the internet strongly . we express our opinion about products , services , books , movies , songs , politics , sports , organizations , etc . through the internet in social_media , blogs , micro-blogging websites or any media . public opinion with bengali text in internet media is increasing very rapidly . due to a few works in bengali text sentiment_analysis , it has become an important issue of extracting opinions , emotions from bengali textual_data through sentiment_analysis ( sa ) for better knowledge extraction . sentiment_analysis ( sa ) is effectively used for classifying the opinion expressed in a text according to its polarity ( e.g. , positive , negative or neutral ) . this paper represents a lexicon dictionary-based approach for polarity_detection of bengali text data . we compared our proposed model with machine_learning classifiers such as decision_tree ( dt ) , naive_bayes ( nb ) and support_vector_machine ( svm ) classifiers and it works as a much better accurate model for bengali text polarity_detection .
title : a new feature_selection algorithm based on category difference for text_categorization ; abstract : the feature_selection is an important step which can reduce the dimensionality and improve the performance of the classifiers in text_categorization . many popular feature_selection methods do not consider the difference in the distribution of different categories on a feature . in this paper , we propose a new filter_based_feature_selection algorithm , namely fused distance feature_selection ( fdfs ) , which evaluates the significance of a feature by taking account of the difference in the distribution of different categories and selects more discriminative_features with the minimal number . the proposed algorithm is investigated both inside and outside perspectives on four benchmark document datasets , 20-newsgroups , webkb , csdmc2010 and ohsumed , using linear_support_vector machine ( lsvm ) and multinomial_naïve_bayes ( mnb ) classifiers . the experimental results indicate that our proposed method provides a competitive result , where its average ranking is 1.25 on lsvm and 1 on mnb .
title : massive : a 1m-example multilingual natural_language_understanding dataset with 51 typologically-diverse languages ; abstract : we present the massive dataset -- multilingual amazon slu resource package ( slurp ) for slot-filling , intent_classification , and virtual_assistant evaluation . massive contains 1m realistic , parallel , labeled virtual_assistant utterances spanning 51 languages , 18 domains , 60 intents , and 55 slots . massive was created by tasking professional translators to localize the english-only slurp dataset into 50 typologically diverse languages from 29 genera . we also present modeling results on xlm-r and mt5 , including exact_match accuracy , intent_classification accuracy , and slot-filling f1 score . we have released our dataset , modeling code , and models publicly .
title : classifying ethics codes using natural_language_processing ; abstract : business ethics scholars have varied opinions of corporate ethics codes . many advocate them as a way to contribute to an organizational environment in which ethics will be a regular consideration in the decision-making process . critics assert that codes of ethics are mere window dressings written to protect the company from litigation or to comply with regulations . the authors maintain that language is a key to distinguishing between these two properties and an aid to how employees and other stakeholders should view a code ’ s intent . however , language is often ambiguous to the reader and results of research on ethics codes are often in conflict . this chapter addresses the issue of intent by quantifying the content of ethics codes . methodologies from natural_language_processing ( nlp ) and machine_learning are applied in a novel way to classify ethics codes . codes from companies selected from lists of “ ethical ” companies are compared with codes from the fortune_500 companies . the model ’ s findings indicate that ethics codes for some of these groups of companies can be distinguishable .
title : performance comparison of machine_learning classifiers for fake_news_detection ; abstract : information sharing on the web particularly via web-based networking media is increasing . ability to identify , evaluate and address such information is significantly important . fake information deliberately created is purposefully or unintentionally engendered over the internet . this is affecting a larger group of society who are blinded by technology . this paper illustrates model and methodology to detect fake_news from news article with the assistance of machine_learning and natural_language_processing . in this proposed work different feature_engineering methods like count vector , tf-idf and word_embedding are used to generate feature_vector . seven different machine_learning classification_algorithms are trained to classify news as fake or real and are compared considering accuracy , f1 score , recall , precision and best one is selected to build a model to classify news as fake or real .
title : beyond supervised_learning of wrappers for extracting information from unseen web_sites ; abstract : we investigate the problem of wrapper adaptation which aims at adapting a previously learned wrapper to an unseen target site . to achieve this goal , we make use of extraction rules previously discovered from a particular site to seek potential candidates of training_examples for the target site . we pose the problem of training example identification for the target site as a hybrid text_classification problem . the idea is to use a classification model to capture the characteristics of the attribute item of interests . based on the automatically annotated training_examples , a new wrapper for the unseen target web_site can then be discovered . we present encouraging experimental results on wrapper adaptation for some real-world web_sites . ©_springer-verlag 2003 .
title : document zone content classification for technical document_images using artificial_neural_networks and support_vector_machines ; abstract : artificial_neural_networks ( ann ) are a classic pattern classifier and widely applicable to various problems and are relatively easy to use . three of the most popular anns are multilayer_perceptron ( mlp ) with backpropagation learning algorithm , self_organizing_map ( som ) and recurrent_neural_network ( rnn ) . support_vector_machines ( svm ) have gained great interest in the last few years in pattern_recognition . thus , this research compares the recognition performance of text and non-text images ( text , table , figure and graph ) from technical document_images based on the pixel intensity of various zones between bpnn , som , rnn and svm . symmetrical and non-symmetrical zoning algorithms were compared as input . 400 different datasets have been tested and the experiments indicate that svm classification is superior to the other three classifiers . the experiments also indicate that the combination of symmetrical and non-symmetrical zoning design is better than non-symmetrical or symmetrical zoning only . ©2009 ieee .
title : classification technique of interviewer-bot result using naïve_bayes and phrase reinforcement algorithms ; abstract : in recent_years , both foreign and national companies tend to conduct english-based interviews when recruiting new employees . consequently , college graduate must be ready for english-based interviews during the process of seeking employment . to meet these requirements potential candidates tend to practice conversing in english with someone who is proficient in the language . nevertheless , it is not easy to have someone who is not only proficient in english , but also have a good understanding of common interview questions . this paper presents the development of a machine which is able to provide practice on english-based interviews , specifically on job interviews . interviewer machine ( interviewer bot ) is expected to help students practice speaking english appropriately for job_interview . the interviewer machine design uses words from a chat bot database named alice to mimic human_intelligence that can be applied to a search_engine using aiml . naïve_bayes algorithm is used to classify the interview results into three categories : potential , talent and interest students . furthermore , based on the classification result , the summary is made at the end of the interview session by using phrase reinforcement algorithms . by using this bot , students are expected to practice their listening and speaking skills , also to be familiar with the questions often asked in job interviews so that they can prepare the proper answers . in addition , the bot users could know their potential , talent and prospects in finding a job . hence , they could apply to the appropriate companies . based on the validation results of 50 respondents , the accuracy degree of interviewer chat-bot ( interviewer engine ) response obtained 86.93 % .
title : classifying cancer pathology reports with hierarchical self-attention networks ; abstract : we introduce a deep_learning architecture , hierarchical self-attention networks ( hisans ) , designed for classifying pathology reports and show how its unique architecture leads to a new state-of-the-art in accuracy , faster_training , and clear interpretability . we evaluate performance on a corpus of 374,899 pathology reports obtained from the national_cancer_institute 's ( nci ) surveillance , epidemiology , and end results ( seer ) program . each pathology report is associated with five clinical classification_tasks – site , laterality , behavior , histology , and grade . we compare the performance of the hisan against other machine_learning and deep_learning approaches commonly used on medical text data – naive_bayes , logistic_regression , convolutional_neural_networks , and hierarchical_attention_networks ( the previous state-of-the-art ) . we show that hisans are superior to other machine_learning and deep_learning text classifiers in both accuracy and macro f-score across all five classification_tasks . compared to the previous state-of-the-art , hierarchical_attention_networks , hisans not only are an order of magnitude faster to train , but also achieve about 1 % better relative accuracy and 5 % better relative macro f-score .
title : hyperbolic centroid calculations for text_classification ; abstract : a new development in nlp is the construction of hyperbolic word_embeddings . as opposed to their euclidean counterparts , hyperbolic embeddings are represented not by vectors , but by points in hyperbolic_space . this makes the most common basic scheme for constructing document_representations , namely the averaging of word_vectors , meaningless in the hyperbolic setting . we reinterpret the vector mean as the centroid of the points represented by the vectors , and investigate various hyperbolic centroid schemes and their effectiveness at text_classification .
title : new naive_bayes text_classification algorithm ; abstract : according to the phenomena that the calculation of prior_probability in text_classification is time-consuming and has little effect on the classification result , and the accuracy loss of posterior_probability affects the accuracy of classification , the classical naive_bayes algorithm is improved and a new text_classification algorithm is proposed which restrains the effect of prior_probability and amplifies the effect of posterior_probability . in the new algorithm , the calculation of prior_probability is removed and an amplification factor is added to the calculation of posterior_probability . the experiments_prove that removing the calculation of prior_probability in text_classification can accelerate the classification speed and has little effect on the classification_accuracy , and adding an amplification factor in the calculation of posterior_probability can reduce the effect of error_propagation and improve the classification_accuracy .
title : sequence-based word_embeddings for effective text_classification ; abstract : in this work we present dive ( distance-based vector embedding ) , a new word_embedding technique based on the logistic markov embedding ( lme ) . first , we generalize lme to consider different distance_metrics and address existing scalability issues using negative_sampling , thus making dive scalable for large_datasets . in order to evaluate the quality of word_embeddings produced by dive , we used them to train standard machine_learning classifiers , with the goal of performing different natural_language_processing ( nlp ) tasks . our experiments demonstrated that dive is able to outperform existing ( more complex ) machine_learning approaches , while preserving simplicity and scalability .
title : polite dialogue generation without parallel data ; abstract : stylistic dialogue response_generation , with valuable applications in personality-based conversational_agents , is a challenging task because the response needs to be fluent , contextually-relevant , as well as paralinguistically accurate . moreover , parallel datasets for regular-to-stylistic pairs are usually unavailable . we present three weakly-supervised models that can generate diverse polite ( or rude ) dialogue responses without parallel data . our late_fusion model ( fusion ) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances . our label-fine-tuning ( lft ) model prepends to each source sequence a politeness-score scaled label ( predicted by our state-of-the-art politeness classifier ) during training , and at test time is able to generate polite , neutral , and rude responses by simply scaling the label embedding by the corresponding score . our reinforcement_learning model ( polite-rl ) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response . we also present two retrieval-based polite dialogue model baselines . human evaluation validates that while the fusion and the retrieval-based models achieve politeness with poorer context-relevance , the lft and polite-rl models can produce significantly more polite responses without sacrificing dialogue quality .
title : olex : effective rule learning for text_categorization ; abstract : this paper describes olex , a novel method for the automatic induction of rule-based text classifiers . olex supports a hypothesis language of the form `` if t1 or ⋯ or tn occurs in document d , and none of tn+1⋯tn + m occurs in d , then classify d under category c , where each ti is a conjunction of terms . the proposed method is simple and elegant . despite this , the results of a systematic experimentation performed on the reuters-21578 , the ohsumed , and the odp data collections show that olex provides classifiers that are accurate , compact , and comprehensible . a comparative_analysis conducted against some of the most well-known learning algorithms ( namely , naive_bayes , ripper , c4.5 , svm , and linear logistic_regression ) demonstrates that it is more than competitive in terms of both predictive_accuracy and efficiency . © 2006 ieee .
title : scalable text and link_analysis with mixed-topic link models ; abstract : many data sets contain rich_information about objects , as well as pairwise relations between them . for instance , in networks of websites , scientific_papers , and other documents , each node has content consisting of a collection of words , as well as hyperlinks or citations to other nodes . in order to perform inference on such data sets , and make predictions and recommendations , it is useful to have models that are able to capture the processes which generate the text at each node and the links between them . in this paper , we combine classic ideas in topic_modeling with a variant of the mixed-membership block model recently developed in the statistical_physics community . the resulting model has the advantage that its parameters , including the mixture of topics of each document and the resulting overlapping communities , can be inferred with a simple and scalable expectation-maximization_algorithm . we test our model on three data sets , performing unsupervised topic classification and link_prediction . for both tasks , our model outperforms several existing state-of-the-art methods , achieving higher_accuracy with significantly less computation , analyzing a data set with 1.3 million_words and 44 thousand links in a few minutes .
title : based on rough_sets and the associated analysis of knn text_classification research ; abstract : with the rapid development of network information_technology , the text is as a basic information carrier and begins to present exponential_growth . the existing text_classification methods have n't got information from the vast amounts of information resources timely and accurately . in order to solve the problem , the paper puts_forward a new method about text_categorization . it is a knn_algorithm based on rough_set and correlation_analysis . firstly , we introduce the concept of rough_set . in the training_set of text vector_space , we divide all kinds of text vector_spaces into certain and uncertain areas . for certain areas , we can directly judge its category . for uncertain areas , we determine the type of text vector through knn text_classification algorithm based on correlation_analysis . experimental results show that the knn text_classification algorithm based on rough_sets and the associated analysis have greatly_improved the efficiency and accuracy of text_categorization . it can meet the requirements of processing large amounts of text data .
title : a document_representation framework with interpretable features using pre-trained_word_embeddings ; abstract : we propose an improved framework for document_representation using word_embeddings . the existing models represent the document as a position vector in the same word_embedding space . as a result , they are unable to capture the multiple_aspects as well as the broad context in the document . also , due to their low representational power , existing_approaches perform_poorly at document_classification . furthermore , the document_vectors obtained using such methods have uninterpretable features . in this paper , we propose an improved document_representation framework which captures multiple_aspects of the document with interpretable features . in this framework , a document is represented in a different feature_space by representing each dimension with a potential feature word with relatively high discriminating_power . a given document is modeled as the distances between the feature words and the document . to represent a document , we have proposed two criteria for the selection of potential feature words and a distance_function to measure the distance between the feature word and the document . experimental results on multiple datasets show that the proposed model consistently performs better at document_classification over the baseline_methods . the proposed approach is simple and represents the document with interpretable word features . overall , the proposed model provides an alternative framework to represent the larger text units with word_embeddings and provides the scope to develop new approaches to improve the performance of document_representation and its applications .
title : an exploratory_study of tweets about the sars-cov-2 omicron variant : insights from sentiment_analysis , language interpretation , source tracking , type classification , and embedded url detection ; abstract : this paper presents the findings of an exploratory_study on the continuously generating big_data on twitter related to the sharing of information , news , views , opinions , ideas , feedback , and experiences about the covid-19_pandemic , with a specific focus on the omicron variant , which is the globally dominant variant of sars-cov-2 at this time . a total of 12028 tweets about the omicron variant were studied , and the specific characteristics of tweets that were analyzed include - sentiment , language , source , type , and embedded urls . the findings of this study are manifold . first , from sentiment_analysis , it was observed that 50.5 % of tweets had a neutral emotion . the other emotions - bad , good , terrible , and great were found in 15.6 % , 14.0 % , 12.5 % , and 7.5 % of the tweets , respectively . second , the findings of language interpretation showed that 65.9 % of the tweets were posted in english . it was followed by spanish , french , italian , and other languages . third , the findings from source tracking showed that twitter for android was associated with 35.2 % of tweets . it was followed by twitter web app , twitter for iphone , twitter for ipad , and other sources . fourth , studying the type of tweets revealed that retweets accounted for 60.8 % of the tweets , it was followed by original tweets and replies that accounted for 19.8 % and 19.4 % of the tweets , respectively . fifth , in terms of embedded url analysis , the most common domain embedded in the tweets was found to be twitter.com , which was followed by biorxiv.org , nature.com , and other domains . finally , to support similar research in this field , we have developed a twitter dataset that comprises more than 500,000 tweets about the sars-cov-2 omicron variant since the first detected case of this variant on november 24 , 2021 .
title : multi-domain active_learning for text_classification ; abstract : active_learning has been proven to be effective in reducing labeling_efforts for supervised_learning . however , existing active_learning work has mainly focused on training models for a single domain . in practical_applications , it is common to simultaneously train classifiers for multiple_domains . for example , some merchant web_sites ( like amazon.com ) may need a set of classifiers to predict the sentiment_polarity of product_reviews collected from various domains ( e.g. , electronics , books , shoes ) . though different domains have their own unique features , they may share some common latent features . if we apply active_learning on each domain separately , some data instances selected from different domains may contain duplicate knowledge due to the common features . therefore , how to choose the data from multiple_domains to label is crucial to further reducing the human labeling_efforts in multi-domain learning . in this paper , we propose a novel multi-domain active_learning framework to jointly select data instances from all domains with duplicate information considered . in our solution , a shared subspace is first learned to represent common latent features of different domains . by considering the common and the domain-specific features together , the model loss reduction induced by each data instance can be decomposed into a common part and a domain-specific part . in this way , the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying . we compare our method with the state-of-the-art active_learning approaches on several text_classification tasks : sentiment_classification , newsgroup classification and email_spam filtering . the experiment results show that our method reduces the human labeling_efforts by 33.2 % , 42.9 % and 68.7 % on the three tasks , respectively . © 2012 acm .
title : predicting sentiment toward transportation in social_media using visual and textual_features ; abstract : social_media platforms can be used by transportation agencies to receive feedback from their customers , thus creating two-way communication between the service_provider and its consumers . sentiment_analysis is one method of aggregating overall polarity ( positive or negative ) towards a topic . however , most sentiment_analysis methods rely on text processing , thus ignoring the large amount of image data present in popular social_networks . the primary aim of this study is to exploit image data in conjunction with text and to evaluate this integrated approach for sentiment_analysis for transportation . this study used image , captions , and comments data from the instagram social_network that were marked as being relevant to california_department_of_transportation ( caltrans ) and attempted to predict the expressed sentiment towards this agency . a set of high-level features were extracted from images using the web-based microsoft cognitive services apis . these features included the detection of faces and 86 categories which describe the images . text features included the set of individual words and structural_features . the experiment results of different machine_learning techniques show a gain in precision when images and texts are combined compared to text-only approaches , thus confirming the relevance of visual_content usage . the precision reaches a performance close to human classification agreement ( typically approximately 80 % ) . however , the results do not indicate that visual_features are more informative than text features .
title : simulated_annealing based classifier_ensemble techniques : application to part_of_speech_tagging ; abstract : part-of-speech ( pos ) tagging is an important pipelined module for almost all natural_language_processing ( nlp ) application_areas . in this paper we formulate pos_tagging within the frameworks of single and multiobjective_optimization techniques . at the very first step we propose a classifier_ensemble technique for pos_tagging using the concept of single objective optimization ( soo ) that exploits the search capability of simulated_annealing ( sa ) . thereafter we devise a method based on multiobjective_optimization ( moo ) to solve the same problem , and for this a recently developed multiobjective simulated_annealing based technique , amosa , is used . the characteristic features of amosa are its concepts of the amount of domination and archive in simulated_annealing , and situation specific acceptance probabilities . we use conditional_random_field ( crf ) and support_vector_machine ( svm ) as the underlying classification methods that make use of a diverse set of features , mostly based on local contexts and orthographic constructs . we evaluate our proposed approaches for two indian languages , namely bengali and hindi . evaluation results of the single objective version shows the overall accuracy of 88.92 % for bengali and 87.67 % for hindi . themoobased ensemble yields the overall accuracies of 90.45 % and 89.88 % for bengali and hindi , respectively . ©_2012_elsevier_b.v .
title : image_classification by latent_topic model with dirichlet processes ; abstract : this paper addresses the problem of accurately classifying image categories without any human interaction . in current latent_topic model , the relationship between different categories is not considered . in fact the parts of different categories may have similar properties . a shared parts latent_topic model with dirichlet process is presented to share mixture_components between categories . different categories share the similar parts which make the model more accurate . as the number of components is unknown and is to be inferred from the train set , the dirichlet process is introduced into the model to provide a nonparametric prior for the number of mixture_components within each category . the object shape feature modeled by wishart_distribution is adopted in the model and gibbs sampler is applied to estimate the parameters . a number of classification experiments are used to verify the success of our model . copyright © 2011 binary information press .
title : a review on sentiment_classification : natural_language_understanding ; abstract : background : with the tremendous_increase in the use of social_networking sites for sharing the emotions , views , preferences etc . a huge volume of data and text is available on the internet , there comes the need for understanding the text and analysing the data to determine the exact intent behind the same for a greater good . this process of understanding the text and data involves loads of analytical methods , several phases and multiple techniques . efficient use of these techniques is important for an effective and relevant understanding of the text/data . this analysis can in turn be very helpful in ecommerce for targeting audience , social_media monitoring for anticipating the foul elements from society and take proactive actions to avoid unethical and illegal activities , business_analytics , market positioning etc . method : the goal is to understand the basic steps_involved in analysing the text data which can be helpful in determining sentiments behind them . this review provides detailed description of steps_involved in sentiment_analysis with the recent research done . patents related to sentiment_analysis and classification are reviewed to throw some light in the work done related to the field . results : sentiment_analysis determines the polarity behind the text data/review . this analysis helps in increasing the business revenue , e-health , or determining the behaviour of a person . conclusion : this study helps in understanding the basic steps_involved in natural_language_understanding . at each step there are multiple techniques that can be applied on data . different classifiers provide variable accuracy depending upon the data set and classification technique used .
title : aspect categorization using domain-trained word_embedding and topic_modelling ; abstract : aspect-based sentiment_analysis is the most important research topic conducted to extract and categorize aspect-terms from online_reviews . recent efforts have shown that topic_modelling is vigorously used for this task . in this paper , we integrated word_embedding into collapsed gibbs_sampling in latent_dirichlet_allocation ( lda ) . specifically , the conditional distribution in the topic_model is improved using the word_embedding model that was trained against ( customer_review ) training dataset . semantic similarity ( cosine measure ) was leveraged to distribute the aspect-terms to their related aspect-category cognitively . the experiment was conducted to extract and categorize the aspect terms from semeval_2014 dataset .
title : precursors to verb learning : infants ' understanding of motion_events ; abstract : this chapter shows how particular relational meanings are formed independently of language , whereas others require linguistic input to develop . it argues that the contribution of nonlinguistic versus linguistic input in the formation of relational meanings varies not only with the concept in question but also with the developmental point at which the concept begins to develop . the chapter begins by reviewing the current literature on infants ' understanding of motion_events , demonstrating that young infants possess a rich understanding of various types of motion_events . it then shows that infants ' perceptual and cognitive abilities do not provide them with an understanding of all action events . finally , it delineates a developmental progression for infants ' understanding of motion_events . specifically , it demonstrates how developmental changes in infants ' ability to form an abstract categorical representation of a dynamic spatial event follows a specific-to-abstract progression , and that language exerts its influence on infants ' discrimination and categorization of particular motion_events by modulating this progression .
title : message classification in the call center ; abstract : customer care in technical domains is increasingly based on e-mail communication , allowing for the reproduction of approved solutions . identifying the customer 's problem is often time-consuming , as the problem space changes if new products are launched . this paper describes a new approach to the classification of e-mail requests based on shallow text processing and machine_learning techniques . it is implemented within an assistance system for call center agents that is used in a commercial setting .
title : practical transformer-based multilingual text_classification ; abstract : transformer-based methods are appealing for multilingual text_classification , but common research benchmarks like xnli ( conneau et al. , 2018 ) do not reflect the data availability and task variety of industry applications . we present an empirical comparison of transformer-based text_classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings . we evaluate these methods on two distinct tasks in five different languages . departing from prior work , our results show that multilingual language models can outperform monolingual ones in some downstream_tasks and target languages . we additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled_data .
title : classification of forensic autopsy reports through conceptual_graph-based document_representation model ; abstract : text_categorization has been used extensively in recent_years to classify plain-text clinical_reports . this study employs text_categorization techniques for the classification of open narrative forensic autopsy reports . one of the key steps in text_classification is document_representation . in document_representation , a clinical report is transformed into a format that is suitable for classification . the traditional document_representation technique for text_categorization is the bag-of-words ( bow ) technique . in this study , the traditional bow technique is ineffective in classifying forensic autopsy reports because it merely extracts frequent but discriminative_features from clinical_reports . moreover , this technique fails to capture word inversion , as well as word-level synonymy_and_polysemy , when classifying autopsy reports . hence , the bow technique suffers from low accuracy and low robustness unless it is improved with contextual and application-specific information . to overcome the aforementioned limitations of the bow technique , this research aims to develop an effective conceptual_graph-based document_representation ( cgdr ) technique to classify 1500 forensic autopsy reports from four ( 4 ) manners of death ( mod ) and sixteen ( 16 ) causes of death ( cod ) . term-based and systematized nomenclature of medicine–clinical terms ( snomed_ct ) based conceptual features were extracted and represented through graphs . these features were then used to train a two-level text classifier . the first level classifier was responsible for predicting mod . in addition , the second level classifier was responsible for predicting cod using the proposed conceptual_graph-based document_representation technique . to demonstrate the significance of the proposed technique , its results were compared with those of six ( 6 ) state-of-the-art document_representation techniques . lastly , this study compared the effects of one-level classification and two-level classification on the experimental results . the experimental results indicated that the cgdr technique achieved 12 % to 15 % improvement in accuracy compared with fully_automated document_representation baseline techniques . moreover , two-level classification obtained better results compared with one-level classification . the promising_results of the proposed conceptual_graph-based document_representation technique suggest that pathologists can adopt the proposed system as their basis for second opinion , thereby supporting them in effectively determining cod .
title : hybrid supervised clustering based ensemble scheme for text_classification ; abstract : purpose : the immense quantity of available unstructured_text documents serve as one of the largest source of information . text_classification can be an essential task for many purposes in information_retrieval , such as document_organization , text filtering and sentiment_analysis . ensemble_learning has been extensively_studied to construct efficient text_classification schemes with higher predictive performance and generalization_ability . the purpose of this paper is to provide diversity among the classification_algorithms of ensemble , which is a key_issue in the ensemble design . design/methodology/approach : an ensemble scheme based on hybrid supervised clustering is presented for text_classification . in the presented scheme , supervised hybrid clustering , which is based on cuckoo search_algorithm and k-means , is introduced to partition the data samples of each class into clusters so that training subsets with higher diversities can be provided . each classifier is trained on the diversified training subsets and the predictions of individual_classifiers are combined by the majority_voting rule . the predictive performance of the proposed classifier_ensemble is compared to conventional classification_algorithms ( such as naïve_bayes , logistic_regression , support_vector_machines and c4.5_algorithm ) and ensemble_learning methods ( such as adaboost , bagging and random_subspace ) using 11 text benchmarks . findings : the experimental results indicate that the presented classifier_ensemble outperforms the conventional classification_algorithms and ensemble_learning methods for text_classification . originality/value : the presented ensemble scheme is the first to use supervised clustering to obtain diverse ensemble for text_classification
title : document analysis based on multi-view intact space learning with manifold regularization ; abstract : document analysis plays an important role in our life , and traditional models like latent_semantic_analysis ( lsi ) or latent_dirichlet_allocation ( lda ) can not handle data from many sources . multi-view learning technology like multi-view intact space learning ( misl ) , which integrates the complementary_information on multiple views to discover a latent intact representation of the data , is effective for image or video application . but the model has not been applied to multi-lingual documents and has not considered the intrinsic geometrical and discriminating structure of the document data . to overcome this issue , we assume that if documents are close in the origin representation , they should also be close in the intact space representation . and we introduce a manifold regularization term to misl so that the data is more smoothly in latent_space . we conduct classification experiments on 10505 wiki documents we crawled , and the result_shows that it is outperforming tfidf , lsi , lda , and misl .
title : induction of classification from lexicon expansion : assigning domain tags to wordnet entries ; abstract :
title : misinformation analysis during covid-19_pandemic ; abstract : online diffusion of misinformation has gained extreme attention in the research from past few years . moreover , during ongoing covid-19_pandemic , the proliferation of misinformation became more prominent . in this paper , a comparison of two feature_engineering techniques , namely term_frequency–inverse_document_frequency ( tf-idf ) and word_embeddings ( doc2vec ) , is done over different machine_learning classifiers . a web scraper is developed for fact-checking web_site , snopes.com , to extract labeled news related to covid-19 . although the size of dataset is less , the body content under headlines contains large amount of text . therefore , the model works well with both the feature_engineering techniques and machine_learning algorithms . apparently , we obtained best accuracy of 95.38 % with tf-idf on decision_tree and same accuracy of 90.77 % using doc2vec on support_vector_machine and logistic_regression machine_learning classifier .
title : srtm : a supervised relation topic_model for multi-classification on large-scale document network ; abstract : the increasing use of social_networking platforms has raised the need to develop automated multi-classifications on document network . in this paper , we propose a supervised relation topic_model that leverages the links between documents to learn the latent content of documents and enhance performance of prediction . our model takes advantage of bayesian generative_model to exploit the relation between word feature and link feature in a document network . we evaluate our model on large-scale data collections that include scientific citation community and medical article network . we demonstrate its effectiveness and efficiency on document_classification with slda model and collective classification approaches .
title : research on the method of educational text_classification based on deep_learning ; abstract : to improve the efficiency of traditional text_classification methods of education and the recall_rate of research objectives , we propose an educational text_classification method based on the depth of learning . the english web_pages covering the economics , politics , sports , entertainment , and life , etc. , which involve carriers containing explanatory texts and discussion papers , etc. , will be used as the source of english educational texts for corresponding collection and processing . we upload the collected texts , introduce deep_learning algorithms , perform preprocessing and feature learning , and input the learning features into the deep_learning softmax_classifier based on the learning results . the output of the classifier consists of the classification results of the educational texts . experiment results show that the proposed method of classification gives good accuracy and high recall_rate , and the average distribution time is only 3.992 s. hence , the proposed method can effectively_improve the classification efficiency of educational texts .
title : investigating communication styles in text-based cmc using a classification of intention : a comparison of same-culture and different-cultures context ; abstract : we investigated how cultural differences influence communication style in text-based computer-mediated_communication ( cmc ) and compared the context of communications within the same culture ( thai-thai pairs ) and different cultures ( thai-japanese pairs and thai-chinese pairs ) by examining significant differences in the number of text chats in each classification pertaining to intentions . the significant finding of this study is the large number of text chats in the interrogation category in the context of different cultures . results showed a significant difference in the number of text chats between thai and japanese participants in the description and interrogation categories . further , we found a significant difference in the interrogation classification between thai and chinese participants . the understanding of cultural differences in this work can be used to improve intercultural competencies .
title : exclude and purify unwanted communications in online_social_networks ; abstract : background : the major problem in today ’ s online_social_networks ( osns ) is to provide users the ability to control the messages_posted on their own private space to avoid that unwanted content is displayed . so far , osns give little support to this requirement . methods : to fill the gap , in this paper , we propose a mechanism allowing osn users to have a direct control on the messages_posted on their walls . improvements : this is achieved through a flexible rule-based system , which allows users to customize the filtering criteria to be applied to their walls , and a machine_learning-based soft classifier automatically labelling messages in support of content-based_filtering .
title : improved spam_email_filtering architecture using several feature_extraction techniques ; abstract : research on spam_email_filtering is drawing experts from all over the world , as these junk email messages continue to affect people ’ s daily lives , whether consciously or unconsciously . the overwhelming use of irritating , destructive , and misleading emails appears to have damaged the values of email which prompted us to perform this research to construct a model for spam_filtering with faster_training time and enhanced accuracy . we have proposed two voting architectures built upon machine_learning models and ensemble_classifiers , respectively . in our work , we have also analyzed the performance of several individually applied classifiers and ensemble_techniques with various feature retrieval strategies . additionally , we have compared the training time of the proposed models with the deep lstm-cnn hybrid model . both of our suggested models have performed adequately , while the ml-based voting model ( type 1 ) produces the most accurate filtering ( 98 % ) taking bag of words for feature_extraction and can be trained above 200 times_faster than the lstm-cnn model .
title : revisiting uncertainty-based query_strategies for active_learning with transformers ; abstract : active_learning is the iterative construction of a classification model through targeted labeling , enabling significant labeling_cost savings . as most research on active_learning has been carried out before transformer-based language models ( `` transformers '' ) became popular , despite its practical importance , comparably few papers have investigated how transformers can be combined with active_learning to date . this can be attributed to the fact that using state-of-the-art query_strategies for transformers induces a prohibitive runtime overhead , which effectively nullifies , or even outweighs the desired cost savings . for this reason , we revisit uncertainty-based query_strategies , which had been largely outperformed before , but are particularly suited in the context of fine-tuning transformers . in an extensive_evaluation , we connect transformers to experiments from previous_research , assessing their performance on five widely used text_classification benchmarks . for active_learning with transformers , several other uncertainty-based approaches outperform the well-known prediction entropy query_strategy , thereby challenging its status as most popular uncertainty baseline in active_learning for text_classification .
title : accuracy evaluation of arabic text_classification ; abstract : categorization of arabic text is a significant challenge nowadays owing to the richness of text that occurs through various modules . also , the arabic_language is considered the fifth spoken one . during the last decade , scholars incubated few concerns about this regard comparing with english_language . the objective behind this investigation is to perform and evaluate new mechanism relating to different techniques of machine_learning specifically for classifying arabic text in fresh different data set . preprocessing_steps along with the representation pattern of text are essential for handling text without artifacts . we use a binary term_occurrence matrix as mutual_information for feature_vector representation method . this paper evaluates the outcomes of classification via using deep_learning , k-nearest_neighbor , support_vector_machine and naïve_bayes classifiers in similarity text level and n-gram level . it has been extracted out the outcomes that the deep_learning achieves better performance compared to itself in case of increasing similarity level and n-gram level .
title : got technical_debt ? surfacing elusive technical_debt in issue trackers ; abstract : concretely communicating technical_debt and its consequences is of common interest to both researchers and software engineers . in the absence of validated tools and techniques to achieve this goal with repeatable results , developers resort to ad_hoc practices . most commonly they report using issue trackers or their existing backlog management practices to capture and track technical_debt . in a manual examination of 1,264 issues from four issue trackers from open_source industry and government projects , we identified 109 examples of technical_debt . our study reveals that technical_debt and its related concepts have entered the vernacular of developers as they discuss development tasks through issue trackers . even when issues are not explicitly tagged as technical_debt , it is possible to identify technical_debt items in these issue trackers using a categorization method we developed . we use our results and data to motivate an improved definition and an approach to explicitly report technical_debt in issue trackers .
title : adaptation of autoencoder for sparsity reduction from clinical_notes representation_learning ; abstract : when dealing with clinical_text classification on a small dataset recent_studies have confirmed that a well-tuned multilayer_perceptron outperforms other generative classifiers , including deep_learning ones . to increase the performance of the neural_network classifier , feature_selection for the learning representation can effectively be used . however , most feature_selection methods only estimate the degree of linear dependency between variables and select the best features based on univariate statistical_tests . furthermore , the sparsity of the feature_space involved in the learning representation is ignored . goal : our aim is therefore to access an alternative approach to tackle the sparsity by compressing the clinical representation feature_space , where limited french clinical_notes can also be dealt with effectively . methods : this study proposed an autoencoder learning algorithm to take advantage of sparsity reduction in clinical note representation . the motivation was to determine how to compress sparse , high-dimensional data by reducing the dimension of the clinical note representation feature_space . the classification performance of the classifiers was then evaluated in the trained and compressed feature_space . results : the proposed approach provided overall performance_gains of up to 3 % for each evaluation . finally , the classifier achieved a 92 % accuracy , 91 % recall , 91 % precision , and 91 % f1-score in detecting the patient 's condition . furthermore , the compression working mechanism and the autoencoder prediction process were demonstrated by applying the theoretic information bottleneck framework .
title : arabic dialect identification in the context of bivalency and code-switching ; abstract : in this paper we use a novel approach towards arabic dialect identification using language bivalency and written code-switching . bivalency between languages or dialects is where a word or element is treated by language users as having a fundamentally similar semantic content in more than one language or dialect . arabic dialect identification in writing is a difficult task even for humans due to the fact that words are used interchangeably between dialects . the task of automatically identifying dialect is harder and classifiers trained using only n-grams will perform_poorly when tested on unseen data . such approaches require significant amounts of annotated training_data which is costly and time consuming to produce . currently available arabic dialect datasets do not exceed a few hundred thousand sentences , thus we need to extract features other than word and character n-grams . in our work we present experimental results from automatically identifying dialects from the four main arabic dialect regions ( egypt , north_africa , gulf and levant ) in addition to standard_arabic . we extend previous work by incorporating additional grammatical and stylistic_features and define a subtractive bivalency profiling approach to address issues of bivalent words across the examined arabic dialects . the results show that our new methods classification_accuracy can reach more than 76 % and score well ( 66 % ) when tested on completely unseen data .
title : dama : a dynamic classification of multimodal ambiguities ; abstract : ambiguities represent uncertainty but also a fundamental item of discussion for who is interested in the interpretation of languages and it is actually functional for communicative purposes both in human–human communication and in human–machine interaction . this paper faces the need to address ambiguity issues in human–machine interaction . it deals with the identification of the meaningful features of multimodal ambiguities and proposes a dynamic classification method that characterizes them by learning , and progressively adapting with the evolution of the interaction language , by refining the existing classes , or by identifying new ones . a new class of ambiguities can be added by identifying and validating the meaningful features that characterize and distinguish it compared to the existing ones . the experimental results demonstrate improvement in the classification rate over considering new ambiguity classes .
title : review and classification of trajectory summarisation algorithms : from compression to segmentation ; abstract : with the continuous development and cost reduction of positioning and tracking technologies , a large amount of trajectories are being exploited in multiple_domains for knowledge extraction . a trajectory is formed by a large number of measurements , where many of them are unnecessary to describe the actual trajectory of the vehicle , or even harmful due to sensor noise . this not only consumes large amounts of memory , but also makes the extracting knowledge process more difficult . trajectory summarisation techniques can solve this problem , generating a smaller and more manageable representation and even semantic segments . in this comprehensive review , we explain and classify techniques for the summarisation of trajectories according to their search strategy and point evaluation_criteria , describing connections with the line simplification problem . we also explain several special concepts in trajectory summarisation problem . finally , we outline the recent trends and best practices to continue the research in next summarisation algorithms .
title : scgru : a general approach for identifying multiple classes of self-admitted technical_debt with text_generation oversampling ; abstract : identifying self-admitted technical_debt ( satd ) plays an important role in maintaining software stability and improving software_quality . although existing_methods can detect satd and researchers have identified design debt and requirement debt , an approach to realize multiple classification of satd , including defect , test , and documentation , is still lacking . in this paper , we combine text_generation oversampling and the convolutional_neural_networks-gated_recurrent_unit ( cnngru ) model , and propose an approach called scgru to classify multiple debt , including defect , test , documentation , design , and requirement . first , seqgan-based text_generation is employed to generate new samples by learning the original satd data , thereby increasing the number of satd samples such as defect debt and reducing data imbalance . then , we apply the cnngru model to refine satd into multiple classes . an experiment with cross-project identification of 10 projects shows that our approach is more effective than existing_methods such as cnn and text_mining . the proposed scgru approach has strong advantages especially in cases of flawed debt with very unbalanced_data such as test debt and documention debt .
title : enriching epidemiological thematic features for disease_surveillance corpora classification ; abstract : we present epidbiobert , a biosurveillance epidemiological document tagger for disease_surveillance over padi-web system . our model is trained on padi-web corpus which contains news articles on animal diseases outbreak extracted from the web . we train a classifier to discriminate between relevant and irrelevant_documents based on their epidemiological thematic feature content in preparation for further epidemiology information_extraction . our approach proposes a new way to perform epidemiological document_classification by enriching epidemiological thematic features namely disease , host , location and date , which are used as inputs to our epidemiological document classifier . we adopt a pre-trained biomedical language model with a novel fine_tuning approach that enriches these epidemiological thematic features . we find these thematic features rich enough to improve epidemiological document_classification over a smaller data set than initially used in padi-web classifier . this improves the classifiers ability to avoid false_positive alerts on disease_surveillance systems . to further understand information encoded in epidbiobert , we experiment the impact of each epidemiology thematic feature on the classifier under ablation studies . we compare our biomedical pre-trained approach with a general language model based model finding that thematic feature embeddings pre-trained on general english documents are not rich enough for epidemiology classification_task . our model achieves an f1-score of 95.5 % over an unseen_test set , with an improvement of +5.5 points on f1-score on the padi-web classifier with nearly half the training_data set .
title : classification of untranscribed handwritten notarial documents by textual_contents ; abstract : huge amounts of digital page images of important manuscripts are preserved in archives worldwide . the amounts are so large that it is generally unfeasible for archivists to adequately tag most of the documents with the required metadata so as to allow proper organization of the archives and effective exploration by scholars and the general public . the class or “ typology ” of a document is perhaps the most important tag to be included in the metadata . the technical problem is one of automatic classification of documents , each consisting of a set of untranscribed handwritten_text images , by the textual_contents of the images . the approach considered is based on “ probabilistic indexing ” , a relatively novel technology which allows to effectively represent the intrinsic word-level uncertainty exhibited by handwritten_text images . we assess the performance of this approach on a large collection of complex notarial manuscripts from the spanish archivo histórico provincial de cádiz , with promising_results .
title : chillax - at arabic hate_speech 2022 : a hybrid machine_learning and transformers based model to detect arabic offensive and hate_speech ; abstract : hate_speech and offensive_language have become a crucial problem nowadays due to the extensive usage of social_media by people of different gender , nationality , religion and other types of characteristics , allowing anyone to share their thoughts and opinions . in this research paper , we proposed a hybrid model for the first and second tasks of osact2022 . this model used the arabic pre-trained bert language model marbert for feature_extraction of the arabic tweets in the dataset provided by the osact2022 shared_task , then fed the features to two classic machine_learning classifiers ( logistic_regression , random_forest ) . the best results achieved for the offensive tweet detection task were achieved by the logistic_regression model with accuracy , precision , recall , and f1-score of 80 % , 78 % , 78 % , and 78 % , respectively . the results for the hate_speech tweet detection task were 89 % , 72 % , 80 % , and 76 % .
title : sesam : semi-automated semantic analysis method of urban_areas ' events with extreme levels of popularity based on public open_data ; abstract : this study describes the semi-automated pipeline created for the comprehensive analysis of the urban_areas with the extremely low and extremely_high popularity levels . it includes the geo-frequency analysis of the russian-language instagram publications for the st. petersburg area and selection of areas with the extreme values of the popularity level according to the number of publications in them . semantic analysis of the urban_areas with an extremely low number of publications includes comparing of algorithms for descriptions extraction and classification for these areas and results of such descriptions extraction and classification using tf-idf vectorization technique and most valuable words extraction . semantic analysis of areas with an extremely_high number of publications includes the structure description of such areas , comparing of algorithms for advertisement publications extraction , results of the advertisement extraction using bigartm model and further development and implementation of the algorithm for extracting events related to the the points of attraction in extremely popular urban_areas , which is based on the strong time binding hypothesis and the idea of similarity queries using combination of lda models for revealing semantic structure and algorithm based on frequency analysis . developed algorithm was tested to extract events in the urban_area of st. petersburg where ice palace is placed and showed interpretable results and allow us to correctly extract 89 events out of 102 which occurred in this area in 2019 . finally , sesam pipeline for comprehensive urban analysis was created that combined the described algorithms .
title : competitor identification by use the sentiment_classification based on the user research ; abstract : —online_shopping 's have achieved an immense growth . all like to do it as there is no need to physically to the shop and we have a wide range of collections available in the online sites from which we can actually buy the product . the customers usually tend to purchase a product that has a good customer_review and has the highest rating . numerous reviews are given for a single product and the most of the important reviews are not organized well which makes it disappear from the other reviews . numerous researchers have worked on structuring the reviews for various purposes . in this work we propose a sentimental_analysis of customer_reviews for various hotel items . all the items are reviewed by the customers and the proposed work makes an analysis of the reviews obtained for a particular item in all the available shops . this analysis is helpful injudging the most likely consumed food by the customers around and can get to know the competiveness of the product being delivered to the customers . machine_learning techniques and natural_language_processing ( nlp ) are used for the proposed work and is observed to produce an efficient result .
title : arabic duplicate questions detection based on contextual representation , class_label matching , and structured self attention ; abstract : question_answering systems ( qas ) are rising solutions providing exact and precise answers to natural questions . duplicate question detection ( dqd ) , which aims to reuse previous answers , has shown its ability to improve user_experience and reduce significantly the response time . however , few arabic qas integrate solutions able to detect duplicate questions in their workflow . in this paper , we build a dqd method based on contextual word_representation , question_classification and forward/backward structured self attention . first , we extract contextual word_representation embeddings from language models ( elmo ) to map questions into a vector_space . next , we train two models to classify question embedding according to two taxonomies : hamza et al . and li & roth . then , we introduce a class_label matching step to filter out questions that have different class_labels . finally , we propose a bidirectional attention bidirectional_lstm ( biattention bilstm ) model that focuses only on keywords to predict whether a question pair is a duplicate or not . we also apply a data augmentation strategy based on symmetry , reflexivity , and transitivity relations to improve the generalization of our model . various experimentations are performed to evaluate the impact of question_classification and pre-processing step on dqd model . the obtained results show that our model achieves good performances as compared to the baseline results .
title : human non-linguistic vocal repertoire : call types and their meaning ; abstract : recent research on human nonverbal vocalizations has led to considerable progress in our understanding of vocal communication of emotion . however , in contrast to studies of animal vocalizations , this research has focused mainly on the emotional interpretation of such signals . the repertoire of human nonverbal vocalizations as acoustic types , and the mapping between acoustic and emotional_categories , thus remain underexplored . in a cross-linguistic naming task ( experiment 1 ) , verbal categorization of 132 authentic ( non-acted ) human vocalizations by english- , swedish- and russian-speaking participants revealed the same major acoustic types : laugh , cry , scream , moan , and possibly roar and sigh . the association between call type and perceived emotion was systematic but non-redundant : listeners associated every call type with a limited , but in some cases relatively wide , range of emotions . the speed and consistency of naming the call type predicted the speed and consistency of inferring the caller ’ s emotion , suggesting that acoustic and emotional categorizations are closely_related . however , participants preferred to name the call type before naming the emotion . furthermore , nonverbal categorization of the same stimuli in a triad classification_task ( experiment 2 ) was more compatible with classification by call type than by emotion , indicating the former ’ s greater perceptual salience . these results suggest that acoustic categorization may precede attribution of emotion , highlighting the need to distinguish between the overt form of nonverbal signals and their interpretation by the perceiver . both within- and between-call acoustic variation can then be modeled explicitly , bringing research on human nonverbal vocalizations more in line with the work on animal_communication .
title : a patent text_classification model based on multivariate neural_network fusion ; abstract : in order to improve the efficiency and accuracy of automatic classification of patent texts , a patent text_classification model ( c3-bigru-at ) based on multivariate neural_network fusion was proposed . firstly , patent text is segmented and represented by text_preprocessing . then , the text features of different levels and different characteristics are extracted through word_embedding layer , convolution layer , bigru layer and attention_layer , and text category recognition is carried out through soft max layer . finally , case_studies show that c3-bigru-at model has a high ability of patent text recognition , and can meet the requirements of accurate and efficient classification of a large number of patent texts .
title : lyric-based sentiment_polarity classification of thai songs ; abstract : song sentiment_polarity provides outlook of a song . it can be used in automatic music recommendation system . sentiment_polarity classification based solely on lyrics is challenging . it involves understanding linguistic knowledge , song characteristics and emotional interpretation of words . since lyric is in a form of text . techniques used in text_mining , text sentiment_analysis and music mood classification are studied and used together in our proposed model . two types of classifier are proposed—lexicon-based classifier and machine_learning-based classifier . n-gram model is used in feature_set generation . features are filtered by information gain . feature_weighting scheme is employed . we create a sentiment_lexicon from thai song corpus . full lyric and certain parts of lyric are chosen for datasets . we evaluate our models under various environments . the best average accuracy achieved is 68 % .
title : classification-based aggregation model on large_scale group_decision_making with hesitant_fuzzy linguistic information ; abstract : a classification and aggregation problem on large-scale group_decision_making is studied based on the hesitant_fuzzy linguistic terms . specifically , a method to measure the similarity of two hesitant linguistic sets is proposed considering both hesitancy and distance . then , the preliminary expert classes are generated using the netting method based on a similarity_matrix , and an acceptable level is set to help make a second classification . through a classification_accuracy index , the final classes are obtained . furthermore , two aggregating frames are constructed respectively for the information within one class and between the classes . the proportional linguistic groups are obtained through combing the information within one class . on that basis , the degree of reliability of each class is calculated and the class weights are determined based on the class reliability and the percentage of expert number in one class to the total number to aggregate the information among classes . additionally , the expected values of the alternatives are calculated to make a selection . finally , a case is given to illustrate the effectiveness and feasibility of the proposed method .
title : hybrid feature_selection based on enhanced genetic_algorithm for text_categorization ; abstract : this paper proposes hybrid feature_selection approaches based on the genetic_algorithm ( ga ) . this approach uses a hybrid search technique that combines the advantages of filter_feature_selection_methods with an enhanced ga ( ega ) in a wrapper_approach to handle the high dimensionality of the feature_space and improve categorization performance simultaneously . first , we propose ega by improving the crossover and mutation operators . the crossover operation is performed based on chromosome ( feature_subset ) partitioning with term and document_frequencies of chromosome entries ( features ) , while the mutation is performed based on the classifier performance of the original parents and feature_importance . thus , the crossover and mutation operations are performed based on useful information instead of using probability and random selection . second , we incorporate six well-known filter_feature_selection_methods with the ega to create hybrid feature_selection approaches . in the hybrid approach , the ega is applied to several feature_subsets of different sizes , which are ranked in decreasing order based on their importance , and dimension reduction is carried out . the ega operations are applied to the most important features that had the higher ranks . the effectiveness of the proposed approach is evaluated by using naïve_bayes and associative_classification on three different collections of arabic text datasets . the experimental results show the superiority of ega over ga , comparisons of ga with ega showed that the latter achieved better results in terms of dimensionality_reduction , time and categorization performance . furthermore , six proposed hybrid fs approaches consisting of a filter method and the ega are applied to various feature_subsets . the results showed that these hybrid approaches are more effective than single filter methods for dimensionality_reduction because they were able to produce a higher reduction rate without loss of categorization precision in most situations .
title : a maximal figure-of-merit learning approach to text_categorization ; abstract : a novel maximal figure-of-merit ( mfom ) learning approach to text_categorization is proposed . different from the conventional techniques , the proposed mfom method attempts to integrate any performance metric of interest ( e.g . accuracy , recall , precision , or f1 measure ) into the design of any classifier . the corresponding classifier parameters are learned by optimizing an overall objective_function of interest . to solve this highly nonlinear optimization_problem , we use a generalized probabilistic descent algorithm . the mfom learning framework is evaluated on the reuters-21578 task with lsi-based feature_extraction and a binary_tree classifier . experimental results indicate that the mfom classifier gives improved f1 and enhanced robustness over the conventional one . it also outperforms the popular svm method in micro-averaging f1 . other extensions to design discriminative multiple-category mfom classifiers for application scenarios with new performance_metrics could be envisioned too .
title : an evaluation of statistical spam_filtering techniques ; abstract : this paper evaluates five supervised_learning methods in the context of statistical spam_filtering . we study the impact of different feature pruning methods and feature_set sizes on each learner 's performance using cost-sensitive measures . it is observed that the significance of feature_selection varies greatly from classifier to classifier . in particular , we found support_vector_machine , adaboost , and maximum_entropy model are top performers in this evaluation , sharing similar characteristics : not sensitive to feature_selection strategy , easily scalable to very high feature_dimension , and good performances across different datasets . in contrast , naive_bayes , a commonly used classifier in spam_filtering , is found to be sensitive to feature_selection methods on small feature_set , and fails to function well in scenarios where false_positives are penalized heavily , the experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams ( such as λ = 999 ) , so as to maintain a better-than-baseline performance . an interesting finding is the effect of mail headers on spam_filtering , which is often ignored in previous_studies . experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only . this implies that message headers can be reliable and powerfully discriminative feature sources for spam_filtering .
title : research on food complains document_classification based-on topic ; abstract : in this paper , we design a classifier based-on topic for food complain documents , and take a series of measures to the implementation process . in order to accomplish feature_reduction , the filter method named term filtering for independent topic features is proposed to compress each topic feature_vector . we introduce the created food ontology as background_knowledge and to expand the semantic of complaint documents with the aid of hownet . so we can supplement effective information and improve the effect of text_classification . in addition , we take account of different importance between title and body in the text , considering that title can stand out the topic of text better than the textual body . consequently , we separately calculate the word_frequency which words are in textual title and body . the experiments show that it is necessary to consider the different importance between textual title and body , and we can achieve good feature_reduction effect using the proposed filter method , and the classification performance get obvious improvement after the process of term expanding . © 2012 academy_publisher .
title : classifying promotion images using optical_character_recognition and naïve_bayes classifier ; abstract : promotion is one of the most effective ways to promote a business , and most people love promotions . usually these businesses announce their promo by uploading images to social medias such as instagram . however , most of the time these promo images are buried in the sea of other non-promotional images . it would be more practical if computers could be utilized to automatically look for images containing promotional offers . that is why this research is done to discuss about creating a system that is able to tell whether an image contains information about a promotional offer or not automatically without human intervention using optical_character_recognition ( ocr ) and naïve_bayes algorithm as the classifier . random_forest and k-nearest_neighbor are also used as a comparison to the naïve_bayes algorithm . in this experiment we use cross_validation method where we divide 158 images into five groups to train and test our model . the naïve_bayes model achieved 94,31 % accuracy , 94,33 % recall , 94,11 % precision , and 0.93 f1 score on average , which is the highest among these three algorithms . based on the results , we can conclude that optical_character_recognition ( ocr ) and naïve_bayes algorithm are quite suitable for this problem .
title : method of mapping vietnamese chunked sentences to definite shallow structures ; abstract : in many natural_language_processing based intelligent_systems , parsing is the first task to perform . however , in the next stages , many systems often have the capacity of processing a limited number of parsed structures . the problem is to determine what parsed sentences can be recognized by a system . the decision of syntactic structures which can be processed by a system is consider as the task of 'classification ' of a parsed sentence into one of given classes of recognizable parses . in this paper we deal with this issue by proposing a method for mapping vietnamese chunked sentences to a set of pre-defined shallow structures . also , we tag lexicons and chunk phrases of the original sentences using our functional part-of-speech ( fpos ) tagset with apache opennlp tools ( tokenizer , pos_tagger , chunker ) . based on the foundation of functional grammar , we define new lexical tags and combine with penn-treebank tagset to build our fpos tagset . due to our set of shallow structures is finite , instead of using a parser , we propose a rule-based algorithm for the mapping process . we establish conversion rules according to the reality experiences when using vietnamese in common communication . the experiment_shows that we converse successfully for the major of testing sentences and the algorithm can be applied for different languages .
title : context –sensitive sentimental based text_summarization and classification based on the occurrence of trigger term in a sentence ; abstract : online_product_reviews and community links have turn out to be the most widespread platform for sharing the product info , with vast quantities of reviews displayed every day . automatically created product summaries help explorers in choosing best product . it analytically explores the effect of statistical and textual reviews on manufactured goods sales performance . this paper suggested a new multi-text_summarization method for distinguishing the best top-most significant sentences of product_reviews . most of the earlier works on review_summarization have mainly scrutinized content exploration , which disrespects grave features like writer reliability and conflicting sentiments . we examined above features and established a novel sentence with significance metric . the content and sentiment similitudes were utilized to define the relationship of two sentences . to categorize the top-most sentences , the k-clustering procedure was utilized to divide sentences towards k-groups . the final summarization sentence are selected from k-group . to calculate the efficiency of the suggested approach , we used product_review from amazon . the results show that the suggested method outpaces the other approach , it can provide more complete information about product . this study paper observes the business impression of product_reviews . it analytically examines the effect of statistical and textual reviews on manufactured goods sales performance and to accepting their products and challengers ’ products , which provide perceptions into their product_development progress .
title : everyday sound categorization ; abstract : this chapter reviews theories and empirical_research on the ways in which people spontaneously and effortlessly categorize sounds into meaningful categories to make sense of their environment . we begin with an overview of prominent theories of categorization in the psychological literature , followed by data collection and analysis methods used in empirical_research on categorization with human participants . we then focus on auditory categorization , synthesizing the main findings of studies on isolated sound events as well as complex sound scenes . finally , we review recently proposed taxonomies for everyday sounds and conclude by providing directions for integrating insights from cognitive_psychology into the design and evaluation of computational systems .
title : sentiment_analysis for arabic dialect using supervised_learning ; abstract : sentiment_analysis is a set of procedures used to extract subjective opinions from the text . generally , there are two techniques for sentiment_analysis , machine_learning method , and lexicon-based method . this work focuses on extracting and analyzing twitter data written in sudanese_arabic dialect to observe opinionated patterns regarding the quality of telecommunication services operating in sudan . one of the significant limitations in the field of text_classification is the exclusive focus on the english_language . there is a need to bridge this gap by developing efficient methods and tools for sentiment_analysis in the arabic_language . moreover , reliable corpus and lexicons are needed . for this study , four classifiers were trained on a dataset consist of 4712 tweets . namely naïve_bayes , svm , multinomial_logistic_regression and k-nearest_neighbor to conduct a comparative_analysis on the performance of the classifiers . these algorithms when ran against the tweets dataset the results revealed that svm gives the highest f1-score ( 72.0 ) while the best accuracy was achieved by knn ( k=2 ) and it equals to 92.0 .
title : research convey on text_classification method based on deep_learning ; abstract : deep_learning technology has been rapidly developed in recent_years and has been increasingly applied in the field of text_classification , and many effective and novel classification methods have emerged . the development history of text_classification is introduced , the text_classification problem based on deep_neural_networks is analyzed , the characteristics and performance of various classical classification methods are compared and summarized , and it is shown that deep_neural_networks are more advantageous than traditional_machine_learning_methods in the field of text_classification on the whole . the shortcomings of current deep text_classification models are pointed out and future_research_directions prospect .
title : multi-resolution classification trees in ocr design ; abstract : this paper recalls the idea of classification trees in ocr ( optical_character_recognition ) systems and proposes a technique for the automatic design of these classification trees . the design of both the classification trees and of the classification operators are based on training from sample pairs of observed-ideal images , allowing the development of customized ocrs .
title : an improved naïve_bayes classifier method in public_opinion analysis ; abstract : an improved naïve_bayes classifier is proposed . the method includes aspects of improvement : to get a reduced text feature word set by filtering the synonym , to iterate two different feature_selection methods , and to effectively_improve the representative feature_set . the experimental results show that this method can effectively_improve the performance of naïve_bayes classifier .
title : a novel approach to data augmentation for document_image_classification using deep convolutional generative_adversarial_networks ; abstract : data augmentation is a procedure where new samples are generated from the training dataset by applying various techniques and algorithms to improve machine and deep_learning models ’ accuracy and generalization_ability . the recent advances in deep_learning and computer vision techniques have made scanned document_classification a painless and straightforward process . however , such approaches require a lot of labeled_data before training and validating the classifiers , which can be done by augmenting the existing dataset by different means . in this contribution , we explored using a system based on deep convolutional adversarial_networks ( dcgan ) to generate fake document_images using an existing scanned_documents dataset . moreover , we compared conventional data_augmentation techniques ( rotation , zoom , random cropping , etc . ) to dcgan-based augmentation in a document_classification context . the newly generated data could then be used along with the labeled_data to train a convolutional_neural_network ( cnn ) to classify scanned document_images . this experiment compared the performances of a system trained on different datasets ( original labeled_data , gan-augmented dataset , hybrid augmented dataset ) . its results revealed the effectiveness of the proposed approach .
title : exploring sbert and mixup data augmentation in rhetorical_role labeling of indian legal sentences ; abstract : the rise of the transformer architecture allowed the creation of huge pre-trained_language_models that led to new state-of-the-art achievements in general-purpose natural_language applications . such models also have the potential to boost domain-specific applications and so this motivates us to evaluate the performance of sbert , a transformer architecture-based model , in a case_study of rhetorical_role labeling of sentences in legal_documents . we perform experiments using classification models and compare their performances through lexical_features and semantic features generated by sbert . we also employ the mixup data augmentation method with the semantic features . from the results , we conclude that exploiting the mixup method is beneficial and that the semantic features have a limited enhancing effect on the classification models of our case_study .
title : automatic kurdish text_classification using kdc 4007 dataset ; abstract : due to the large_volume of text documents uploaded on the internet daily . the quantity of kurdish documents which can be obtained via the web increases drastically with each passing day . considering news appearances , specifically , documents identified with categories , for example , health , politics , and sport appear to be in the wrong category or archives might be positioned in a nonspecific category called others . this paper is concerned with text_classification of kurdish text documents to placing articles or an email into its right class per their contents . even though there are considerable numbers of studies directed on text_classification in other languages , and the quantity of studies conducted in kurdish is extremely restricted because of the absence of openness , and convenience of datasets . in this paper , a new dataset named kdc-4007 that can be widely used in the studies of text_classification about kurdish news and articles is created . kdc-4007 dataset its file formats are compatible with well-known text_mining tools . comparisons of three best-known algorithms ( such as support_vector_machine ( svm ) , naïve bays ( nb ) and decision_tree ( dt ) classifiers ) for text_classification and tf × idf feature_weighting_method are evaluated on kdc-4007 . the paper also studies the effects of utilizing kurdish stemmer on the effectiveness of these classifiers . the experimental results indicate that the good accuracy value 91.03 % is provided by the svm classifier , especially when the stemming and tf × idf feature_weighting are involved in the preprocessing_phase . kdc-4007 datasets are available publicly and the outcome of this study can be further used in future as a baseline for evaluations with other classifiers by other researchers .
title : blockchain technology based information classificationmanagement service ; abstract : hyper-connectivity in industry 4.0 has resulted in not only a rapid increase in the amount of information , but also the expansion of areas and assets to be protected . in terms of information_security , it has led to an enormous economic cost due to the various and numerous security solutions used in protecting the increased assets . also , it has caused difficulties in managing those issues due to reasons such as mutual interference , countless security events and logs data , etc . within this security environment , an organization should identify and classify assets based on the value of data and their security perspective , and then apply appropriate protection measures according to the assets security classification for effective security management . but there are still difficulties stemming from the need to manage numerous security solutions in order to protect the classified assets . in this paper , we propose an information classificationmanagement service based on blockchain , which presents and uses a model of the value of data and the security perspective . it records transactions of classifying assets and managing assets by each class in a distributed ledger of blockchain . the proposed service reduces assets to be protected and security solutions to be applied , and provides security measures at the platform level rather than individual security solutions , by using blockchain . in the rapidly changing security environment of industry 4.0 , this proposed service enables economic security , provides a new integrated security platform , and demonstrates service value .
title : automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label ; while the second set was handled with 10 % of all the tweets contained in the database mentioned in the article , where the first set shows a polarity precision of 74.9 % , having logistic_regression as the best classifier using the classification scenario known as cross_validation , while the second set shows an accuracy of 78.5 % , also having random_forest as the best classifier using cross_validation as the best classification scenario .
title : identifying anatomical phrases in clinical_reports by shallow semantic parsing methods ; abstract : natural_language_processing ( nlp ) is being applied for several information_extraction tasks in the biomedical domain . the unique nature of clinical information requires the need for developing an nlp system designed specifically for the clinical domain . we describe a method to identify semantically coherent phrases within clinical_reports . this is an important step towards full syntactic parsing within a clinical nlp system . we use this semantic phrase chunker to identify anatomical phrases within radiology reports related to the genitourinary domain . a discriminative classifier based on support_vector_machines was used to classify words into one of ave phrase classification categories . training of the classifier was performed using 1000 hand-tagged sentences from a corpus of genitourinary radiology reports . features used by the classifier include n-grams , syntactic tags and semantic labels . evaluation was conducted on a blind test set of 250 sentences from the same domain . the system achieved overall performance scores of 0.87 ( precision ) , 0.91 ( recall ) and 0.89 ( balanced f-score ) . anatomical phrase_extraction can be rapidly and accurately accomplished . © 2007 ieee .
title : neural emotional response_generation via adversarial transfer_learning ; abstract : emotional response_generation is a key step to build an empathetic chatbot . however , previous emotional chatting models mainly focus on single-turn conversation , and multi-turn context emotional response_generation has not been explored . in this paper , we propose an adversarial transfer emotional chatting ( atec ) model for multi-turn conversation which is based on conditional variational autoencoders ( cvae ) . atec has two alternate training phases : supervised training and transfer training . in the supervised training stage , we train the cvae model , a content discriminator and an emotional classifier based on ground_truth corpus . and in the transfer training stage , we change the target emotion and use the content discriminator to force the model to transfer the multi-turn context information , while the emotional classifier regularizes the emotions expressed in the generated responses . experiments show that the proposed approach achieves state of the art performance with diverse responses and accurate emotional_expression .
title : automatic sentiment_analysis from opinion of thais speech audio ; abstract : automatic classification of sentiment is widely used in academia and industry by several techniques . this paper aims to develop a method of sentiment_analysis for thais customers to identify the different notions into two opinions ( positive or negative ) to consume the products . these opinions are represented by text that is derived from the thais speech audio content in social_media especially video reviews about beauty product . then , this work implements the model by the naïve_bayes text_classification . the results could be demonstrated that the method can provide more effectiveness and satisfactory accuracy for automatic sentiment_analysis .
title : social_media bullying detection using machine_learning on bangla_text ; abstract : with the popularity of unicode system and growing use of internet , the use of bangla over social_media is increasing . however , very few works have been done on bangla_text for social_media activity monitoring due to a lack of a large number of annotated_corpora , named dictionaries and morphological analyzer , which demands in-depth analysis on bangladesh 's perspective . moreover , solving the issue by applying available techniques is very content specific , which means that false detection can occur if contents changed from formal english to verbal_abuse or sarcasm . also , performance may vary due to linguistic differences between english and non-english contents and the socio-emotional behaviour of the study population . to combat such issues , this paper proposes the use of machine_learning algorithms and the inclusion of user information for cyber_bullying detection on bangla_text . for this purpose , a set of bangla_text has been collected from available social_media platforms and labelled as either bullied or not bullied for training different machine_learning based classification models . cross-validation results of the models indicate that a support_vector_machine based algorithm achieves superior performance on bangla_text with a detection accuracy of 97 % . besides , the impact of user specific information such as location , age and gender can further improve the classification_accuracy of bangla cyber_bullying detection system .
title : a multi-input multi-label claims channeling system using insurance-based language models ; abstract : servicing claims , a time consuming and labor-intensive task , plays a pivotal role in how insurance_companies serve their policyholders . claims may not get routed early enough in the process to the correct team , leading to dissatisfied customers because of inefficient claim 's management . claims departments need to process substantial amount of structured and unstructured_data to successfully route claims — a process referred to as channeling . the scope of the present work is limited to the auto insurance claims with a focus on four different downstream_classification_tasks including claims ’ fraud and bodily injuries . we propose a system that utilizes claims ’ notes and structured_data to build machine_learning models , which employ an insurance-based language model built by enhancing google 's bert , to route claims to domain_experts . the proposed channeling system successfully routes important claims to domain_experts for additional review , which can substantially_improve claims management and customer_satisfaction .
title : progress notes classification and keyword_extraction using attention-based deep_learning models with bert ; abstract : various deep_learning algorithms have been developed to analyze different types of clinical data including clinical_text classification and extracting information from 'free text ' and so on . however , automate the keyword_extraction from the clinical_notes is still challenging . the challenges include dealing with noisy clinical_notes which contain various abbreviations , possible typos , and unstructured sentences . the objective of this research is to investigate the attention-based deep_learning models to classify the de-identified clinical progress notes extracted from a real-world ehr system . the attention-based deep_learning models can be used to interpret the models and understand the critical words that drive the correct or incorrect classification of the clinical progress notes . the attention-based models in this research are capable of presenting the human interpretable text_classification models . the results show that the fine-tuned bert with the attention_layer can achieve a high classification_accuracy of 97.6 % , which is higher than the baseline fine-tuned bert classification model . in this research , we also demonstrate that the attention-based models can identify relevant keywords that are strongly related to the clinical progress note categories .
title : relationship classification in online social_network ; abstract : in today 's world sns i.e . social_networking sites have become an integral part of our day to day life . sns is the place where a person is free to express his views and opinions about others , share information with others . millions of visitors daily share their views and opinions on websites like twitter , myspace and facebook which produces enormous data . it has become important to analyze this data to find the emotions from this views and opinions_expressed by an individual . this paper focuses on extracting comment of users from social_networking sites by using text_mining processes . sentiment_analysis is the technique to processes those comments to find the emotions in the text of comments . along with this the paper explains how the friendship strength between two individuals commenting on single post can be determined using emotion mining .
title : predicting event time by classifying sub-level temporal_relations induced from a unified representation of time anchors ; abstract : extracting event time from news articles is a challenging but attractive task . in contrast to the most existing pair-wised temporal link annotation , reimers et al . ( 2016 ) proposed to annotate the time anchor ( a.k.a . the exact time ) of each event . their work represents time anchors with discrete representations of single-day/multi-day and certain/uncertain . this increases the complexity of modeling the temporal_relations between two time anchors , which can not be categorized into the relations of allen 's interval algebra ( allen , 1990 ) . in this paper , we propose an effective method to decompose such complex temporal_relations into sub-level relations by introducing a unified quadruple representation for both single-day/multi-day and certain/uncertain time anchors . the temporal_relation classifiers are trained in a multi-label_classification manner . the system structure of our approach is much simpler than the existing decision_tree model ( reimers et al. , 2018 ) , which is composed by a dozen of node classifiers . another contribution of this work is to construct a larger event time corpus ( 256 news documents ) with a reasonable inter-annotator_agreement ( iaa ) , for the purpose of overcoming the data shortage of the existing event time corpus ( 36 news documents ) . the empirical results show our approach_outperforms the state-of-the-art decision_tree model and the increase of data size obtained a significant improvement of performance .
title : fusion with language models improves spelling accuracy for erp-based brain computer interface spellers ; abstract : event_related potentials ( erp ) corresponding to a stimulus in electroencephalography ( eeg ) can be used to detect the intent of a person for brain computer interfaces ( bci ) . this paradigm is widely_utilized to build letter-by-letter text input systems using bci . nevertheless using a bci-typewriter depending only on eeg responses will not be sufficiently accurate for single-trial operation in general , and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed . hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed . in this paper , we study the effects of bayesian fusion of an n-gram language model with a regularized discriminant analysis erp detector for eeg-based bcis . the letter classification_accuracies are rigorously evaluated for varying language model orders as well as number of erp-inducing trials . the results demonstrate that the language models contribute significantly to letter classification_accuracy . specifically , we find that a bci-speller supported by a_4-gram language model may achieve the same performance using 3-trial erp classification for the initial letters of the words and using single trial erp classification for the subsequent ones . overall , fusion of evidence from eeg and language models yields a significant opportunity to increase the word rate of a bci based typing system . © 2011 ieee .
title : effect of incremental feature_enrichment on healthcare text_classification system : a machine_learning paradigm ; abstract : background and objective : healthcare tweets are particularly challenging due to its sparse layout and its limited character size . compared to previous method based on “ bag of words ” ( bow ) model , this study uniquely identifies the enrichment protocol and learns how semantically different aspects of feature_selection such as bow ( feature f0 ) , term_frequency_inverse_document_frequency ( tf-idf , feature f1 ) , and latent_semantic_indexing ( lsi , feature f2 ) when applied sequentially with classifier improves the overall performance . methods : to study this enrichment concept , our ml model is tested on two kinds of diverse data sets : ( i ) d1 : disease data with conjunctivitis , diarrhea , stomach ache , cough and nausea related_tweets , and ( ii ) d2 : webkb4 dataset , while adapting three kind of classifiers ( a ) c1 : support_vector_machine with radial_basis_function ( svmr ) , ( b ) c2 : multi-layer_perceptron ( mlp ) and ( c ) c3 : random_forest ( rf ) . partition protocol ( k10 ) was adapted with different performance_metrics to evaluate machine_learning ( ml ) -system . results : using the combination of f1 , c1 , d1 , k10 , ml accuracy was : 94 % , while with f2 , c1 , d1 , k10 , ml accuracy was 97 % . using the incremental feature_enrichment from f0 to f2 , k10 protocol gave f1 improvement over f0 by 4.98 % on disease dataset , while f2 improvement over f0 was by 11.78 % on webkb4 dataset . we demonstrated the generalization over memorization process in our ml-design . the system was tested for stability and reliability . conclusions : we conclude that semantically different aspects of feature_selection , when adapted sequentially , leads to improvement in ml-accuracy for healthcare data sets . we validated the system by taking non-healthcare data sets .
title : flaubert : unsupervised language model pre-training for french ; abstract : language models have become a key step to achieve state-of-the art results in many different natural_language_processing ( nlp ) tasks . leveraging the huge amount of unlabeled texts nowadays available , they provide an efficient way to pre-train continuous word_representations that can be fine-tuned for a downstream_task , along with their contextualization at the sentence_level . this has been widely demonstrated for english using contextualized_representations ( dai and le , 2015 ; peters et al. , 2018 ; howard and ruder , 2018 ; radford et al. , 2018 ; devlin_et_al. , 2019 ; yang et al. , 2019b ) . in this paper , we introduce and share flaubert , a model learned on a very large and heterogeneous french corpus . models of different sizes are trained using the new cnrs ( french_national_centre_for_scientific_research ) jean_zay supercomputer . we apply our french_language models to diverse nlp tasks ( text_classification , paraphrasing , natural_language inference , parsing , word_sense_disambiguation ) and show that most of the time they outperform other pre-training approaches . different versions of flaubert as well as a unified evaluation protocol for the downstream_tasks , called flue ( french_language understanding evaluation ) , are shared to the research community for further reproducible experiments in french nlp .
title : an enhanced artificial_neural_network based optical_character_recognition mechanism for business information_extraction and classification ; abstract : -- automated text processing and information_extraction have gained attention of many researchers as it plays a vital role in the context of business information_processing and extraction . with the advent of various artificial_intelligence technologies using neural_networks which can certainly overcome this problem . this paper proposes a neural_networks and natural_language_processing based approach for hand_written and optical_character_recognition . the proposed methodology is based on a combination of optical_character_recognition ( ocr ) and a named_entity_recognition ( ner ) model for classification . the ocr produces text for a given image ( businesscard ) with is further classified by a well trained nlp-ner model to extract names and other details such as emails phone numbers , websites . furthermore , the obtained results indicate that the proposed method provides high efficiency of text_classification inspite of unstructured_text and lack of sentence formation in text extracted from business cards . the results obtained were further improved by the scikit-learn classifier and achieved 97.5 % accuracy on a significantly large dataset .
title : a multi-scale convolutional attention_based gru network for text_classification ; abstract : neural_network models have been widely used in natural_language_processing ( nlp ) . recurrent_neural_network ( rnns ) has proved to be a powerful sequence model . gated_recurrent_unit ( gru ) is one kind of rnns which has achieved excellent_performance in nlp . nevertheless , because of the sparsity and high dimensionality of text data , there are some difficulties in complex semantic representations . to solve these problems , a novel and efficient method is proposed in this paper for text_classification . the proposed model is called multi-scale convolutional attention_based gru network ( mca-gru ) . in mca-gru , one-dimension convolutions with dense connections extract attention signals from text sequences . then the attention signals are combined with features of gru network . mca-gru is able to capture the local feature of phrases and sequence information . experimental verifications are conducted on five text_classification datasets . the results clearly show that the proposed model mca-gru approach achieves equivalent or even superior performance than other state-of-the-art text_classification methods .
title : sensitive_information detection using hmm & amp ; svm ; abstract : with the use of electronic data , the protection and detection of sensitive_information is an important issue for businesses and individuals . however , most sensitive detection methods focus on traditional_machine_learning_methods like cnn and rnn , which are traditional and inefficient compared to hmm and svm . this paper analyzes text information , using hmm and svm to detect the sensitive_information in the text_file . the conclusion is drawn out that besides traditional_machine_learning_methods , svm and hmm models can also help us do the classification problem to extract the focused information , which can not be exposed to the public .
title : hybrid optimization for feature_selection in opinion_mining ; abstract : a sub-discipline of information_retrieval ( ir ) is opinion_mining and the lexicon of computers is not concerned of the subject of the doc-ument , but about the opinion expressed . it has caused a large impact in the arena of academics and industry as it has a wide area of re-search and the applications are widespread . feature_selection is a vital step in opinion_mining , as its individual feature decides the opin-ions expressed by the customers . feature_selection reduces the dimensionality of data by avoiding non-relevant features ; it can be con-sidered as a necessary and excellent process for data mining applications . in this study , feature_subset is optimized through particle_swarm_optimization ( pso ) algorithm , cuckoo search ( cs ) algorithm and hybridized pso-cs algorithm . classification is done through naïve_bayes and k-nearest_neighbours ( knn ) classifiers . feature_extraction has its basis on term_frequency-inverse document fre-quency ( tf-idf ) . the accuracy of classification precision is increased by the reduction in size of feature_subset and computational com-plexity .
title : a deep action-oriented video image_classification system for text detection and recognition ; abstract : abstract : for the video images with complex actions , achieving accurate text detection and recognition results is very challenging . this paper presents a hybrid model for classification of action-oriented video images which reduces the complexity of the problem to improve text detection and recognition performance . here , we consider the following five categories of genres , namely concert , cooking , craft , teleshopping and yoga . for classifying action-oriented video images , we explore resnet50 for learning the general pixel-distribution level information and the vgg16 network is implemented for learning the features of maximally_stable_extremal regions and again another vgg16 is used for learning facial components obtained by a multitask cascaded convolutional_network . the approach integrates the outputs of the three above-mentioned models using a fully_connected_neural_network for classification of five action-oriented image classes . we demonstrated the efficacy of the proposed method by testing on our dataset and two other standard_datasets , namely , scene_text dataset dataset which contains 10 classes of scene_images with text information , and the stanford 40 actions dataset which contains 40 action classes without text information . our method_outperforms the related existing work and enhances the class-specific performance of text detection and recognition , significantly . article highlights : 1.the method uses pixel , stable-region and face-component information in a noble way for solving complex classification problems.2.the proposed work fuses different deep_learning models for successful classification of action-oriented images.3.experiments on our own dataset as well as standard_datasets show that the proposed model outperforms related state-of-the-art ( sota ) methods .
title : a study on machine_learning applied to software_bug priority prediction ; abstract : bugs are among the top problems faced by software developers . as the size and complexity of software projects increase so does the number of bugs and their complexity . bug priority prediction helps software developers focus their efforts on the most critical bugs that affect the core functionality of a software . by automating the process of priority prediction , it is possible to reduce the time spent analyzing new bug_reports . in this paper , we extract bug_reports from the bug_tracking software of six popular open-source projects hadoop , hbase , hdfs , mesos , spark , and mapreduce and apply five machine_learning classifiers multinomial_naive_bayes , decision_tree , logistic_regression , random_forest , adaboost to automatically predict bug priority using the title , description , and summary of the bug_report . we use tf-idf to extract useful features from the bug_reports and employ precision , recall , and f1-score for measuring the performance of the classifiers . a stratified 10-fold_cross-validation technique is used for model evaluation and the results are averaged over all 10 folds . we find that machine_learning applied to bug priority prediction provides excellent_results and can be used to significantly_reduce the time involved in the bug prioritization process . from our experiments , we observe that no single classifier consistently performs best on all priority levels and metrics across all datasets . however , trends from results show that multinomial_naive_bayes gives well balanced performance and is also fast to train and test . logistic_regression and adaboost also performed well and are potential alternatives .
title : iterative network pruning with uncertainty regularization for lifelong sentiment_classification ; abstract : lifelong_learning capabilities are crucial for sentiment classifiers to process continuous streams of opinioned information on the web . however , performing lifelong_learning is non-trivial for deep_neural_networks as continually training of incrementally available information inevitably results in catastrophic_forgetting or interference . in this paper , we propose a novel iterative network pruning with uncertainty regularization method for lifelong sentiment_classification ( iprls ) , which leverages the principles of network pruning and weight regularization . by performing network pruning with uncertainty regularization in an iterative manner , iprls can adapta single bert model to work with continuously arriving data from multiple_domains while avoiding catastrophic_forgetting and interference . specifically , we leverage an iterative pruning method to remove redundant parameters in large deep_networks so that the freed-up space can then be employed to learn new tasks , tackling the catastrophic_forgetting problem . instead of keeping the old-tasks fixed when learning new tasks , we also use an uncertainty regularization based on the bayesian online learning framework to constrain the update of old tasks weights in bert , which enables positive backward transfer , i.e . learning new tasks improves performance on past tasks while protecting old knowledge from being lost . in addition , we propose a task-specific low-dimensional residual function in parallel to each layer of bert , which makes iprls less prone to losing the knowledge saved in the base bert network when learning a new task . extensive_experiments on 16 popular review corpora demonstrate that the proposed iprls method sig-nificantly outperforms the strong_baselines for lifelong sentiment_classification . for reproducibility , we submit the code and data at : https : //github.com/siat-nlp/iprls .
title : a survey of document_image_classification : problem_statement , classifier architecture and performance evaluation ; abstract : document_image_classification is an important step in office automation , digital_libraries , and other document_image analysis applications . there is great diversity in document_image classifiers : they differ in the problems they solve , in the use of training_data to construct class models , and in the choice of document features and classification_algorithms . we survey this diverse literature using three components : the problem_statement , the classifier architecture , and performance evaluation . this brings to light important issues in designing a document classifier , including the definition of document classes , the choice of document features and feature_representation , and the choice of classification algorithm and learning mechanism . we emphasize techniques that classify single-page typeset document_images without using ocr results . developing a general , adaptable , high-performance classifier is challenging due to the great variety of documents , the diverse criteria used to define document classes , and the ambiguity that arises due to ill-defined or fuzzy document classes . ©_springer-verlag 2006 .
title : evidence weighted tree ensembles for text_classification ; abstract : text documents are often mapped to vectors of binary values where 1 indicates the presence of a word and 0 indicates the absence . the vectors are then used to train predictive_models . in tree-based ensemble_models , predictions from some decision_trees may be made purely from absent words . this type of predictions should be trusted less as absent words can be interpreted in multiple ways . in this work , we propose to improve the comprehensibility and accuracy of ensemble_models by distinguishing word presence and absence . the presented method weights predictions based on word presence . experimental results on 35 real text datasets indicate that our method_outperforms state-of-the-art ensemble_methods on various text_classification tasks .
title : sentiment_analysis in brazilian_portuguese tweets in the domain of calamity : application of the summarization method and semantic similarity in polarized terms ; abstract : this research integrates an interdisciplinary project which mobilizes the areas of computer engineering , linguistics and communication to perform the processing of texts in a natural_language extracted from microblogging service twitter as well as to conduct an analysis and classification of the sentiments mined . many proposals have been formulated using the polarization method ; however , most projects do not encompass an automatic classification by semantic proximity . this research aims to evaluate the reaction of individuals shared in the social_network , not only to classify them as positive or negative , but also to ascertain the semantic similarity of these messages in the same domain . based on the set of tweets in portuguese extracted from a corpus of calamity , we apply three methods : a ) the lexical classifier , called summarization method ; b ) the semantic classifier , called lsa - latent_semantic_analysis ; c ) the asstps classifier - analysis of semantic similarity in polarized and summarized terms . the results are applied to a set of 811 tweets of the calamity domain and point out which method obtained the best hit rate and semantic approximation . in this sense , the classification of sentiments by semantic proximity can help greatly , performing the sorting of content of relevant messages , discarding unnecessary information , linking messages with the same theme in common , and even generating metrics for classifying emotions .
title : leveraging evaluation_metric-related training_criteria for speech_summarization ; abstract : many of the existing machine-learning approaches to speech_summarization cast important sentence selection as a two-class classification problem and have shown empirical success for a wide variety of summarization tasks . however , the imbalanced-data problem sometimes results in a trained speech summarizer with unsatisfactory_performance . on the other hand , training the summarizer by improving the associated classification_accuracy does not always lead to better summarization evaluation performance . in view of such phenomena , we hence investigate two different training_criteria to alleviate the negative effects caused by them , as well as to boost the summarizer 's performance . one is to learn the classification capability of a summarizer on the basis of the pair-wise ordering information of sentences in a training document according to a degree of importance . the other is to train the summarizer by directly maximizing the associated evaluation score . experimental results on the broadcast news summarization task show that these two training_criteria can give substantial_improvements over the baseline svm summarization system . ©2010 ieee .
title : finding the real subject : the application of categorisation methods to forum messages ; abstract : the growth of interactive communities and multimedia nets seems to be uncontrollable to the extent that both human and global_communication are impossible today without the internet . nowadays computer_mediated_communication has created and developed on-line communities that well in digitalized territories . this new segment of social_science is mostly built up of words and the necessary strategy and tools should be aimed at enabling the scanning , the attentive reading , the understanding and the explanation of such segment . on-line communities are perfect places to study linguistic behaviours and mass interactions . the virtual space in which people can speak are chat lines , mailing_lists , forums , etc . in a forum we have the possibility to send one or more messages , participating to the discussion of a specific topic , in this case wine . this forum gathers specialists , technicians and ordinary people who end up becoming a real community , where information and news about wine are frequently swapped . the study of a forum is a method normally used to verify customer_satisfaction ; in our case we want to find out if there is a parallelism between the titles of the subject and the content of the messages . because we frequently answer messages without changing their title , to monitor only these titles is not enough for marketing or communication specialists if they want to obtain an exhaustive description of customer communication . the purpose of this work is to obtain a new list of subjects for each message . for this reason we use text_mining techniques which allow us to look for sets of words inside the texts . taltac entity search utility has been used in order to search a distinctive word_sequence inside fragments by using complex_queries with regular_expression .
title : ensembles of classifiers for parallel categorization of large number of text documents expressing_opinions ; abstract : opinions provided by people that used some services or purchased some goods are a rich source of knowledge . the opinion classification , applying mostly supervised_classifiers , is one of the essential tasks . computer ’ s technological capabilities are still a major obstacle , especially when processing huge volumes of data . this study proposes and evaluates experimentally a parallelism application to the classification of a very large number of contrary opinions_expressed as freely written text reviews . instead of training a single classifier on the entire data_set , an ensemble of classifiers is trained on disjunctive subsets of data and a group decision is used for the classification of unlabelled items . the main assessment criteria are computational_efficiency and error_rates , combined into a single measure to be able to compare ensembles of different sizes . support_vector_machines , artificial_neural_networks , and decision_trees , belonging to frequently used classification methods , were examined . the paper demonstrates the suggested method viability when the number of text reviews leads to computational_complexity , which is beyond the contemporary common pc ’ s capabilities . classification_accuracy and the values of other classification performance_measures ( precision , recall , f-measure ) did not decrease , which is a positive finding .
title : bigram based deep_neural_network for extremism detection in online user_generated contents in the kazakh_language ; abstract : countering the spread of aggressive information and extremism in the global network is an urgent problem of society and government agencies , which is solved in particular by filtering unwanted internet resources . a necessary condition for such filtering is the classification of the content of websites , texts and documents of the information flow . therefore , an urgent problem of information technologies is the classification of texts in natural_languages in order to detect extremist texts , such as calls for extremism and other messages that threaten the security of citizens . therefore , our research examines the detection of extremist messages in online content in the kazakh_language . to do this , we have collected a corpus of extremist texts from open sources , developed a deep_neural_network based on bigrams for detecting extremist texts in the kazakh_language . the proposed model has shown high efficiency in comparison with classical methods of machine_learning and deep_learning .
title : criminal activity detection in social_network by text_mining : comprehensive analysis ; abstract : criminal activity detection in social_network by text_mining is the process of finding criminal activity by the criminals and help law text_mining technique , the ability to detect hidden text from corpus documents . text_mining is process of transforming data from unstructured_text to structured text which is easily perceived and processed by humans , but hard for machines to understand without designing algorithms , tools and methods in order to effectively process , such enforcing agencies to keep control of the prevailing crimes text_mining is method deriving high-quality information from raw_data through the pattern devising and statistical pattern learning . text_mining is field a multidisciplinary field that relies on data_mining , information_retrieval , statistics , machine_learning , and computational_linguistics . the main thing in text_mining process of analyzing and exploring is natural_language_processing , information_retrieval , information_extraction , content_analysis , text clustering , and text_classification . all that processes are wanted after you complete a step , the preprocess task . the importance of pre-processing task is to reduce the volume of the corpus textual_documents and the tasks involved in that step are text boundary determinant , natural_language specific stemming stop-word , elimination , and tokenization to remove unwanted data and handling missing data . among this , doing the most important work is tokenization . tokenization assist to divide the text data to individual words , open_source tools become available for those interested such as spacy , nltk with python , gensim and many other . after that define model architecture to fit the model on the training_data and evaluate this model on test simple data in order to predict values .
title : ranking of daily deals with concept expansion ; abstract : daily deals have emerged in the last three years as a successful form of online_advertising . the downside of this success is that users are increasingly overloaded by the many thousands of deals offered each day by dozens of deal providers and aggregators . the challenge is thus offering the right deals to the right users i.e. , the relevance_ranking of deals . this is the problem we address in our paper . exploiting the characteristics of deals data , we propose a combination of a term- and a concept-based retrieval model that closes the semantic gap between queries and documents expanding both of them with category information . the method consistently_outperforms state-of-the-art methods based on term-matching alone and existing_approaches for ad classification and ranking .
title : critical_discourse_analysis on women ’ s position in prohaba daily news texts ; abstract : this research aimed to describe women ’ s position in prohaba daily news texts based on sara mills and theo van leeuwen perspective of critical_discourse_analysis , especially the analysis of actor position , exclusion and inclusion . this is a descriptive qualitative_research in which data were collected by documentation technique . the data were prohaba daily news texts during 2018 . the data were analyzed using sara mills ’ actor position analysis model and theo van leeuwen ’ s exclusion and inclusion analysis . actor position analysis included subject position and object position . exclusion analysis included the passivation , nominalization , and substitution of clauses . while the inclusion analysis included differentiation-indifferentiation , objectivity-abstraction , nomination-identification , nomination-categorization , determination-indetermination , assimilation-individualization , and association-disassociation . the results showed that prohaba daily news texts positioned female actors in subject and object position in their news texts . women as non-marginalized subject found in three news texts . women as non-marginalized objects found in two news texts . women in the marginalized object position found in eight news texts . marginalization was conducted by using exclusion and inclusion strategies . the exclusion strategy used includes the nomination and substitution of clauses . inclusion strategies used were differentiation-indifferentiation , objectivity-abstraction , nomination-identification , nomination-categorization , and association-disassociation . in addition , the use of certain vocabularies can marginalize the position of women in the daily news text prohaba .
title : curriculum based discriminative language model training ; abstract : discriminative language modeling is a technique used for correcting automatic_speech_recognition errors , and can be handled as a classification or a ranking problem . the aim of curriculum learning is to train the model with examples or concepts of gradually increasing level of difficulty . in this work , we use the classification and ranking versions of the perceptron algorithm and investigate three different curriculum learning approaches based on selection , ordering and clustering of the training_examples . the results show that curriculum learning can help increase the performance of a classifying perceptron system , and with the ranking perceptron , it is possible achieve similar system performance with a shorter training time . © 2013 ieee .
title : generating pseudo connectives with mlms for implicit_discourse_relation recognition ; abstract : due to the lack of connectives , the recognition of implicit discourse_relations faces a big challenge . an early attempt overcomes this difficulty by predicting connectives with the use of the statistical language model . recent_years have witnessed the great_success of masked_language_models ( mlm ) . then a new problem naturally arises , i.e. , how can connectives benefit implicit_discourse_relation_classification from such models ? in this paper , we address this problem by developing a novel framework to generate the pseudo connectives using the pre-trained mlm . the key idea is to treat the absent connectives as missing words between two arguments and produce the pseudo connective from its contexts by fine-tuning mlm on the classification_task . moreover , we leverage the real connectives in explicit discourse_relations to supervise the generation of pseudo connectives . extensive_experiments show that our model achieves the state-of-the-art performance on the pdtb benchmark .
title : prominent feature_extraction for review analysis : an empirical_study ; abstract : sentiment_analysis ( sa ) research has increased tremendously in recent_times . sa aims to determine the sentiment_orientation of a given text into positive or negative_polarity . motivation for sa research is the need for the industry to know the opinion of the users about their product from online portals , blogs , discussion boards and reviews and so on . efficient features need to be extracted for machine-learning algorithm for better sentiment_classification . in this paper , initially various features are extracted such as unigrams , bi-grams and dependency features from the text . in addition , new bi-tagged features are also extracted that conform to predefined part-of-speech patterns . furthermore , various composite features are created using these features . information gain ( ig ) and minimum_redundancy maximum relevancy ( mrmr ) feature_selection methods are used to eliminate the noisy and irrelevant_features from the feature_vector . finally , machine-learning algorithms are used for classifying the review document into positive or negative class . effects of different categories of features are investigated on four standard data-sets , namely , movie review and product ( book , dvd and electronics ) review data-sets . experimental results show that composite features created from prominent features of unigram and bi-tagged features perform better than other features for sentiment_classification . mrmr is a better feature_selection method as compared with ig for sentiment_classification . boolean multinomial_naïve_bayes ) algorithm performs better than support_vector_machine classifier for sa in terms of accuracy and execution time .
title : automatic evaluation of document_classification using n-gram statistics ; abstract : due to the development of world_wide_web technologies , people are living in the place flooding trillions of web_pages in every moment . the amount of web size has been increasing dramatically . for this reason , it is getting more difficult to find relevant web_documents corresponding to what users want to read . classifying documents into predefined_categories is one of the most important tasks in natural_language_processing field . over the years , many statistical and linguistical approaches have been applied to overcome traditional classification machine . however , it still remains in unsolved problem . there is a no perfect solution to machine understand human language yet . we have to consider every possibility for making machine think like human does . in this paper , we propose a method for classifying textural document using n-gram co-occurrence statistics which have a great possibility to find similarities between given documents . we also compare our proposed method with traditional method suggested by keselj . this paper only covers simple approaches and still needs more sophisticated experiments . however , the performance using this method is better than the keselj approach . © 2012 ieee .
title : analyzing the effectiveness and applicability of co-training ; abstract : recently there has been significant interest in supervised_learning algorithms that combine labeled and unlabeled_data for text learning tasks . the co-training setting [ 1 ] applies to datasets that have a natural separation of their features into two disjoint_sets . w e demonstrate that when learning from labeled and unlabeled_data , algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not . when a natural split does not exist , co-training algorithms that manufacture a feature split may out-perform algorithms not using a split . these results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers .
title : natural_language inference prompts for zero-shot emotion classification in text across corpora ; abstract : within textual emotion classification , the set of relevant labels depends on the domain and application scenario and might not be known at the time of model development . this conflicts with the classical paradigm of supervised_learning in which the labels need to be predefined . a solution to obtain a model with a flexible set of labels is to use the paradigm of zero-shot learning as a natural_language inference task , which in addition adds the advantage of not needing any labeled_training_data . this raises the question how to prompt a natural_language inference model for zero-shot learning emotion classification . options for prompt formulations include the emotion name anger alone or the statement `` this text expresses anger '' . with this paper , we analyze how sensitive a natural_language inference-based zero-shot-learning classifier is to such changes to the prompt under consideration of the corpus : how carefully does the prompt need to be selected ? we perform experiments on an established set of emotion datasets presenting different language registers according to different sources ( tweets , events , blogs ) with three natural_language inference models and show that indeed the choice of a particular prompt formulation needs to fit to the corpus . we show that this challenge can be tackled with combinations of multiple prompts . such ensemble is more robust across corpora than individual prompts and shows nearly the same performance as the individual best prompt for a particular corpus .
title : enhancing the generalization for intent_classification and out-of-domain detection in slu ; abstract : intent_classification is a major task in spoken_language_understanding ( slu ) . since most models are built with pre-collected in-domain ( ind ) training utterances , their ability to detect unsupported out-of-domain ( ood ) utterances has a critical effect in practical use . recent_works have shown that using extra data and labels can improve the ood detection performance , yet it could be costly to collect such data . this paper proposes to train a model with only ind data while supporting both ind intent_classification and ood detection . our method designs a novel domain-regularized module ( drm ) to reduce the overconfident phenomenon of a vanilla classifier , achieving a better generalization in both cases . besides , drm can be used as a drop-in replacement for the last layer in any neural_network-based intent_classifier , providing a low-cost strategy for a significant improvement . the evaluation on four datasets shows that our method built on bert and roberta models achieves state-of-the-art performance against existing_approaches and the strong_baselines we created for the comparisons .
title : evaluating on-line courses via reviews mining ; abstract : because the participants are not limited by age- , gender- , race- , or geography-related barriers , recently , massive_open_online courses ( mooc ) have witnessed remarkable growth in number of online self-learners , courses providers and online_platforms . mooc learners usually share some learning experiences and release millions of course-related comments in discussion_forum . on the one hand , these comments could reflect the learners ' attitudes toward the online_courses . on the other hand , semantic knowledge hidden in these comments would assist other learners to choose the appropriate courses and help instructors to improve their courses ' attraction . recently , few research works focus on evaluating the courses through reviews mining . thus , this paper constructs a curriculum evaluation system based on mooc reviews , which quantifies the curriculum from different topics . firstly , we employ latent_dirichlet_allocation ( lda ) to mine the reviews generated by students , and obtain a topic-word distribution matrix and a comment-topic_distribution matrix which can describe the topics of the course comments . next , the emotion values of the comments in each topic are calculated by the auto-encoder and bi-lstm text_classification model . we utilize the emotions and the quantified scores of the courses on different topics to establish a comprehensive curriculum evaluation system . the experimental results show that there are five main indicators abstracted from students ' reviews , which are instructor , course content , course assessment , mooc platform , and hot courses . moreover , comment texts of each course under different evaluation_indicators are objectively and accurately converted into numerical marks , which can provide the students and educators with reliable references .
title : multi-label_classification of legislative contents with hierarchical label attention networks ; abstract : eurovoc is a thesaurus maintained by the european_union publication office , used to describe and index legislative documents . the eurovoc concepts are organized following a hierarchical_structure , with 21 domains , 127 micro-thesauri terms , and more than 6,700 detailed descriptors . the large number of concepts in the eurovoc thesaurus makes the manual classification of legal_documents highly costly . in order to facilitate this classification work , we present two main contributions . the first one is the development of a hierarchical deep_learning model to address the classification of legal_documents according to the eurovoc thesaurus . instead of training a classifier for each hierarchy level , our model allows the simultaneous prediction of the three levels of the eurovoc thesaurus . our second contribution concerns the proposal of a new legal corpus for evaluating the classification of documents written in portuguese . this corpus , named eur-lex pt , contains more than 220k documents , labeled under the three eurovoc hierarchical_levels . comparative_experiments with other state-of-the-art models indicate that our approach has competitive_results , at the same time offering the ability to interpret predictions through attention_weights .
title : adversarial_training for community question_answer selection based on multi-scale matching ; abstract : community-based question_answering ( cqa ) websites represent an important source of information . as a result , the problem of matching the most valuable answers to their corresponding questions has become an increasingly_popular research topic . we frame this task as a binary ( relevant/irrelevant ) classification problem , and present an adversarial_training framework to alleviate label imbalance_issue . we employ a generative_model to iteratively sample a subset of challenging negative_samples to fool our classification model . both models are alternatively optimized using reinforce algorithm . the proposed method is completely different from previous ones , where negative_samples in training_set are directly used or uniformly down-sampled . further , we propose using multi-scale matching which explicitly inspects the correlation between words and ngrams of different levels of granularity . we evaluate the proposed method on semeval_2016 and semeval_2017 datasets and achieves state-of-the-art or similar performance .
title : information-theoretic method for assessing the quality of translations ; abstract : in recent_years , the task of translating from one language to another has attracted wide attention from researchers due to numerous practical uses , ranging from the translation of various texts and speeches , including the so-called “ machine ” translation , to the dubbing of films and numerous other video materials . to study this problem , we propose to use the information-theoretic method for assessing the quality of translations . we based our approach on the classification of sources of text variability proposed by a.n . kolmogorov : information content , form , and unconscious author ’ s style . it is clear that the unconscious “ author ’ s ” style is influenced by the translator . so researchers need special methods to determine how accurately the author ’ s style is conveyed , because it , in a sense , determines the quality of the translation . in this paper , we propose a method that allows us to estimate the quality of translation from different translators . the method is used to study translations of classical english-language works into russian and , conversely , russian classics into english . we successfully used this method to determine the attribution of literary_texts .
title : on exploiting transformers for detecting explicit song lyrics ; abstract : determining if the lyrics of a given song could be hurtful or inappropriate for children is of utmost_importance to prevent the reproduction of songs whose textual_content is unsuitable for them . this problem can be computationally tackled as a binary_classification task , and in the last couple of years various machine_learning approaches have been applied to perform this task automatically . in this work , we investigate the automatic detection of explicit song lyrics by leveraging transformer-based language models , i.e. , large language representations , unsupervisely built from huge textual corpora , that can be fine-tuned on various natural_language_processing tasks , such as text_classification . we assess the performance of various transformer-based language model classifiers on a dataset consisting of more than 800k lyrics , marked with explicit information . the evaluation shows that while the classifiers built with these powerful tools achieve state-of-the-art performance , they do not outperform lighter and computationally less demanding approaches . we complement this empirical_evaluation with further analyses , including an assessment of the performance of these classifiers in a few-shot learning scenario , where they are trained with just few thousands of samples .
title : evolving customer_experience management in internet_service_provider company using text_analytics ; abstract : customer_experience is of crucial significance to the constant growth of a business . it is necessary to ensure great customer_experience , thus maintaining customer loyalty and satisfaction . an approach that intended to develop and improve customer_experience is called customer_experience management ( cem ) . cem is a strategy practiced to track , supervise , and arrange all synergy to help a business focal point on the needs of its customers . this research uses sentiment_analysis and topic_modeling to analyze the experience of internet_service_provider customers . the output of this research expected to drive the strategies change in cem . this research uses data taken from customer tweets on twitter . it is considering that the data on social_media is enormous and unstructured . therefore , classification using naive_bayes_classifier applied to assist and expedite in the sentiment_analysis process . the classification for sentiment_analysis using nbc gained accuracy above 82 % . hence , the classification models using nbc achieve excellent capability for sentiment_analysis . to determining topics that often discussed by customers , this research uses the latent_dirichlet_allocation models for topic_modeling .
title : fact versus opinion questions classification and answering : challenges and keys ; abstract : this article presents a study of the challenges and possible solutions to the issues raised by opinion ( multi-perspective ) question_answering in a non-traditional textual genre setting . we show why this task is more difficult than traditional question_answering and what additional methods , tools and resources are needed to solve it . we test our different hypotheses on question_classification answer retrieval and validation on mixed fact/opinion question sets , and opinion questions and annotated answers from two different genres : opqa , a corpus of newspaper articles and emotiblog , the blog post corpus we created and annotated . we discuss on our findings , drawing conclusions and tracing lines for future work .
title : a document classifier for medicinal_chemistry publications trained on the chembl corpus ; abstract : background : the large increase in the number of scientific_publications has fuelled a need for semi- and fully_automated text_mining approaches in order to assist in the triage process , both for individual scientists and also for larger-scale data extraction and curation into public databases . here , we introduce a document classifier , which is able to successfully distinguish between publications that are 'chembl-like ' ( i.e . related to small_molecule drug_discovery and likely to contain quantitative bioactivity data ) and those that are not . the unprecedented size of the medicinal_chemistry literature collection , coupled with the advantage of manual curation and mapping to chemistry and biology make the chembl corpus a unique resource for text_mining .
title : grammar rule-based sentiment categorization model for tamil tweets ; abstract : the widespread of social_media is growing every day where users are sharing their opinions , reviews , and comments on an item or product . the aim is to develop a model to mine user tweets collected from twitter . in this paper , our contribution on user tweets to find the sentiments_expressed by users about tamil movies based on the grammar rule . tamil movies domain is selected to confine our scope of the work . after preprocessing , n-gram approach is applied to classify tweets into different genres . this work intends to find the polarity of tamil tweets in addition to genre classification . in this work , it is also shown how to collect user tweets which comes as data_stream using modified n-gram approach to predict the sentiments of the users in the dataset . results suggest that n-gram model not only remove the complexity of natural_language process but also help to improve the decision-making process .
title : improved frame_level features and svm supervectors approach for the recogniton of emotional_states from speech : application to categorical and dimensional states ; abstract : the purpose of speech_emotion_recognition system is to classify speakers utterances into different emotional_states such as disgust , boredom , sadness , neutral and happiness . speech features that are commonly used in speech_emotion_recognition rely on global utterance_level prosodic_features . in our work , we evaluate the impact of frame_level feature_extraction . the speech_samples are from berlin emotional database and the features extracted from these utterances are energy , different variant of mel frequency cepstrum coefficients , velocity and acceleration features .
title : semantic-interactive graph_convolutional_network for multilabel image_recognition ; abstract : multilabel image_recognition , a critically practical task in computer vision , aims to predict multiple objects present in each image . the existing_studies mainly focus on conceptual visual_cues but fail to reconcile the visual information with their semantic guidance . intuitively , humans can not only associate extra topological concepts but also imagine other approximate scenes based on a semantic description . inspired by such semantic-interactive capability , two different types of semantic priors , i.e. , the concept correlations of the same scene and semantic similarities among different scenes , should be further explored for the recognition decisions . to efficiently interact with these semantic relationships , in this article , we propose a novel semantic-interactive graph_convolutional_network ( si-gcn ) , which can leverage the topological information learned from knowledge graphs to boost the performance of multilabel recognition . specifically , the proposed si-gcn framework consists of two different gcn-based branches in parallel , i.e. , concept correlations learning ( ccl ) branch and semantic similarity learning ( ssl ) branch . inputting the semantic-embedding_vectors of all the concepts , the ccl branch maps the label co-occurrence graph into a set of interdependent concept classifiers . recalibrating the image feature embedding with the standardized supervision of the semantic similarity graph , the ssl branch learns the semantically consistent in-batch visual representations . finally , a well-established interactive learning scheme is formulated to concurrently optimize the obtained concept classifiers and the visual representation_learning in an end-to-end manner . extensive_experiments on the ms-coco and pascal voc 2007 & 2012 benchmarks demonstrate the superiorities of the proposed si-gcn method compared to the state-of-the-art baselines .
title : a novel kernel for text_classification based on semantic and statistical information ; abstract : in text_categorization , a document is usually represented by a vector_space_model which can accomplish the classification_task , but the model can not deal with chinese synonyms and polysemy phenomenon . this paper presents a novel approach which takes into account both the semantic and statistical information to improve the accuracy of text_classification . the proposed approach computes semantic information based on hownet and statistical information based on a kernel_function with class-based weighting . according to our experimental results , the proposed approach could achieve state-of-the-art or competitive_results as compared with traditional_approaches such as the k-nearest_neighbor ( knn ) , the naive_bayes and deep_learning models like convolutional_networks .
title : egfi : drug-drug_interaction extraction and generation with fusion of enriched entity and sentence information ; abstract : the rapid_growth in literature accumulates diverse and yet comprehensive biomedical knowledge hidden to be mined such as drug interactions . however , it is difficult to extract the heterogeneous knowledge to retrieve or even discover the latest and novel knowledge in an efficient_manner . to address such a problem , we propose egfi for extracting and consolidating drug interactions from large-scale medical literature text data . specifically , egfi consists of two parts : classification and generation . in the classification part , egfi encompasses the language model biobert which has been comprehensively pre-trained on biomedical corpus . in particular , we propose the multi-head_attention_mechanism and pack bigru to fuse multiple semantic information for rigorous context modeling . in the generation part , egfi utilizes another pre-trained_language_model biogpt-2 where the generation sentences are selected based on filtering_rules . we evaluated the classification part on `` ddis 2013 '' dataset and `` dtis '' dataset , achieving the fi score of 0.842 and 0.720 respectively . moreover , we applied the classification part to distinguish high-quality generated sentences and verified with the exiting growth truth to confirm the filtered sentences . the generated sentences that are not recorded in drugbank and ddis 2013 dataset also demonstrate the potential of egfi to identify novel drug relationships .
title : classification ensemble to improve medical named_entity_recognition ; abstract : an accurate named_entity_recognition ( ner ) is important for knowledge discovery in text_mining . this paper proposes an ensemble machine_learning approach to recognise named_entities ( nes ) from unstructured and informal medical text . specifically , conditional_random_field ( crf ) and maximum_entropy ( me ) classifiers are applied individually to the test data set from the i2b2 2010 medication challenge . each classifier is trained using a different set of features . the first set focuses on the contextual features of the data , while the second concentrates on the linguistic features of each word . the results of the two classifiers are then combined . the proposed approach achieves an f-score of 81.8 % , showing a considerable improvement over the results from crf and me classifiers individually which achieve f-scores of 76 % and 66.3 % for the same data set , respectively .
title : improved hierarchical patient classification with language model pretraining over clinical_notes ; abstract : clinical_notes in electronic_health_records contain highly heterogeneous writing_styles , including non-standard terminology or abbreviations . using these notes in predictive_modeling has traditionally required preprocessing ( e.g . taking frequent_terms or topic_modeling ) that removes much of the richness of the source data . we propose a pretrained hierarchical recurrent_neural_network model that parses minimally processed clinical_notes in an intuitive fashion , and show that it improves performance for discharge diagnosis classification_tasks on the medical information mart for intensive_care iii ( mimic-iii ) dataset , compared to models that treat the notes as an unordered collection of terms or that conduct no pretraining . we also apply an attribution technique to examples to identify the words that the model uses to make its prediction , and show the importance of the words ' nearby context .
title : static analysis through topic_modeling and its application to malware programs classification ; abstract : we perform static analysis of malware programs in the big 2015 dataset , a repository containing nine different families of malware programs . our main_goal is to provide a framework for classification of the programs in the dataset . our analysis of the programs is static in the sense that the contents of the said programs are looked at and their representations are constructed without executing the programs . more precisely , assembly_language opcodes are extracted from the programs in the dataset and concatenated in order to construct documents representing these programs . opcodes being words , we then employ natural_language_processing tools and techniques for analysis of the documents . mainly , the latent_dirichlet_allocation ( lda ) algorithm is used to model documents as weighted mixtures of a fixed number of topics . a topic is a collection of words grouped together for their ability to capture meaningful attributes about the documents . we note that the weight_distribution of topics within documents of the same family ( visually ) shows a common pattern that seemingly varies from one family to another . this , therefore , aids in justifying the use of the lda technique as a feature_extraction method , with the features here being the weights of the topics representing each and every document . ensuing , after training a fine k-nearest_neighbors classifier , which takes topic weights as inputs , testing results show a 97.2 % classification_accuracy , thereby attesting to the efficacy of the overall approach .
title : analysis and safety_engineering of fuzzy string_matching algorithms ; abstract : in this paper we explore fuzzy string_matching in an automatic ticket_classification and processing system . we compare performance of the following string_similarity algorithms : longest_common subsequence ( lcs ) , dice coefficient , cosine_similarity , levenshtein ( edit ) distance and damerau distance . through optimisation , we accomplished a 15 % improvement in the ratio of false_positives to true_positive classifications over the existing approach used by a customer_support system for free customers . to introduce greater safety ; we compliment fuzzy string_matching algorithms with a second layer convolutional_neural_network ( cnn ) binary classifier , achieving an improved keyword classification ratio for two ticket categories by a relative 69 % and 78 % . such an approach allows for classification to only be applied where a desired level of safety achieved , such as in instances where automated answers .
title : learning to few-shot learn across diverse natural_language classification_tasks ; abstract : self-supervised pre-training of transformer models has shown enormous success in improving performance on a number of downstream_tasks . however , fine-tuning on a new task still requires large amounts of task-specific labelled_data to achieve good performance . we consider this problem of learning to generalize to new tasks with few examples as a meta-learning problem . while meta-learning has shown tremendous progress in recent_years , its application is still limited to simulated problems or problems with limited diversity across tasks . we develop a novel method , leopard , which enables optimization-based meta-learning across tasks with different number of classes , and evaluate different methods on generalization to diverse nlp classification_tasks . leopard is trained with the state-of-the-art transformer architecture and shows better generalization to tasks not seen at all during training , with as few as 4 examples per label . across 17 nlp tasks , including diverse_domains of entity_typing , natural_language inference , sentiment_analysis , and several other text_classification tasks , we show that leopard learns better initial parameters for few-shot learning than self-supervised pre-training or multi-task training , outperforming many strong_baselines , for example , yielding 14.5 % average relative gain in accuracy on unseen tasks with only 4 examples per label .
title : investor_sentiment identification based on the universum svm ; abstract : universum refers to additional samples which contain priori_knowledge for classification but belonging to none of the class . it has been proved that universum positioned “ in between ” the two classes obtain better results . since opinions on stock_market defined as investor_sentiment involve quite a number of neutral views , these neutral views can be used as universum samples to better identify investor_sentiment . with universum samples , this paper uses support_vector_machine ( svm ) to classify posts on stock forum . we define bullish views as positive samples , define bearish views as negative_samples , and also further discuss the situation of a 3-class problem with neutral views . compared with standard svm , the empirical_studies with universum samples in this paper show better performance for both 2- and 3-class classifications .
title : forestexter : an efficient random_forest algorithm for imbalanced text_categorization ; abstract : in this paper , we propose a new random_forest ( rf ) based ensemble_method , forestexter , to solve the imbalanced text_categorization problems . rf has shown great_success in many real-world applications . however , the problem of learning from text data with class_imbalance is a relatively new challenge that needs to be addressed . a rf algorithm tends to use a simple random_sampling of features in building their decision_trees . as a result , it selects many subspaces that contain few , if any , informative_features for the minority_class . furthermore , the gini measure for data splitting is considered to be skew sensitive and bias towards the majority_class . due to the inherent complex characteristics of imbalanced text datasets , learning rf from such data requires new approaches to overcome challenges related to feature_subspace selection and cut-point choice while performing node splitting . to this end , we propose a new tree induction method that selects splits , both feature_subspace selection and splitting criterion , for rf on imbalanced text data . the key idea is to stratify features into two groups and to generate effective term_weighting for the features . one group contains positive features for the minority_class and the other one contains the negative features for the majority_class . then , for feature_subspace selection , we effectively select features from each group based on the term_weights . the advantage of our approach is that each subspace contains adequate informative_features for both minority and majority_classes . one difference between our proposed tree induction method and the classical rf method is that our method uses support_vector_machines ( svm ) classifier to split the training_data into smaller and more balance subsets at each tree node , and then successively retrains the svm classifiers on the data partitions to refine the model while moving down the tree . in this way , we force the classifiers to learn from refined feature subspaces and data subsets to fit the imbalanced_data better . hence , the tree model becomes more robust for text_categorization task with imbalanced_dataset . experimental results on various benchmark imbalanced text datasets ( reuters-21578 , ohsumed , and imbalanced 20_newsgroup ) consistently demonstrate the effectiveness of our proposed forestexter method . the performance of our proposed approach is competitive against the standard random_forest and different variants of svm algorithms . © 2014 elsevier b.v. all rights_reserved .
title : a dcrc model for text_classification ; abstract : traditional text_classification models have some drawbacks , such as the inability of the model to focus on important parts of the text contextual_information in text processing . to solve this problem , we fuse the long and short-term_memory network bigru with a convolutional_neural_network to receive text sequence input to reduce the dimensionality of the input sequence and to reduce the loss of text features based on the length and context dependency of the input text sequence . considering the extraction of important features of the text , we choose the long and short-term_memory network bilstm to capture the main features of the text and thus reduce the loss of features . finally , we propose a bigru-cnn-bilstm model ( dcrc model ) based on cnn , gru and lstm , which is trained and validated on the thucnews and toutiao news datasets . the model outperformed the traditional model in terms of accuracy , recall and f1 score after experimental comparison .
title : stratifying risk of coronary_artery_disease using discriminative knowledge-guided medical concept pairings from clinical_notes ; abstract : document_classification ( dc ) is one of the broadly investigated natural_language_processing tasks . medical document_classification can support doctors in making decision and improve medical services . since the data in document_classification often appear in raw form such as medical discharge_notes , extracting meaningful_information to use as features is a challenging task . there are many specialized words and expressions in medical documents which make them more challenging to analyze . the classification_accuracy of available methods in medical field is not good enough . this work aims to improve the quality of the input feature_sets to increase the accuracy . a new three-stage approach is proposed . in the first stage , the unified medical language system ( umls ) which is a medical-specific dictionary is used to extract the meaningful phrases by considering disease or symptom concepts . in the second stage , all the possible pairs of the extracted concepts are created as new features . in the third stage , particle swarm optimisation ( pso ) is employed to select features from the extracted and constructed features in the previous stages . the experimental results show that the proposed three-stage method achieved substantial_improvement over the existing medical dc approaches .
title : exploiting the matching information in the support set for few shot event classification ; abstract : the existing event classification ( ec ) work primarily focuseson the traditional supervised_learning setting in which models are unableto extract event mentions of new/unseen event_types . few-shot learninghas not been investigated in this area although it enables ec models toextend their operation to unobserved event_types . to fill in this gap , inthis work , we investigate event classification under the few-shot learningsetting . we propose a novel training method for this problem that exten-sively exploit the support set during the training process of a few-shotlearning model . in particular , in addition to matching the query exam-ple with those in the support set for training , we seek to further matchthe examples within the support set themselves . this method providesmore training signals for the models and can be applied to every metric-learning-based few-shot learning methods . our extensive_experiments ontwo benchmark ec datasets show that the proposed method can improvethe best reported few-shot learning models by up to 10 % on accuracyfor event classification
title : discovering context : classifying tweets through a semantic transform based on wikipedia ; abstract : by mapping messages into a large context , we can compute the distances between them , and then classify them . we test this conjecture on twitter messages : messages are mapped onto their most similar wikipedia pages , and the distances between pages are used as a proxy for the distances between messages . this technique yields more accurate classification of a set of twitter messages than alternative techniques using string edit_distance and latent_semantic_analysis . ©_2011_springer-verlag .
title : solution for the epo codefest on green plastics : hierarchical multi-label_classification of patents relating to green plastics using deep_learning ; abstract : this work aims at hierarchical multi-label patents classification for patents disclosing technologies related to green plastics . this is an emerging_field for which there is currently no classification_scheme , and hence , no labeled_data is available , making this task particularly challenging . we first propose a classification_scheme for this technology and a way to learn a machine_learning model to classify patents into the proposed classification_scheme . to achieve this , we come up with a strategy to automatically assign labels to patents in order to create a labeled training dataset that can be used to learn a classification model in a supervised_learning setting . using said training dataset , we come up with two classification models , a scibert neural_network ( sbnn ) model and a scibert hierarchical neural_network ( sbhnn ) model . both models use a bert model as a feature_extractor and on top of it , a neural_network as a classifier . we carry out extensive_experiments and report commonly evaluation_metrics for this challenging classification problem . the experiment results verify the validity of our approach and show that our model sets a very strong benchmark for this problem . we also interpret our models by visualizing the word importance given by the trained model , which indicates the model is capable to extract high-level semantic information of input documents . finally , we highlight how our solution fulfills the evaluation_criteria for the epo codefest and we also outline possible directions for future work . our code has been made available at https : //github.com/epo/cf22-green-hands
title : a new cross-domain strategy based xai models for fake_news_detection ; abstract : in this study , we presented a four-level cross-domain strategy for fake_news_detection on pre-trained models . cross-domain text_classification is a task of a model adopting a target domain by using the knowledge of the source_domain . explainability is crucial in understanding the behaviour of these complex models . a fine-tune bert model is used to . perform cross-domain classification with several experiments using datasets from different domains . explanatory models like anchor , eli5 , lime and shap are used to design a novel explainable approach to cross-domain levels . the experimental analysis has given an ideal pair of xai models on different levels of cross-domain .
title : a solution of the multiaspect text_categorization problem by a hybrid hmm and lda based technique ; abstract : in our previous work we introduced a novel concept of the multiaspect text_categorization ( mtc ) task meant as a special , extended form of the text_categorization ( tc ) problem which is widely_studied in information_retrieval . the essence of the mtc problem is the classification of documents on two levels : first , on a more or less standard level of thematic categories and then on the level of document sequences which is much less studied in the literature . the latter stage of classification , which is by far more challenging , is the main focus of this paper . a promising way of attacking it requires some kind of modeling of connections between documents forming sequences . to solve this problem we propose a novel approach that combines a well-known techniques to model sequences , i.e. , the hidden_markov_models ( hmm ) and the latent_dirichlet_allocation ( lda ) technique for the advanced document_representation , hence obtaining a hybrid approach . we present details of our proposed approach as well as results of some computational experiments .
title : self-supervised short-text modeling through auxiliary context generation ; abstract : short text is ambiguous and often relies predominantly on the domain and context at hand in order to attain semantic relevance . existing classification models perform_poorly on short text due to data sparsity and inadequate context . auxiliary context , which can often provide sufficient background regarding the domain , is typically available in several application scenarios . while some of the existing_works aim to leverage real-world knowledge to enhance short-text representations , they fail to place appropriate emphasis on the auxiliary context . such models do not harness the full potential of the available context in auxiliary sources . to address this challenge , we reformulate short-text_classification as a dual_channel self-supervised_learning problem ( that leverages auxiliary context ) with a generation network and a corresponding prediction model . we propose a self-supervised framework , pseudo-auxiliary context generation network for short-text modeling ( pacs ) , to comprehensively leverage auxiliary context and it is jointly learned with a prediction network in an end-to-end manner . our pacs model consists of two sub-networks : a context generation network ( cgn ) that models the auxiliary context 's distribution and a prediction network ( pn ) to map the short-text features and auxiliary context distribution to the final class_label . our experimental results on diverse datasets demonstrate that pacs outperforms formidable state-of-the-art baselines . we also demonstrate the performance of our model on cold-start scenarios ( where contextual_information is non-existent ) during prediction . furthermore , we perform interpretability and ablation studies to analyze various representational features captured by our model and the individual contribution of its modules to the overall performance of pacs , respectively .
title : cimi : classify and itemize medical image system for pft big_data based on deep_learning ; abstract : the value of pulmonary function test ( pft ) data is increasing due to the advent of the coronavirus infectious_disease 19 ( covid‐19 ) and increased respiratory_disease . however , these pft data can not be directly used in clinical studies , because pft results are stored in raw image files . in this study , the classification and itemization medical image ( cimi ) system generates valuable data from raw pft images by automatically classifying various pft results , extracting texts , and storing them in the pft database and excel files . the deep‐learning‐based optical_character_recognition ( ocr ) technology was mainly used in cimi to classify and itemize pft images in st. mary ’ s hospital . cimi classified seven types and itemized 913,059 texts from 14,720 pft image sheets , which can not be done by humans . the number , type , and location of texts that can be extracted by pft type are all different , but cimi solves this issue by classifying the pft image sheets by type , allowing researchers to analyze the data . to demonstrate the superiority of cimi , the validation results of cimi were compared to the results of the other four algorithms . a total of 70 randomly_selected sheets ( ten sheets from each type ) and 33,550 texts were used for the validation . the accuracy of cimi was 95 % , which was the highest_accuracy among the other four algorithms .
title : impact of word_segmentation errors on automatic chinese text_classification ; abstract : in this paper , several sets of experiments were carried out to study the impact of word_segmentation errors on automatic chinese text_classification . comparison experiment of four word-based approaches was first carried out and the results show that the performance was significantly_reduced when using automatic word_segmentation instead of manual word_segmentation which means errors caused by automatic word_segmentation have an obvious impact on classification performance . we further conducted the experiment using character-based approach ( n-gram ) . although n-gram approach produces a large number of ambiguous_words , the results show that it performed better than automatic word_segmentation . © 2012 ieee .
title : a comparative research of different granularities in korean text_classification ; abstract : text_classification is a process , which can make the specified documents group into several categories , predefined at the beginning through learning a series of rules or under the guidance of the goal function . this paper compared the subword-level , spacing-level of korean and the word-level , then analyzed the influence of the preprocessing of different granularities on the text_classification task of korean . after that , analyzed the results of classification linguistically . thus we can choose the proper granularity as the input to improve the classification effect . firstly , cut the corpus according to different granularities ; then , used glove_word_embedding . finally , used self-attention classification mechanism to verify the effect of the corpus which are preprocessed through different granularities . and using the accuracy and loss as the index . after experimental and comparison , the best result is selected when the accuracy is 83.46 % through the spacing-level . experiments show that the research in this paper can improve the effect of text_classification .
title : study on the subjective and objective text_classification and pretreatment of chinese network text ; abstract : subjective and objective text_classification is widely used in product_reviews , video reviews , social public_opinion analysis and micro-blogging attitude analysis . to solve the existing problem of network text formalization in subjective and objective text_classification , a machine_learning classification method based on network informal_language ( nil ) is proposed . firstly , a network informal dictionary is constructed by writing a web_crawler to collect informal words which can be divided into two categories : typical type and fuzzy type . then , different methods are put forward to formalize the informal network text based on the two types of informal words . finally , we adopt the native bayes_classifier and sequential_minimal_optimization classifier to distinguish subjectivity and objectivity of the text . the experimental results reveal that the method we proposed can improve the accuracy of subjective and objective text_classification . © 2012 ieee .
title : constructing contrastive samples via summarization for text_classification with limited annotations ; abstract : contrastive_learning has emerged as a powerful representation_learning method and facilitates various downstream_tasks especially when supervised data is limited . how to construct efficient contrastive samples through data augmentation is key to its success . unlike vision tasks , the data augmentation method for contrastive_learning has not been investigated sufficiently in language tasks . in this paper , we propose a novel approach to construct contrastive samples for language tasks using text_summarization . we use these samples for supervised_contrastive_learning to gain better text_representations which greatly benefit text_classification tasks with limited annotations . to further improve the method , we mix up samples from different classes and add an extra regularization , named mixsum , in addition to the cross-entropy-loss . experiments on real-world text_classification datasets ( amazon-5 , yelp-5 , ag_news , and imdb ) demonstrate the effectiveness of the proposed contrastive_learning framework with summarization-based data augmentation and mixsum regularization .
title : improving the accuracy of text_classification using the over sampling technique in the case of sinovac vaccine ; abstract : the who has declared covid-19 ( coronavirus disease 2019 ) a global_health emergency . up to 19 november 2021 , the total positive cases in indonesia reached 4,252,705 , of which 4,100,837 recovered , and 143,714 died . therefore , vaccines have been developed to minimize covid-19 transmission . there are some kinds of vaccines developed by several companies such as sinovac , astrazeneca , pfizer , and moderna . the general public has a different opinion on sinovac vaccine on twitter , where some people promote it while others reject it . data used in this study were 1000 tweets about the sinovac vaccine . during the dataset collection unequal distribution often occurs , where the number of labels is more on one side . such a situation is called imbalance class . imbalance class in a dataset can reduce classification performance . to overcome the imbalance class , this study used the synthetic_minority_over-sampling_technique ( smote ) . the classification methods used were k-nearest_neighbors ( knn ) , support_vector_machine ( svm ) , and random_forest , and tf-idf was used to determine the weight of the words . the average rise of the accuracy value of the three algorithms after smote optimization was 14 % . the results of sentiment_analysis for the sinovac vaccine revealed a positive_sentiment of 81 % . thus , it can be concluded that the sinovac vaccine received a positive response from the public .
title : scalable sentiment_classification across multiple dark_web forums ; abstract : this study examines several approaches to sentiment_classification in the dark_web forum portal , and opportunities to transfer classifiers and text features across multiple forums to improve scalability and performance . although sentiment classifiers typically perform_poorly when transferred across domains , experimentation reveals the devised approaches offer performance equivalent to the traditional forum-specific approach in classification in an unknown domain . furthermore , incorporating the text features identified as significant indicators of sentiment in other forums can greatly_improve the classification_accuracy of the traditional forum-specific approach . © 2012 ieee .
title : baseline english and maltese-english classification models for subjectivity detection , sentiment_analysis , emotion analysis , sarcasm detection , and irony detection ; abstract : this paper presents baseline classification models for subjectivity detection , sentiment_analysis , emotion analysis , sarcasm detection , and irony detection . all models are trained on user-generated content gathered from newswires and social_networking services , in three different languages : english -a high-resourced language , maltese -a low-resourced language , and maltese-english -a code-switched language . traditional supervised algorithms namely , support_vector_machines , naïve_bayes , logistic_regression , decision_trees , and random_forest , are used to build a baseline for each classification_task , namely subjectivity , sentiment_polarity , emotion , sarcasm , and irony . baseline_models are established at a monolingual ( english ) level and at a code-switched level ( maltese-english ) . results obtained from all the classification models are presented .
title : short_text_classification combining keywords and knowledge ; abstract : with the continuous development of the internet , short text data are increasing . however , short text has poor classification performance due to sparse features . nowadays , integrating external_knowledge into the classification model has become one of the best solutions to the sparse problem of short text data , but this method is still insufficient . when the mapping result between knowledge and text is not enough , we choose keywords as content feature extensions to compensate for the lack of external_knowledge . for the problem of noise generated by external_knowledge . we design a context-based attention_mechanism , which combines semantic information to make the model more focused on knowledge that has a more positive impact on classification . we conducted experiments on common data sets to verify the effectiveness of the model .
title : a cfs-based feature_weighting approach to naive_bayes text classifiers ; abstract : recent work in supervised_learning has shown that naive_bayes text classifiers with strong assumptions of independence among features , such as multinomial_naive_bayes ( mnb ) , complement naive_bayes ( cnb ) and the one-versus-all-but-one model ( ova ) , have achieved remarkable classification performance . this fact raises the question of whether a naive_bayes text classifier with less restrictive assumptions can perform even better . responding to this question , we firstly evaluate the correlation-based feature_selection ( cfs ) approach in this paper and find that it performs even worse than the original versions . then , we propose a cfs-based feature_weighting approach to these naive_bayes text classifiers . we call our feature weighted versions fwmnb , fwcnb and fwova respectively . our proposed approach weakens the strong assumptions of independence among features by weighting the correlated features . the experimental results on a large suite of benchmark_datasets show that our feature weighted versions significantly_outperform the original versions in terms of classification_accuracy . ©_2014_springer_international_publishing_switzerland .
title : a hybrid documents classification based on svm and rough_sets ; abstract : standard machine_learning techniques like support_vector_machines ( svm ) and related large_margin methods have been successfully applied for text_classification . unfortunately , the high dimensionality of input feature_vectors impacts on the classification speed . the kernel parameters setting for svm in a training process impacts on the classification_accuracy . feature_selection is another factor that impacts classification_accuracy . the objective of this work is to reduce the dimension of feature_vectors , optimizing the parameters to improve the svm classification_accuracy and speed . in order to improve classification speed we spent rough_sets theory to reduce the feature_vector space . we present a genetic_algorithm approach for feature_selection and parameters optimization to improve classification_accuracy . experimental results indicate our method is more effective than traditional svm methods and other traditional_methods . © 2009 ieee .
title : integrating noun-based feature_ranking and selection methods with arabic text associative_classification approach ; abstract : feature_ranking and selection ( fr & s ) is an important preprocessing_phase for text_classification , and it is in most cases produces small valuable sub-feature_space among the whole feature_space and reduces the classification errors . as the associative_classification ( ac ) approach is an efficient method and its training and testing depend on the way that features ranked and selected , the examining of feature_ranking methods is very significant . this paper presents an integration method of arabic noun extraction with four fr & s methods : term_frequency–inverse_document_frequency ( tf-idf ) , document_frequency , odd ratio , and class discriminating measure ( cdm ) . association_rule technology uses the result of the integrated feature_selection to construct an arabic text associative classifier . in this study , the majority_voting and ordered decision list prediction methods are used by ac to assign test document to its category . a set of experiments are conducted on collection of arabic text documents , and the experimental results show that our ac method works better with extracted nouns and feature_selection method than with feature_selection method individually . the ac based on cdm and tf-idf methods outperforms the other methods in terms of ac accuracy . as the results indicate , the proposed method produces satisfactory classification_accuracy and it has good selecting effect on the arabic text associative classifier .
title : research and implementation of chinese microblog_sentiment_classification ; abstract : this paper studies sentiment_analysis in weibo . the study focuses on three types of tasks : emotion sentence identification and classification , emotion tendency classification , and emotion expression extraction . an unsupervised topic sentiment model , utsm , is proposed based on the lda collocation model to facilitate automatic hashtag labeling . a gibbs_sampling implementation is presented for deriving an algorithm that can be used to automatically categorize emotion tendency with computer . to address the issue of lower recall_ratio for emotion expression extraction in weibo , dependency_parsing is used to divide dependency model into two categories with subject and object . six dependency models are also constructed from evaluation objects and emotion words , and a merging algorithm is proposed to accurately_extract emotion expression . result of experiments indicates that the presented method has a strong innovative and practical value .
title : an optimised support_vector_machine with ringed_seal search_algorithm for efficient text_classification ; abstract : nowadays , with the increasing availability of online text documents , it becomes an important task for an organization to automatically_classify the document . in text_classification ( tc ) , support_vector_machine is the commonly used machine-learning algorithm . performance of svm highly depends on parameter_tuning using metaheuristic algorithm for text_classification . to integrate dynamic searching to parameter setting for svm is a big issue that produced great influence in the classification_accuracy . in order to improve the generalization and learning capability of svm , this paper presents a new approach known as rss-svm , which is used to optimize kernel_function and penalty parameters through the ringed_seal search_algorithm . experiments are conducted on three text datasets named : reuter21578 , 20_newsgroup and tdt2 with a different number of classes , which shows that proposed rss-svm present significant results having 79.22 % accuracy , 70.79 % recall , 58 % precision and 54.71 % f-measure among the previous ga-svm and cs-svm algorithms .
title : identification of covid-19 related fake_news via neural stacking ; abstract : identification of fake_news plays a prominent role in the ongoing pandemic , impacting multiple_aspects of day-to-day life . in this work we present a solution to the shared_task titled covid19 fake_news_detection in english , scoring the 50th place amongst 168 submissions . the solution was within 1.5 % of the best performing solution . the proposed solution employs a heterogeneous representation ensemble , adapted for the classification_task via an additional neural classification head comprised of multiple hidden_layers . the paper consists of detailed ablation studies further displaying the proposed method 's behavior and possible implications . the solution is freely available . \url_{_https : //gitlab.com/boshko.koloski/covid19-fake-news }
title : physical movement monitoring using body sensor networks : a phonological approach to construct spatial decision_trees ; abstract : monitoring human activities using wearable sensor nodes has the potential to enable many useful applications for everyday situations . limited computation , battery lifetime and communication bandwidth make efficient use of these platforms crucial . in this paper , we introduce a novel classification model that identifies physical movements from body-worn inertial sensors while taking collaborative nature and limited_resources of the system into consideration . our action recognition model uses a decision_tree structure to minimize the number of nodes involved in classification of each action . the decision_tree is constructed based on the quality of action recognition in individual nodes . a clustering technique is employed to group similar actions and measure quality of per-node identifications . we pose an optimization_problem for finding a minimal set of sensor nodes contributing to the action recognition . we then prove that this problem is np-hard and provide fast greedy algorithms to approximate the solution . finally , we demonstrate the effectiveness of our distributed algorithm on data collected from five healthy_subjects . in particular , our system achieves a 72.4 % reduction in the number of active nodes while maintaining 93.3 % classification_accuracy . © 2006 ieee .
title : a systematic approach to design of a text categorizer ; abstract : in this paper , we implement a systematic approach to text_categorization using latent_semantic_indexing ( lsi ) . a novel feature of our approach is that we iteratively refine the lsi space used for categorization . using a verification set , we also employ lsi to determine the values of all parameters controlling the steps of the categorization process . our approach is designed to scale to enterprise-level implementations . we test the categorizer using the standard reuters 21578 test set . in order to accurately compare our results with other prior work , we carried out a review of over 500 previous reports of document_categorization using the reuters 21578 collection . at least within the scope of that review , the categorization performance reported here is the best yet attained for the single-label case on this standard test set .
title : prediction of election result by enhanced sentiment_analysis on twitter data using word_sense_disambiguation ; abstract : sentiment_analysis is the computational study of opinions , sentiments , evaluations , attitudes , views and emotions expressed in text . it refers to a classification problem where the main focus is to predict the polarity of words and then classify them into positive or negative_sentiment . sentiment_analysis over twitter offers people a fast and effective way to measure the public 's feelings towards their party and politicians . the primary issues in previous sentiment_analysis techniques are classification_accuracy , as they incorrectly classify most of the tweets with the biasing towards the training_data . in opinion texts , lexical content alone also can be misleading . therefore , here we adopt a lexicon based sentiment_analysis method , which will exploit the sense definitions , as semantic indicators of sentiment . here we propose a novel approach for accurate sentiment_classification of twitter messages using lexical_resources sentiwordnet and wordnet along with word_sense_disambiguation . thus we applied the sentiwordnet lexical_resource and word_sense_disambiguation for finding political sentiment from real time tweets . our method also uses a negation handling as a pre-processing step in order to achieve high accuracy .
title : performance evaluation of different similarity_functions and classification methods using web_based hindi_language question_answering system ; abstract : question_answering ( qa ) system is an approach to extract the correct_answer for the query asked by the user in its own language . the work discussed is implemented for hindi_language objective type questions and answers . the paper implements the comparison of nine different similarity_functions and two classification methods used to retrieve the desired information . the results revealconclude that smith waterman outperforms the other similarity_functions in perforamnce evaluation . the k-nearest_neighbor ( k-nn ) algorithm gives 97 % , 95.6 % and neasret neighbor ( nn ) algorithm gives 93.3 % ,95 % for two differtent test data sets , respectively .
title : novel ogbee-based feature_selection and feature-level_fusion with mlp neural_network for social_media multimodal_sentiment_analysis ; abstract : numerous public networks , namely instagram , youtube , facebook , twitter , etc. , share their own feelings and idea as videotapes , posts , and pictures . in future_research , adapting to such data and mining valuable_information from it will be an undeniably troublesome errand . this paper proposes a novel audio–video–textual-based multimodal_sentiment_analysis approach . the proposed approach investigates the sentiments that are collected from the web recordings that utilize audio , video , and textual modalities for further extraction . a feature-level_fusion technique is employed in fusing the extracted features from different modalities . therefore , the extracted features are optimally chosen by using a novel oppositional grass bee optimization ( ogbee ) algorithm to obtain the best optimal feature_set . here , 12 benchmark functions are developed to validate the numerical efficiency and the effectiveness of a novel ogbee algorithm for various aspects . moreover , our proposed approach utilizes multilayer_perceptron-based neural_network ( mlp-nn ) for sentiment_classification . the experimental analysis_reveals that the proposed approach provides better classification_accuracy of about 95.2 % with less computational time .
title : efficient extraction of technical requirements applying data augmentation ; abstract : requirements for complex technical systems are documented in natural_language sources . manually extracting requirements from these documents-e.g. , to transfer them to a requirements_management tool-is time-consuming and error-prone . today , machine_learning approaches are used to classify natural_language requirements and thus enable extraction of these requirements . however , in practice there is often not enough labeled domain-specific data available to train such models . for this reason , this work investigates the performance in artificially generating requirements through data augmentation . first , success criteria for a method for extracting and augmenting requirements are elicited in cooperation with industry experts . second , the performance in the augmentation of requirements data is investigated . the results show that gpt-j is suitable for generating artificial requirements : weighted_average f1-score : 62.74 % . third , a method is developed to extract requirements from specifications , augment requirements data , and then classify the requirements . as a final_step , the method is evaluated with requirements data from three industry case examples of the engineering service_provider edag engineering gmbh : assembly latch hood , adjustable stopper hood and trunk curtain roller blind . evaluation shows that especially the transferability of models is improved when they are trained with augmented_data . the developed method facilitates eliciting complete requirements sets . performance of artificial_intelligence models in requirements extraction is improved applying augmented_data and therefore the method leads to efficient product_development .
title : n-gram based approach for automatic prediction of essay rubric marks ; abstract : automatic essay scoring , applied to the prediction of grades for dimensions of a scoring rubric , can provide automatic detailed feedback on students ’ written assignments . we apply a character and word n-gram based technique proposed originally for authorship_identification—common n-gram ( cng ) classifier—to this task . we report promising_results for the rubric mark prediction for essays by cng , and perform analysis of suitability of different types of n-grams for the task .
title : a novel ontology-based method to represent and classify failure_modes of sensors ; abstract : it is generally recognized that sensors play important_roles in modern control systems and failure_modes of sensors are key_points for safety and reliability analysis of such systems . failure_modes rely only on specific components rather than the system architecture , and therefore could be reused in different analysis . in this paper , we present a novel ontology-based method to represent and classify failure_modes of sensors , so that they could be reused effectively and sufficiently . the principle and essence of the method are described in details appending a demonstration illustrating how to construct a concrete ontology with the method . furthermore , the retrieve method based on semantic similarity is introduced . additionally , a free_software tool is developed for the method . the method and the tool could both be used in other fields with similar scenarios . © 2013 ifsa .
title : fusing location and text features for sentiment_classification ; abstract : geo-tagged twitter data has been used recently to infer insights on the human aspects of social_media . insights related to demographics , spatial distribution of cultural activities , space-time travel trajectories for humans as well as happiness has been mined from geo-tagged twitter data in recent_studies . to date , not much study has been done on the impact of the geo-location features of a tweet on its sentiment . this observation has inspired us to propose the usage of geo-location features as a method to perform sentiment_classification . in this method , the sentiment_classification of geo-tagged tweets is performed by concatenating geo-location features and one-hot encoded word_vectors as inputs for convolutional_neural_networks ( cnn ) and long short-term_memory ( lstm ) networks . the addition of language-independent features in the form of geo-location features has helped to enrich the tweet representation in order to combat the sparse nature of short tweet message . the results achieved has demonstrated that concatenating geo-location features to one-hot encoded word_vectors can achieve higher_accuracy as compared to the usage of word_vectors alone for the purpose of sentiment_classification .
title : a semantic case-based_reasoning framework for text_categorization ; abstract : this paper presents a semantic case-based_reasoning framework for text_categorization . text_categorization is the task of classifying text documents under predefined_categories . accidentology is our application field and the goal of our framework is to classify documents describing real road accidents under predefined road accident prototypes , which also are described by text documents . accidents are described by accident_reports while accident prototypes are described by accident scenarios . thus , text_categorization is done by assigning each accident report to an accident scenario , which highlights particular mechanisms leading to accident . we propose a textual case-based_reasoning approach ( tcbr ) , which allows us to integrate both textual and domain_knowledge aspects in order to carry out this categorization . cbr solves a new problem ( target case ) by identifying its similarity to one or several previously solved problems ( source cases ) stored in a case base and by adapting their known solutions . cases of our framework are created from text . most of tcbr applications create cases from text by using information_retrieval techniques , which leads to knowledge-poor descriptions of cases . we show that using semantic resources ( two ontologies of accidentology ) makes possible to overcome this difficulty , and allows us to enrich cases by using formal knowledge . in this paper , we argue that semantic resources are likely to improve the quality of cases created from text , and , therefore , such resources can support the reasoning cycle . we illustrate this claim with our framework developed to classify documents in the accidentology domain . ©_2008_springer-verlag_berlin_heidelberg .
title : sentence_similarity techniques for short vs variable length text using word_embeddings ; abstract : in goal-oriented conversational_agents like chatbots , finding the similarity between user input and representative text result is a big challenge . generally , the conversational_agent developers tend to provide a minimal number of utterances per intent , which makes the classification_task difficult . the problem becomes more complex when the length of the representative text per action is short and the length of the user input is long . we propose a methodology that derives sentence_similarity score based on n-gram and sliding_window and uses the fasttext word_embeddings technique which outperforms the current state-of-the-art sentence_similarity results . we are also publishing a dataset on the shopping domain , to build conversational_agents . and the extensive_experiments done on the dataset fetched better results in accuracy , precision and recall by 6 % , 2 % and 80 % respectively . it also evinces that our solution generalizes well on the low corpus and requires no training .
title : multi-level fuzzy based renyi entropy for linguistic classification of texts in natural_scene_images ; abstract : this paper focuses on linguistic classification of scene_texts in natural_scene_images . in this paper , an attempt is made to localize texts based on multi-level thresholding by fuzzy-based renyi entropy . complex natural_scene_images with diversified challenges are considered . a set of heuristic rules comprising geometric filters and stroke width transform govern the process of locating potential text_regions . the scene_images may contain more than one language , where text recognition by optical_character_recognition system becomes challenging . manual_intervention is needed to specify the language of each text . to overcome this hurdle , linguistic classification of text_regions is suggested in this paper . the proposed method is validated using publicly available dataset—msra-td500 . results show that fuzzy-based renyi entropy thresholding is able to segment the foreground text from complex natural_scene_images . geometric filters could capture the inherent uniformity of the text . stroke width transform eliminates the non-text_regions . the performance_measures such as precision , recall and f-measures are 78 % , 77 % and 76 % , respectively . this shows the ability of the algorithm to extract the text from the scenes . the geometric feature such as area and corner shows better variation in discriminating the linguistic texts . further , the first three hu moment features also contribute remarkable role in analyzing the shape of extracted text_regions . the classifier based on support_vector_machine ( svm ) yields classification_accuracy of 85.45 % in discriminating english and chinese alphabets . area under the roc curve ( auc ) is 0.851 for svm classifier . the proposed methodology has proved its robustness against common degradations , such as uneven illumination , varying font characteristics and blurring effects . experimental results show that our method_achieves better performance in linguistic classification .
title : chinese sentiment_analysis using regularized extreme_learning_machine and stochastic optimization ; abstract : in chinese sentiment_analysis , accuracy and execution speed are a comprehensive performance evaluation of an algorithm . this paper proposes an efficient method for sentiment_analysis . firstly , the classification data_set is directly used to construct the corpus and word2vec for conversion of processed corpus . secondly , regularized extreme_learning_machine ( rlm ) for sentiment_analysis . considering that the single objective only considers one objective ( precision ) , the particle swarm algorithm is used to solve a multi-objective function for the non-dominated pareto front optimal value . experiments were performed on the chnsenticorp , and compared with other mainstream models , better classification results and faster execution speed were achieved .
title : indonesian conjunction rule_based sentiment_analysis for service complaint regional water utility company surabaya ; abstract : pdam ( regional drinking_water company ) is a company that provides clean_water . pdam develops their services with by considering complaints , suggestions and complaints from users . over time pdam services users are increasing thus allowing the number of complaints to also increase and pdam is impossible to analyze the complaint data using the manual data . in this research proposes ideas to analyze pdam complaints data with rule_based sentiment_analysis and categorization methods . the rule_based sentiment_analysis in this research used twelve rules , where the uniqueness of this rule_based is a detection conjunction . indonesian conjunction detection is the first method available in indonesia . detection of conjunction is proposed to find out whether conjunction has an important influence in the meaning of a sentence . the result of sentiment_analysis is a score from complaint sentence are negative , positive or neutral . and categorization is a method to provide sentence score including complaints on turbid , leaky water , leakage , meters , usage , or not getting water . an experiment sentiment_analysis was conducted on 392 data containing conjunctions and have score manually sentiment . the accuracy value obtained used rule_based with conjunctions detection increases 13 % than rule_based do not use conjunction detection . and the accuracy value of categorization on 100 complaint data are 84 % true and 16 % false . so for high accuracy values in conjunction detection needs to notice the context of the sentence and word_dictionary and in categorization must also notice to words and write priority categories in the program .
title : estimating the generalization performance of polynomial svm classifier for text_categorization ; abstract : vc theory and structural risk minimization principle are key_concepts of statistical learning theory . developed from this theory , svm is widely investigated and used for text_categorization because of its high generalization performance . previous work showed that polynomial svm 's performance was irrevelant of the order and it was appropriate for high dimensional text_categorization problems without feature_selection . the research indicates over-fitting problems occur as the polynomial order increases . svm 's generalization performance decreases drastically if too many features are used , so feature_selection is necessary . based on the structural risk minimization principle , this fact is analyzed via estimating functional classes 's vc dimension . and the empirical results support the theoretical conclusions .
title : narrative text_classification for automatic key_phrase_extraction in web document corpora ; abstract : automatic key_phrase_extraction is a useful tool in many text related applications such as clustering and summarization . state-of-the-art methods are aimed towards extracting key_phrases from traditional text such as technical papers . application of these methods on web_documents , which often contain diverse and heterogeneous contents , is of particular interest and challenge in the information_age . in this work , we investigate the significance of narrative text_classification in the task of automatic key_phrase_extraction in web document corpora . we benchmark three methods , tfidf , kea , and keyterm , used to extract key_phrases from all the plain_text and from only the narrative text of web_pages . anova tests are used to analyze the ranking data collected in a user study using quantitative measures of acceptable percentage and quality value . the evaluation shows that key_phrases extracted from the narrative text only are significantly better than those obtained from all plain_text of web_pages . this demonstrates that narrative text_classification is indispensable for effective key_phrase_extraction in web document corpora . copyright 2005 acm .
title : staresgru-cnn with cmedlms : a stacked residual gru-cnn with pre-trained biomedical language models for predictive intelligence ; abstract : as a task requiring strong professional experience as supports , predictive biomedical intelligence can not be separated from the support of a large amount of external domain_knowledge . by using transfer_learning to obtain sufficient prior experience from massive biomedical text data , it is essential to promote the performance of specific downstream predictive and decision-making task models . this is an efficient and convenient method , but it has not been fully developed for chinese natural_language_processing ( nlp ) in the biomedical_field . this study proposes a stacked residual gated_recurrent_unit-convolutional_neural_networks ( staresgru-cnn ) combined with the pre-trained_language_models ( plms ) for biomedical text-based predictive tasks . exploring related paradigms in biomedical nlp based on transfer_learning of external expert_knowledge and comparing some chinese and english_language models . we have identified some key_issues that have not been developed or those present difficulties of application in the field of chinese biomedicine . therefore , we also propose a series of chinese biomedical language models ( cmedlms ) with detailed evaluations of downstream_tasks . by using transfer_learning , language models are introduced with prior_knowledge to improve the performance of downstream_tasks and solve specific predictive nlp tasks related to the chinese biomedical_field to serve the predictive medical system better . additionally , a free-form text electronic_medical_record ( emr ) -based disease diagnosis prediction task is proposed , which is used in the evaluation of the analyzed language models together with clinical named_entity_recognition , biomedical text_classification tasks . our experiments_prove that the introduction of biomedical knowledge in the analyzed models significantly_improves their performance in the predictive biomedical nlp tasks with different granularity . and our proposed model also achieved competitive_performance in these predictive intelligence tasks .
title : imbalanced sentiment_classification enhanced with discourse marker ; abstract : imbalanced_data commonly exists in real_world , espacially in sentiment-related corpus , making it difficult to train a classifier to distinguish latent sentiment in text data . we observe that humans often express transitional emotion between two adjacent discourses with discourse_markers like `` but '' , `` though '' , `` while '' , etc , and the head discourse and the tail discourse 3 usually indicate opposite emotional_tendencies . based on this observation , we propose a novel plug-and-play method , which first samples discourses according to transitional discourse_markers and then validates sentimental polarities with the help of a pretrained attention-based model . our method increases sample diversity in the first place , can serve as a upstream preprocessing part in data augmentation . we conduct_experiments on three public sentiment datasets , with several frequently used algorithms . results show that our method is found to be consistently effective , even in highly_imbalanced scenario , and easily be integrated with oversampling method to boost the performance on imbalanced sentiment_classification .
title : a case-based_reasoning decision-making model for hesitant_fuzzy linguistic information ; abstract : in some complicated decision-making problems , because of time pressure or the lack of necessary information , decision_makers ( dms ) infrequently select optimal alternatives , but acquire satisfactory alternatives that can be obtained by analyzing the correlation between the decision problems and past similar cases . case-based_reasoning ( cbr ) is an effective approach to obtain preferential information for dms from past successful decision cases . using the cbr approach , we aim to process hesitant_fuzzy linguistic information , and classify and rank the alternatives according to past successful decision cases . we first sum the distance_measures for hesitant_fuzzy linguistic term_sets ( hfltss ) and then propose a new axiomatic definition for hfltss , which are compared with existing distance_measures from relationships and properties . furthermore , based on our proposed distance_measure , we propose a cbr decision model for hesitant_fuzzy linguistic information to calculate the weights of criteria and classifying thresholds . we then classify and rank the alternatives according to the most satisfactory solution in past successful decision cases . finally , we consider an example to demonstrate the effectiveness and advantages of our proposed method .
title : malayalam word_sense_disambiguation using naïve_bayes classifier ; abstract : word_sense_disambiguation is the process of identifying accurate meaning of a polysemous_words given in a context . the paper proposes a supervised malayalam word_sense_disambiguation system using naive_bayes_classifier . word_sense_disambiguation is a complex problem in nlp because a particular word may have different meanings in different situations . for all human beings it is very easy to find out the accurate sense in a particular context but for machines it is very difficult to predict . some extent of intelligence may add to the machine for an accurate prediction . here this proposed system provide us 95 % reliability using a corpora of 1 lakh words .
title : an integrated fuzzy neural_network with topic-aware auto-encoding for sentiment_analysis ; abstract : recent advanced deep_learning architectures , such as neural seq2seq and transformer , have demonstrated remarkable improvements in multi-typed sentiment_classification tasks . even though recent transformer-based and seq2seq-based models have successfully enabled to capture rich contextual_information of texts , they still lacked attention on incorporating global semantic information which enables to sufficiently leverage the performance of downstream sa tasks . moreover , emotional_expressions of users are normally in the form of natural human-written textual_data which contains a lot of noises and ambiguities that impose great_challenges on the processes of textual_representation learning as well as sentiment_polarity prediction . to meet these challenges , we propose a novel integrated fuzzy neural_architecture with a topic-driven textual_representation learning approach for handling the sa task , called as : topfuzz4sa . specifically , in the proposed topfuzz4sa model , we first apply a topic-driven neural encoder–decoder architecture with the incorporation of latent_topic embedding and attention_mechanism to sufficiently learn both rich contextual and global semantic information of the given textual_data . then , the achieved rich_semantic representations of texts are fed into a fused deep fuzzy neural_network to effectively_reduce the feature ambiguity and noise , forming the final textual_representations for sentiment_classification task . extensive_experiments in benchmark_datasets demonstrate the effectiveness of our proposed topfuzz4sa model compared with contemporary state-of-the-art baselines .
title : multi-language video subtitle recognition with convolutional_neural_network and long short-term_memory networks ; abstract : nowadays , many videos are published on internet channels such as youtube and facebook . many audiences , however , can not understand the contents of the video , maybe due to the different languages and even hearing_impairment . as a result , subtitles have been added to videos . in this paper , we proposed deep_learning techniques , which are the combination between convolutional_neural_network ( cnn ) and long short-term_memory ( lstm ) networks , called cnn-lstm , to recognize video subtitles . we created the simplified cnn architecture with 16 weight layers . the last layer of the cnn was downsampling using max-pooling before sending it to the lstm network . we first trained our cnn-lstm architecture on printed_text data which contained various font styles , diverse font sizes , and complicated backgrounds . the connectionist temporal classification was then used as a loss_function to calculate the loss value and decode the output of the network . for the video subtitle dataset , we collected 24 videos from youtube and facebook , containing thai , english , arabic , and thai numbers . the dataset also contained 157 characters . in this dataset , we extracted 4,224 subtitle images from the videos . the proposed cnn-lstm architecture achieved an average character_error_rate of 9.36 % .
title : a comparison of approaches for imbalanced classification problems in the context of retrieving relevant_documents for an analysis ; abstract : one of the first steps in many text-based social_science studies is to retrieve documents that are relevant for the analysis from large corpora of otherwise irrelevant_documents . the conventional approach in social_science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords . but the application of incomplete keyword lists risks drawing biased inferences . more complex and costly methods such as query_expansion techniques , topic_model-based classification rules , and active as well as passive supervised_learning could have the potential to more accurately separate relevant from irrelevant_documents and thereby reduce the potential size of bias . yet , whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all , and if so , by how much , is unclear as a comparison of these approaches is lacking . this study closes this gap by comparing these methods across three retrieval tasks associated with a data set of german tweets ( linder , 2017 ) , the social_bias inference corpus ( sbic ) ( sap et al. , 2020 ) , and the reuters-21578 corpus ( lewis , 1997 ) . results show that query_expansion techniques and topic_model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance . active supervised_learning , however , if applied on a not too small set of labeled training_instances ( e.g . 1,000 documents ) , reaches a substantially higher retrieval performance than keyword lists .
title : sectioning of biomedical_abstracts : a sequence of sequence classification_task ; abstract : rapid_growth of the biomedical_literature has led to many advances in the biomedical_text_mining field . among the vast amount of information , biomedical article abstracts are the easily_accessible sources . however , the number of the structured abstracts , describing the rhetorical sections with one of background , objective , method , result and conclusion categories is still not considerable . exploration of valuable_information in the biomedical_abstracts can be expedited with the improvements in the sequential sentence_classification task . deep_learning based models has great performance/potential in achieving significant results in this task . however , they can often be overly complex and overfit to specific data . in this project , we study a state-of-the-art deep_learning model , which we called ssn-4 model here . we investigate different components of the ssn-4 model to study the trade-off between the performance and complexity . we explore how well this model generalizes to a new data set beyond randomized controlled trials ( rct ) dataset . we address the question that whether word_embeddings can be adjusted to the task to improve the performance . furthermore , we develop a second model that addresses the confusion pairs in the first model . results show that ssn-4 model does not appear to generalize well beyond rct dataset .
title : degree of belonging as an input for automatic text_classification : a syntactic approach ; abstract : grouping documents into categories is one of the solutions adopted to streamline the information_retrieval process , which is increasingly relevant due to the large amount of information available today . the manual localization of documents of a specific theme , available in digital repositories , involves reading the title , abstract and keywords , in addition to further detailed evaluation in order to identify whether the publication belongs to the desired thematic axis . considering the number of publications in a digital repository , manually locating all the desired texts on a given topic can be laborious and time-consuming . this research proposes an architecture for automatic classification of texts that is based on syntactic questions , that is , it undertakes a comparison of n-grams , which are combinations of n-pairs of words that are identified throughout the text . an exploratory applied research was carried out , which applied a type of supervised_learning , fundamentally based on the document_representation model called bag-of-words ( bow ) . the paper ’ s macro objective was to classify texts in general , according to pre-defined_categories , by generating and comparing degrees of belonging between texts , as one of the key criteria . the results of these comparisons , using n-gram = 3 , demonstrate that in the use of classifications by n-grams , the greater the number of grams , and with the removal of the stop_words , we obtain a reduced degree of belonging , demonstrating greater rigor in identifying the match during the classification . in order to have greater confidence in the results , a larger training corpus is necessary to expand the number of words that characterize the pre-defined_categories , to be used in the classification of the texts .
title : evaluation of retweet clustering method classification method using retweets on twitter without text data ; abstract : burst phenomena , which frequently occur on social_media , are caused by such social events as flaming on the internet , elections , and natural_disasters . to understand people 's thoughts and feelings , we must classify their opinions from burst phenomena . therefore , classification methods that categorize tweets are critical . however , since most classification methods focus on text_mining , they can not group tweets by topics because each tweet has poor linguistic similarities . we used a non-text-based classification method proposed by baba et al . that groups tweets by topics , even if they have poor linguistic similarities , and verified its validity by comparing it with a text-based classification method in two different evaluations : qualitative and quantitative . in the qualitative evaluation part , we did a questionnaire survey and validated the suitability of the topic clusters created using both the nonand text-based methods . since evaluating the similarity of every pair of tweets in each topic is difficult , we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method . in the quantitative evaluation part , we focused on the robustness of each method against data reduction . many approaches analyze social_media data , especially because collecting data from social_media is comparatively easy . however , since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social_media data , robustness against data reduction is an important index to evaluate classification methods . with the non-text-based method , over 55 % of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10 % in all three of our example cases . in this paper , as a source we focus on twitter , one of the most popular microblogging services . using clustering to conduct detailed case analyses , we scrutinized three burst cases that include natural_disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method .
title : immune based feature_selection for opinion_mining ; abstract : opinions about a particular product , service or person are communicated effectively through online_media such as facebook , myspace and twitter . unfortunately only a few researchers had researched on the performance of opinion_mining using online messages that were written in malay languages . opinion_mining processing that uses natural_language_processing approach is difficult due to the high content of noisy_texts in online messages . on the other hand , opinion_mining that uses machine_learning approach requires a good feature_selection technique since the current filter typed feature_selection techniques require interference from the user to select the appropriate features . this study used a feature_selection technique based on artificial immune system to select the appropriated features for opinion_mining . experiments with 2000 online movie reviews illustrated that the technique has reduced 90 % of the features and improved opinion_mining accuracy up to 15 % with k_nearest_neighbor classifier and up to 6 % with naïve baiyes classifier .
title : classification and ranking of trending_topics in twitter using tweets text ; abstract : every day millions of twitter user tweet their views on various topics using short messages of 140 to 280 characters length . here we monitor the public opinion in twitter thereby identifying trending_topics . to identify the current trending_topics on twitter , tweet classification is done . tweet classification is a process of classifying the tweets based on the topics using the keywords of the tweets as a feature . tweets are extracted from twitter using twitter api . in the existing system , trending_topics in twitter can be identified by hash_tags . this work is going to categorize the tweets into seventeen classes and identify the trending_topics by using words . dataset used in this work is the list of tweets taken for two to three days from twitter . by classifying tweets based on sensitive_words for a certain time , top k most trending words from a topic will be obtained that convey the dynamic trend .
title : an enhanced text_classification model by the inverted attention orthogonal projection module ; abstract : the orthogonal projection_method has made significant_progress in text_classification , especially in generating discriminative_features . this method obtains more pure and suitable for classification features by projecting text features onto the orthogonal direction of common features ( which are not helpful for classification and actually confuse performance ) . however , this approach requires an additional branch network to generate these common features , which reduces the flexibility of this method compared to representation optimisation methods such as self-attention_mechanisms , as it requires significant modification of the base network structure to use . to address this issue , this paper proposes the inversed attention orthogonal projection module ( iaopm ) . iaopm uses inversed attention ( ia ) to iteratively reverse the attention map on text features , encouraging the network to remove discriminating_features from the text features and obtain potential common features . unlike the original orthogonal projection_method , iaopm can extract common features within a single network without any branch networks , increasing the flexibility of the orthogonal projection_method . we also use an orthogonal loss to ensure the quality of the common features during training , so iaopm also has better purity performance than the original method . experiments show that text_classification models based on iaopm outperform the baseline_models , self-attention_mechanisms , and the original orthogonal projection_method on multiple text_classification datasets with an average accuracy increase of 1.02 % , 0.44 % , and 0.52 % , respectively .
title : automatic_language_identification ; abstract : automatic_language_identification or simply language identification is the task of automatically identifying the language ( s ) contained in a given document . it is an important part of many text processing pipelines including text_mining applications . this chapter provides a concise overview on language identification research from early approaches to state-of-the-art methods .
title : classifying syntactic errors in learner language ; abstract : we present a method for classifying syntactic errors in learner language , namely errors whose correction alters the morphosyntactic structure of a sentence . the methodology builds on the established universal dependencies syntactic representation scheme , and provides complementary_information to other error-classification systems . unlike existing error classification methods , our method is applicable across languages , which we showcase by producing a detailed picture of syntactic errors in learner english and learner russian . we further demonstrate the utility of the methodology for analyzing the outputs of leading grammatical_error_correction ( gec ) systems .
title : multi-label arabic text_classification : an overview ; abstract : —there is a massive_growth of text documents on the web . this led to the increasing need for methods that can organize and classify electronic documents ( instances ) automatically . multi-label_classification task is widely used in real-world problems and it has been applied on different applications . it assigns multiple_labels for each document simultaneously . few and insufficient research studies have investigated the multi-label_text_classification problem in the arabic_language . therefore , this survey paper aims to present an extensive review of the existing multi-label_classification methods and techniques that can deal with multi-label problem . besides , we focus on arabic_language by covering the relevant applications of multi-label_classification on the arabic text , and identify the main challenges_faced by these studies . furthermore , this survey presents an experimental comparisons of different multi-label_classification methods applied for the arabic context and points out some baseline results . we found that further investigations are also needed to improve the multi-label_classification task in the arabic_language , especially the hierarchical classification_task .
title : learning fused representations for large-scale multimodal classification ; abstract : multimodal strategies combine different input sources into a joint_representation that provides enhanced information from the unimodal strategy . in this article , we present a novel multimodal_approach that fuses image and encoded text description to obtain an information-enriched image . this approach casts encoded text obtained from word2vec word_embedding into visual embedding to be concatenated with the image . we employ standard convolutional_neural_networks to learn representations of information-enriched images . finally , we compare our approach with the unimodal approach and their combination on three large-scale multimodal datasets . our findings indicate that the joint_representation of encoded text and image in feature_space improves the multimodal classification performance aiding the interpretability .
title : chinese relation_extraction based on deep_belief nets ; abstract : relation_extraction is a fundamental task in information_extraction , which is to identify the semantic relationships between two entities in the text . in this paper , deep_belief nets ( dbn ) , which is a classifier of a combination of several unsupervised_learning networks , named rbm ( restricted_boltzmann machine ) and a supervised_learning network named bp ( back-propagation ) , is presented to detect and classify the relationships among chinese name entities . the rbm layers maintain as much information as possible when feature_vectors are transferred to next layer . the bp layer is trained to classify the features generated by the last rbm layer . the experiments are conducted on the automatic content extraction 2004 dataset . this paper proves that a character-based feature is more suitable for chinese relation_extraction than a word-based feature . in addition , the paper also performs a set of experiments to assess the chinese relation_extraction on different assumptions of an entity categorization feature . these experiments showed the comparison among models with correct entity_types and imperfect entity_type classified by dbn and without entity_type . the results show that dbn is a successful approach in the high-dimensional-feature-space information_extraction task . it outperforms state-of-the-art learning models such as svm and back-propagation networks . © 2012 iscas .
title : improvement in the efficiency of a distributed multi-label_text_classification algorithm using infrastructure and task-related data ; abstract : distributed_computing technologies allow a wide variety of tasks that use large amounts of data to be solved . various paradigms and technologies are already widely used , but many of them are lacking when it comes to the optimization of resource usage . the aim of this paper is to present the optimization methods used to increase the efficiency of distributed implementations of a text-mining model utilizing information about the text-mining task extracted from the data and information about the current state of the distributed_environment obtained from a computational node , and to improve the distribution of the task on the distributed infrastructure . two optimization solutions are developed and implemented , both based on the prediction of the expected task duration on the existing infrastructure . the solutions are experimentally_evaluated in a scenario where a distributed tree-based multi-label classifier is built based on two standard text data collections .
title : does the geometry of word_embeddings help document_classification ? a case_study on persistent homology based representations ; abstract : we investigate the pertinence of methods from algebraic_topology for text data analysis . these methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document_embedding , which is stable with respect to the geometry of the document in the selected metric_space . in this work , we evaluate the utility of these topology-based document_representations in traditional nlp tasks , specifically document_clustering and sentiment_classification . we find that the embeddings do not benefit text analysis . in fact , performance is worse than simple techniques like $ \textit { tf-idf } $ , indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets .
title : lessons_learned from applying off-the-shelf bert : there is no silver bullet ; abstract : one of the challenges in the nlp field is training large classification models , a task that is both difficult and tedious . it is even harder when gpu hardware is unavailable . the increased availability of pre-trained and off-the-shelf word_embeddings , models , and modules aim at easing the process of training large models and achieving a competitive_performance . we explore the use of off-the-shelf bert models and share the results of our experiments and compare their results to those of lstm networks and more simple baselines . we show that the complexity and computational_cost of bert is not a guarantee for enhanced predictive performance in the classification_tasks at hand .
title : 1cademy @ causal news corpus 2022 : leveraging self-training in causality classification of socio-political event data ; abstract : this paper details our participation in the challenges and applications of automated extraction of socio-political_events from text ( case ) workshop @ emnlp 2022 , where we take part in subtask 1 of shared_task 3 { citep { tan-etal-2022-event } . we approach the given task of event causality detection by proposing a self-training pipeline that follows a teacher-student classifier method . more specifically , we initially train a teacher model on the true , original task data , and use that teacher model to self-label data to be used in the training of a separate student model for the final task prediction . we test how restricting the number of positive or negative self-labeled_examples in the self-training process affects classification performance . our final results show that using self-training produces a comprehensive performance_improvement across all models and self-labeled training_sets tested within the task of event causality sequence classification . on top of that , we find that self-training performance did not diminish even when restricting either positive/negative examples used in training.our code is be publicly available at { hyperlink { https : //github.com/gzhang-umich/1cademyteamofcase } { https : //github.com/gzhang-umich/1cademyteamofcase } .
title : classification of phonological parameters in sign_languages ; abstract : signers compose sign_language phonemes that enable communication by combining phonological parameters such as handshape , orientation , location , movement , and non-manual features . linguistic research often breaks down signs into their constituent parts to study sign_languages and often a lot of effort is invested into the annotation of the videos . in this work we show how a single model can be used to recognise the individual phonological parameters within sign_languages with the aim of either to assist linguistic annotations or to describe the signs for the sign recognition models . we use danish sign_language data set ` ordbog over dansk tegnsprog ' to generate multiple data sets using pose estimation model , which are then used for training the multi-label fast r-cnn model to support multi-label modelling . moreover , we show that there is a significant co-dependence between the orientation and location phonological parameters in the generated data and we incorporate this co-dependence in the model to achieve better performance .
title : pseudo-labeling with transformers for improving question_answering systems ; abstract : advances in neural_networks contributed to the fast development of natural_language_processing systems . as a result , question_answering systems have evolved and can classify and answer_questions in an intuitive yet communicative way . however , the lack of large_volumes of labeled_data prevents large-scale training and development of question_answering systems , confirming the need for further research . this paper aims to handle this real-world problem of lack of labeled datasets by applying a pseudo-labeling technique relying on a neural_network transformer model distilbert . in order to evaluate our contribution , we examined the performance of a text_classification transformer model that was fine-tuned on the data subject to prior pseudo-labeling . research has shown the usefulness of the applied pseudo-labeling technique on a neural_network text_classification transformer model distilbert . the results of our analysis indicated that the model with additional pseudo-labeled_data achieved the best results among other compared neural_network architectures . based on that result , question_answering systems may be directly improved by enriching their training steps with additional data acquired cost-effectively .
title : semi-supervised and unsupervised methods for categorizing posts in web discussion_forums ; abstract : web discussion_forums are used by millions of people worldwide to share information belonging to a variety of domains such as automotive vehicles , pets , sports , etc . they typically contain posts that fall into different categories such as problem , solution , feedback , spam , etc . automatic identification of these categories can aid information_retrieval that is tailored for specific user_requirements . previously , a number of supervised methods have attempted to solve this problem ; however , these depend on the availability of abundant training_data . a few existing unsupervised and semi-supervised approaches are either focused on identifying a single category or do not report category-specific_performance . in contrast , this work proposes unsupervised and semi-supervised methods that require no or minimal training_data to achieve this objective without compromising on performance . a fine-grained analysis is also carried out to discuss their limitations . the proposed methods are based on sequence models ( specifically , hidden_markov_models ) that can model language for each category using word and part-of-speech probability_distributions , and manually specified features . empirical_evaluations across domains demonstrate that the proposed methods are better suited for this task than existing ones .
title : grounded language understanding for manipulation instructions using gan-based classification ; abstract : the target task of this study is grounded language understanding for domestic service robots ( dsrs ) . in particular , we focus on instruction understanding for short sentences where verbs are missing . this task is of critical importance to build communicative dsrs because manipulation is essential for dsrs . existing instruction understanding methods usually estimate missing information only from non-grounded knowledge ; therefore , whether the predicted action is physically executable or not was unclear . in this paper , we present a grounded instruction understanding method to estimate appropriate objects given an instruction and situation . we extend the generative_adversarial nets ( gan ) and build a gan-based classifier using latent representations . to quantitatively evaluate the proposed method , we have developed a data set based on the standard data set used for visual qa . experimental results have shown that the proposed method gives the better result than baseline_methods .
title : semantic term `` blurring '' and stochastic `` barcoding '' for improved unsupervised text_classification ; abstract : the abundance of text data being produced in the modern age makes it increasingly important to intuitively group , categorize , or classify text data by theme for efficient retrieval and search . yet , the high dimensionality and imprecision of text data , or more generally language as a whole , prove to be challenging when attempting to perform unsupervised document_clustering . in this thesis , we present two novel methods for improving unsupervised document_clustering/classification by theme . the first is to improve document_representations . we look to exploit `` term neighborhoods '' and `` blur '' semantic weight across neighboring terms . these neighborhoods are located in the semantic space afforded by `` word_embeddings . '' the second method is for cluster revision , based on what we deem as `` stochastic barcoding '' , or `` s- barcode '' patterns . text data is inherently high dimensional , yet clustering typically takes_place in a low_dimensional representation space . our method utilizes lower dimension clustering results as initial cluster configurations , and iteratively revises the configuration in the high_dimensional space . we show with experimental results how both of the two methods improve the quality of document_clustering . while this thesis elaborates on the two new conceptual contributions , a joint thesis by david yan details the feature_transformation and software_architecture we developed for unsupervised document_classification .
title : a novel text-mining approach for retrieving pharmacogenomics associations from the literature ; abstract : text_mining in biomedical_literature is an emerging_field which has already been shown to have a variety of implementations in many research areas , including genetics , personalized_medicine , and pharmacogenomics . in this study , we describe a novel text-mining approach for the extraction of pharmacogenomics associations . the code that was used toward this end was implemented using r programming_language , either through custom scripts , where needed , or through utilizing functions from existing libraries . articles ( abstracts or full texts ) that correspond to a specified query were extracted from pubmed , while concept annotations were derived by pubtator central . terms that denote a mutation or a gene as well as chemical_compound terms corresponding to drug compounds were normalized and the sentences containing the aforementioned terms were filtered and preprocessed to create appropriate training_sets . finally , after training and adequate hyperparameter_tuning , four text classifiers were created and evaluated ( fasttext , linear_kernel svms , xgboost , lasso , and elastic-net regularized generalized linear models ) with regard to their performance in identifying pharmacogenomics associations . although further improvements are essential toward proper implementation of this text-mining approach in the clinical_practice , our study stands as a comprehensive , simplified , and up-to-date approach for the identification and assessment of research articles enriched in clinically_relevant pharmacogenomics relationships . furthermore , this work highlights a series of challenges concerning the effective application of text_mining in biomedical_literature , whose resolution could substantially contribute to the further development of this field .
title : dan : dual-view representation_learning for adapting stance classifiers to new domains ; abstract : we address the issue of having a limited number of annotations for stance_classification in a new domain , by adapting out-of-domain classifiers with domain_adaptation . existing_approaches often align different domains in a single , global feature_space ( or view ) , which may fail to fully capture the richness of the languages used for expressing stances , leading to reduced adaptability on stance data . in this paper , we identify two major types of stance expressions that are linguistically distinct , and we propose a tailored dual-view adaptation network ( dan ) to adapt these expressions across domains . the proposed model first learns a separate view for domain transfer in each expression channel and then selects the best adapted parts of both views for optimal transfer . we find that the learned view features can be more easily aligned and more stance-discriminative in either or both views , leading to more transferable overall features after combining the views . results from extensive_experiments show that our method can enhance the state-of-the-art single-view methods in matching stance data across different domains , and that it consistently_improves those methods on various adaptation tasks .
title : investigating embeddings for sentiment_analysis in italian ; abstract : the present paper compares the performance of both contextualized and context-free embeddings used for sentiment_analysis tasks in italian . the selected scenario is a pre-analysis stage when the gross architectural parameters of the pipeline have to be devised , while both small data sets can be used for training the model and experiments have to be performed with reduced computational_power . two pipelines have been set up to this aim : the first one makes use of glove , which has been suitably trained on the same domain of the task at hand , and a deep neural_architecture is used for classification . the second model uses a pre-trained bert for the italian_language to perform the whole task . the result of our study is that a context-free embedding trained on the task domain outperforms the generic contextualized one . the presented models are reported in detail , along with the experimentations on both the sentipolc 2016 data set and a collection of about 100k tripadvisor reviews .
title : classification of complaint categories in e-commerce : a case_study of pt bukalapak ; abstract : bukalapak is one of the companies in indonesia that is engaged in e-commerce . it was recorded that in 2019 , bukalapak experienced a 230 % increase in user growth compared to the previous year . unfortunately , the growth in users is also followed by an increase in the number of complaints that occur in bukalapak . it was recorded that in 2020 , bukalapak complaints increased by 50 % at the end of 2020 when compared to the beginning of 2020 . the increase in complaints led to an increase in the average handle time for complaints which led to a decrease in user_satisfaction . 3 main issues that cause an increase in average handle time , namely an unstable system , complaint categorization is slow , and difficult to find solutions . this is certainly a concern for the management . in this study , the classification of complaints categories in bukalapak will be carried out . this study aims to find out what classification model is suitable to be used in determining the category of complaints in bukalapak . the classification model that will be used in this research is logistic_regression , k_nearest_neighbor , and support_vector_machine . while the data that will be used is data on complaints from january 2021 to december 2021 . from the results of the study , it was found that the logistic_regression classification model had the highest value among the other 2 models . the logistic_regression model managed to get an accuracy value of 83.9 % . the second position is occupied by the k_nearest_neighbors model with an accuracy of 77.6 % . last occupied by the svm model with an accuracy value of 40.5 % .
title : predictive aspect-based_sentiment_classification of online tourist reviews ; abstract : with the increase of online tourists reviews , discovering sentimental idea regarding a tourist place through the posted reviews is becoming a challenging task . the presence of various aspects discussed in user_reviews makes it even harder to accurately_extract and classify the sentiments . aspect-based sentiment_analysis aims to extract and classify user ’ s positive or negative orientation towards each aspect . although several aspect-based_sentiment_classification methods have been proposed in the past , limited work has been targeted towards the automatic extraction of implicit , infrequent and co-referential aspects . moreover , existing_methods lack the ability to accurately_classify the overall polarity of multi-aspect sentiments . this study aims to develop a predictive framework for aspect-based extraction and classification . the proposed framework utilises the semantic relations among review phrases to extract implicit and infrequent aspects for accurate sentiment predictions . experiments have been performed using real-world data sets crawled from predominant tourist websites such as tripadvisor and opentable . experimental results and comparison with previously_reported findings prove that the predictive framework not only extracts the aspects effectively but also improves the prediction_accuracy of aspects .
title : classification and impact of call-to-actions in push-notifications ; abstract : push-notifications are a constant presence in our lives today due to the growing ubiquity of devices and services with push-technology enabled . each push-notification is created with a purpose driven by the spawning device or application , however this purpose is not always clear to the users receiving the notifications . this paper explores the text content of push-notifications and the subsequent call-to-actions ( ctas ) associated with them . using an existing notification data set annotated with ctas , we present the results of a number of classifiers capable of inferring 11 distinct cta categories within push-notifications . by subsequently applying the cta classifier to two existing user-studies on push-notification engagement in-the-wild , the impact of ctas on push-notification receptivity was quantified and the implications for marketers creating and subscribers receiving them was analysed .
title : healthcare text_classification system and its performance evaluation : a source of better intelligence by characterizing healthcare text ; abstract : a machine_learning ( ml ) -based text_classification system has several classifiers . the performance evaluation ( pe ) of the ml system is typically driven by the training_data size and the partition protocols used . such systems lead to low accuracy because the text_classification systems lack the ability to model the input text data in terms of noise characteristics . this research study proposes a concept of misrepresentation ratio ( mrr ) on input healthcare text data and models the pe criteria for validating the hypothesis . further , such a novel system provides a platform to amalgamate several attributes of the ml system such as : data size , classifier type , partitioning protocol and percentage mrr . our comprehensive data analysis consisted of five types of text data sets ( twittera , webkb4 , disease , reuters ( r8 ) , and sms ) ; five kinds of classifiers ( support_vector_machine with linear_kernel ( svm-l ) , mlp-based neural_network , adaboost , stochastic_gradient_descent and decision_tree ) ; and five types of training protocols ( k2 , k4 , k5 , k10 and jk ) . using the decreasing order of mrr , our ml system demonstrates the mean classification_accuracies as : 70.13 ± 0.15 % , 87.34 ± 0.06 % , 93.73 ± 0.03 % , 94.45 ± 0.03 % and 97.83 ± 0.01 % , respectively , using all the classifiers and protocols . the corresponding auc is 0.98 for sms data using multi-layer_perceptron ( mlp ) based neural_network . all the classifiers , the best accuracy of 91.84 ± 0.04 % is shown to be of mlp-based neural_network and this is 6 % better over previously_published . further we observed that as mrr decreases , the system robustness increases and validated by standard deviations . the overall text system accuracy using all data types , classifiers , protocols is 89 % , thereby showing the entire ml system to be novel , robust and unique . the system is also tested for stability and reliability .
title : a novel density-based clustering method using word_embedding features for dialogue intention_recognition ; abstract : in dialogue_systems , understanding user_utterances is crucial for providing appropriate responses . various classification models have been proposed to deal with natural_language_understanding tasks related to user intention analysis , such as dialogue_acts or emotion recognition . however , models that use original lexical_features without any modifications encounter the problem of data sparseness , and constructing sufficient_training_data to overcome this problem is labor-intensive , time-consuming , and expensive . to address this issue , word_embedding models that can learn lexical synonyms using vast raw corpora have recently been proposed . however , the analysis of embedding features is not yet sufficient to validate the efficiency of such models . specifically , using the cosine_similarity score as a feature in the embedding_space neglects the skewed nature of the word_frequency distribution , which can affect the improvement of model performance . this paper describes a novel density-based clustering method that efficiently integrates word_embedding vectors into dialogue intention_recognition . experimental results show that our proposed model helps overcome the data_sparseness problem seen in previous classification models and can assist in improving the classification performance .
title : improving text_categorization by resolving semantic ambiguity ; abstract : in this investigation , we propose a new method for text_categorization ( tc ) based on bayesian approach by resolving ambiguity . the tc assumes weights to words of which meanings are ambiguous in a sense of synonymy_and_polysemy . we give weights to articles by examining dictionaries of thesaurus and of dimensionality_reduction to improve the quality of tc . also we show some experiments to illustrate how well our approach goes .
title : improving thai educational web_page_classification using inverse_class_frequency ; abstract : automatic text_classification for a web collection is a challenge task , especially in the case that the language is not english , such as thai . however , most of thai educational web_pages usually include english terms due to their technical aspect lots of technical_terms and typing errors both in thai and in english are found in web_sites of universities . most previous_works on text_categorization applied term_frequency_and_inverse_document_frequency for representing importance of terms . in this paper , we use inverse_class_frequency instead of inverse_document_frequency in centroid-based text_categorization because it works well on a collection with a large number of unique terms . the experimental results show that inverse_class_frequency is useful , especially when it is applied on both prototype and query vectors . © 2005 ieee .
title : comparative study of word_embeddings for classification of scientific_article on human health risk of electromagnetic fields ; abstract : this paper presents a comparative study on pre-trained_word_embeddings for the classification of scientific_articles on the human health risk of emf . in this study , the various neural_network ( nn ) models are trained using the three biomedical pre-trained_word_embeddings ( i.e. , pubmed-word2vec , biowordvec , and pubmed-bert ) . then , the performances of the trained nn models are evaluated to compare the pre-trained_word_embeddings . the evaluation results show that the nn models using pubmed-bert outperform the other nn models using pubmed-word2vec and biowordvec in the classification of scientific_articles on the human health risk of emf .
title : electric_power audit text_classification with multi-grained pre-trained_language_model ; abstract : electric_power audit text_classification is one of the important research problem in electric_power systems . recently , kinds of automatic classification methods for these texts based on machine_learning or deep_learning models have been applied . at present , the development of computing technology makes 'pre-training and fine-tuning ' the newest paradigm of text_classification , which achieves better results than previous fully-supervised models . based on pre-training theory , domain-related pre-training tasks can enhance the performance of downstream_tasks in the specific domain . however , existing pre-training models usually use general corpus for pre-training , and do not use texts related to the field of electric_power , especially electric_power audit texts . this results in that the model does not learn too much electric-power-related morphology or semantics in the pre-training stage , so that less information can be used in the fine-tuning stage . based on the research status , in this paper , we propose epat-bert , a bert-based model pre-trained by two-granularity pre-training tasks : word-level masked_language_model and entity-level masked_language_model . these two tasks predict word and entity in electric-power-related texts to learn abundant morphology and semantics about electric_power . we then fine-tune epat-bert for electric_power audit text_classification task . the experimental results show that , compared with fully_supervised machine_learning models , neural_network models , and general pre-trained_language_models , epat-bert can significantly_outperform existing models in a variety of evaluation_metrics . therefore , epat-bert can be further applied to electric_power audit text_classification . we also conduct ablation studies to prove the effectiveness of each component in epat-bert to further illustrate our motivations .
title : detecting and monitoring hate_speech in twitter ; abstract : social_media are sensors in the real_world that can be used to measure the pulse of societies . however , the massive and unfiltered feed of messages_posted in social_media is a phenomenon that nowadays raises social alarms , especially when these messages contain hate_speech targeted to a specific individual or group . in this context , governments and non-governmental organizations ( ngos ) are concerned about the possible negative_impact that these messages can have on individuals or on the society . in this paper , we present haternet , an intelligent system currently being used by the spanish national office against hate_crimes of the spanish_state secretariat for security that identifies and monitors the evolution of hate_speech in twitter . the contributions of this research are many-fold : ( 1 ) it introduces the first intelligent system that monitors and visualizes , using social_network_analysis techniques , hate_speech in social_media . ( 2 ) it introduces a novel public dataset on hate_speech in spanish consisting of 6000 expert-labeled tweets . ( 3 ) it compares several classification approaches based on different document_representation strategies and text_classification models . ( 4 ) the best approach consists of a combination of a ltsm+mlp neural_network that takes as input the tweet ’ s word , emoji , and expression tokens ’ embeddings enriched by the tf-idf , and obtains an area under the curve ( auc ) of 0.828 on our dataset , outperforming previous methods presented in the literature .
title : semi-supervised latent_dirichlet_allocation and its application for document_classification ; abstract : latent_dirichlet_allocation ( lda ) is an unsupervised topic_modeling method widely applied in natural_language_processing . however , standard lda does not permit the use of supervised labels to incorporate expert_knowledge into the learning procedure . this paper describes a semi-supervised lda ( sslda ) method that supports multiple-topic labels per document , to incorporate available expert_knowledge during the model construction . this improvement enables the alignment of resulting model with human expectations for topic_modeling and extraction . we apply sslda to document_classification problem on benchmark_datasets . we investigate and compare how the size of training_set and proportion of supervised data affect the final model structure and improve the prediction_accuracy . © 2012 ieee .
title : paclic 23 - proceedings of the 23rd pacific asia conference on language , information and computation - volume 2 ; abstract : the proceedings contain 53 papers . the topics include : a syntax-pragmatics approach ; a lexicalized tree adjoining grammar for thai ; a hybrid model for sense guessing of chinese unknown_words ; hybrid n-gram probability estimation in morphologically_rich_languages ; summarization approaches based on document probability_distributions ; chinese function tag labeling ; passage_retrieval using answer_type profiles in question_answering ; sess ; a bootstrapping method for finer-grained opinion_mining using graph model ; summarizing opinions in blog threads ; interpolated plsi for learning plausible verb arguments ; building online corpora of philippine_languages ; latin etymologies as features on bnc text_categorization ; experiments on domain_adaptation for english-hindi smt ; effective use of chinese structural auxiliaries for chinese parsing ; word boundary decision with crf for chinese_word_segmentation ; towards bilingual term_extraction in comparable patents ; factors_affecting part-of-speech_tagging for tagalog ; using wikipedia for hierarchical finer categorization of named_entities ; supertagging with factorial hidden_markov_models ; basenp supersense tagging for japanese texts ; design of chinese hpsg framework for data-driven parsing ; generalizable features help semantic_role_labeling ; and towards establishing a hierarchy in the japanese sentence structure .
title : n-gram based text_classification for persian newspaper corpus ; abstract : statistical n-gram language modeling is applied in many domains like speech_recognition , language identification , machine_translation , character_recognition and topic classification . most language modeling approaches work on n-grams of words . in this paper , we employ language models classifier based on word_level n-grams for persian text_classification . the presented approach computes the occurrence probability on word_sequence in training_data . then by extracting the word_sequence in test data , it can predict the highest probability for related class to given news text . we show that statistical language modeling can significantly cause high classification performance . the experimental results on hamshahri corpus show satisfactory_results and n-grams of length 3 are the most useful for persian text_classification . © 2011 aicit .
title : cyberbullying detection through sentiment_analysis ; abstract : in recent_years with the widespread of social_media platforms across the globe especially among young people , cyberbullying and aggression have become a serious and annoying problem that communities must deal with . such platforms provide various ways for bullies to attack and threaten others in their communities . various techniques and methodologies have been used or proposed to combat cyberbullying through early detection and alerts to discover and/or protect victims from such attacks . machine_learning ( ml ) techniques have been widely used to detect some language patterns that are exploited by bullies to attack their victims . also . sentiment_analysis ( sa ) of social_media content has become one of the growing areas of research in machine_learning . sa provides the ability to detect cyberbullying in real-time . sa provides the ability to detect cyberbullying in real-time . this paper proposes a sa model for identifying cyberbullying s in twitter social_media . support_vector_machines ( svm ) and naïve_bayes ( nb ) are used in this model as supervised_machine_learning classification tools . the results of the experiments conducted on this model showed encouraging outcomes when a higher n-grams language model is applied on such s in comparison with similar previous_research . also , the results showed that svm classifiers have better performance_measures than nb classifiers on such tweets .
title : interactive_attention network fusion bi-lstm and cnn for text_classification ; abstract : at present , many text_classification model methods integrate convolution neural_network ( cnn ) and long short-term_memory ( lstm ) , although they have achieved good performance , they have not fully taking advantage of the characteristics of these two models , this paper proposes a text_classification model cr-ian model based on interactionlevel attention fusion of multi-channel cnn and bi-lstm . first : the local feature_vectors of different latitudes of text are extracted through multi-channel cnn ; at the same time : pass the text matrix through the bi-lstm layer to obtain the result vector of the bidirectional semantic learning of the text . then : calculate the interactive_attention according to the bidirectional output matrix and the local feature_vector matrix . this can fully consider the characteristics of the text with semantics , the model can learn more text information . finally : pass the interactive_attention through the fully_connected layer and use the softmax activation_function to get the classification result .
title : mining scientific_articles powered by machine_learning techniques ; abstract : literature_review is one of the most important phases of research . scientists must identify the gaps and challenges about certain area and the scientific_literature , as a result of the accumulation of knowledge , should provide enough information . the problem is where to find the best and most important articles that guarantees to ascertain the state of the art on that specific domain . a feasible literature_review consists on locating , appraising , and synthesising the best empirical evidences in the pool of available publications , guided by one or more research questions . nevertheless , it is not assured that searching interesting articles in electronic databases will retrieve the most relevant content . indeed , the existent search_engines try to recommend articles by only looking for the occurrences of given keywords . in fact , the relevance of a paper should depend on many other factors as adequacy to the theme , specific tools used or even the test strategy , making automatic recommendation of articles a challenging problem . our approach allows researchers to browse huge article collections and quickly find the appropriate publications of particular interest by using machine_learning techniques . the proposed solution automatically_classifies and prioritises the relevance of scientific_papers . using previous samples manually_classified by domain_experts , we apply a naive_bayes_classifier to get predicted articles from real_world journal repositories such as ieee xplore or acm digital . results suggest that our model can substantially recommend , classify and rank the most relevant_articles of a particular scientific field of interest . in our experiments , we achieved 98.22 % of accuracy in recommending articles that are present in an expert classification list , indicating a good prediction of relevance . the recommended papers worth , at least , the reading . we envisage to expand our model in order to accept user 's filters and other inputs to improve predictions .
title : question_answering on agricultural knowledge_graph based on multi-label_text_classification ; abstract : traditional search_engines retrieve relevant web_pages based on keywords in the entered questions , while sometimes the required information may not be included in these keyword-based retrieved web_pages . compared to the search_engines , the question_answering system can provide more accurate_answers . however , traditional question_answering systems can only provide answers to users based on matching the questions in a question_answering pair . at the same time , the number of question_answering pairs remain somewhat limited . as a result , the user ’ s requirements can not be met well . in contrast , knowledge graphs can store information such as entities and their relationships in a structured pattern . therefore , the knowledge_graph is highly scalable as the data is stored in a structured_form . besides , the relationship between entities and the knowledge_graph structure allows the desired answer to be found quickly . moreover , the process of relation_classification can also be regarded as an operation of text_classification . therefore , this study proposed a new approach to knowledge_graph-based question_answering systems that require a named_entity_recognition method and a multi-label_text_classification method to search for the answers . the results of entity name and question_type are turned into a cypher query that searches for the answer in the knowledge_graph . in this paper , three models , i.e. , textcnn , bi-lstm , and bi-lstm + att , are used to examine the effectiveness of multi-label_text_classification , demonstrating our method ’ s feasibility . among these three models , textcnn worked best , attaining an f1 score of 0.88 .
title : automated classification of social_network messages into smart_cities dimensions ; abstract : a smart_city can be defined as a high-tech city with several public and private services capable to strategically solve ( or mitigate ) problems normally generated by rapid urbanization . different models of indicators have been developed to follow cities ’ evolution to become a smart_city . an example of such model is the standard 37120 from the international_organization_for_standardization ( iso ) that proposes a set of dimensions and indicators ( e.g . transportation , recreation , solid_waste ) for services and quality of life for sustainable cities and communities . it has been common to find official social_network profiles of organizations and governmental entities related to the services they provide or are responsible for ( water , waste , transportation , cultural events , etc . ) and that are used by citizens as a gateway to directly interact and communicate their complains and problems about those services . the present paper proposes to apply machine_learning algorithms over the urban data generated by social_networks in order to create classifiers to automatically categorize citizens messages according to the different cities services dimensions . for that , two distinct text datasets in portuguese were collected from two social_networks : twitter ( 1,950 tweets ) and colab.re ( 65,066 posts ) . the texts were mapped according to the different iso 37120 categories , preprocessed and mined through the use of 8 algorithms implemented in scikit-learn . initial results pointed out the feasibility of the proposal with models achieving average f1-measures around 55 % for f1-macro and 78 % for f1-micro when using linear vector classification , logistic_regression , decision_tree and complement naive_bayes . however , as the datasets were highly unbalanced , the performances of the models vary significantly for each iso category , with the best results occurring for wastewater , water & sanitation , energy and transportation . the classifiers generated here can be integrated on a number of different city services and systems such as : governmental support decision systems , customer complain systems , communities dashboards , police offices , transportation 's companies , cultural producers , environmental agencies , and recyclers ’ companies .
title : automatic recognition of native advertisements for the slovak_language ; abstract : in recent_years the native advertisement is becoming more and more prevalent in online spaces . differentiating between genuine content and native advertisement using natural_language_processing is therefore also becoming a very interesting research topic . in this paper , we examine the possibilities of using deep textual_representation for the slovak_language to recognize the “ pr ( public_relations ) articles ” ( that serve as a native advertisement in this context ) from authentic news articles on popular slovak news websites . we show that the bert ( bidirectional_encoder_representations_from_transformers ) embeddings as a text_representation are sufficient for this task ( achieving accuracy over 80 % even with a statistical_model - logistic_regression ) and that the models generally perform better without prior lemmatization . we have scraped three slovak news websites ( for a total of 5455 news articles containing both paid-for content and a wide variety of genuine categories ) , and we have evaluated multiple binary classification methods ( logistic_regression , random_forest classifier and support_vector_machines ) trained on top of generated roberta sentence_embeddings . on our testing_set , we were able to achieve an accuracy of 85.13 % .
title : better reasoning behind classification predictions with bert for fake_news_detection ; abstract : fake_news_detection has become a major task to solve as there has been an increasing number of fake_news on the internet in recent_years . although many classification models have been proposed based on statistical learning methods showing good results , reasoning behind the classification performances may not be enough . in the self-supervised_learning studies , it has been highlighted that a quality of representation ( embedding ) space matters and directly_affects a downstream_task performance . in this study , a quality of the representation space is analyzed visually and analytically in terms of linear separability for different classes on a real and fake_news dataset . to further add interpretability to a classification model , a modification of class activation mapping ( cam ) is proposed . the modified cam provides a cam score for each word token , where the cam score on a word token denotes a_level of focus on that word token to make the prediction . finally , it is shown that the naive bert model topped with a learnable linear layer is enough to achieve robust performance while being compatible with cam .
title : accelerating semi-supervised text_classification by k-way projecting networks ; abstract : the state of the art semi-supervised_learning framework has greatly shown its potential in making deep and complex language models such as bert highly effective for text_classification tasks when labeled_data is limited . however , the large size and low inference_speed of such models may hinder their application on resources-limited or real-time use cases . in this paper , we propose a new approach in semi-supervised_learning framework to distill large complex teacher model into a fairly lightweight student model which has the ability of acquiring knowledge from different layers of teacher with the usage of k-way projecting networks . across four english datasets in text_classification benchmarks and one dataset collected from an chinese online course , our experiment_shows that this student model achieves comparable_results with the state of the art transformer-based semi-supervised text_classification methods , while using only 0.156mb parameters and having an inference_speed 785 times_faster than the teacher model .
title : improving biochemical named_entity_recognition using pso classifier selection and bayesian combination methods ; abstract : named_entity_recognition ner is a basic step for large number of consequent text_mining tasks in the biochemical domain . increasing the performance of such recognition systems is of high importance and always poses a challenge . in this study , a new community based decision_making system is proposed which aims at increasing the efficiency of ner_systems in the chemical/drug name context . particle_swarm_optimization pso algorithm is chosen as the expert selection_strategy along with the bayesian combination method to merge the outputs of the selected classifiers as well as evaluate the fitness of the selected candidates . the proposed system performs in two steps . the first step focuses on creating various numbers of baseline_classifiers for ner with different features sets using the conditional_random_fields crfs . the second step involves the selection and efficient combination of the classifiers using pso and bayesisan combination . two comprehensive corpora from biocreative events , namely chemdner and cemp , are used for the experiments conducted . results show that the ensemble of classifiers selected by means of the proposed approach perform better than the single best classifier as well as ensembles formed using other popular selection/combination strategies for both corpora . furthermore , the proposed method_outperforms the best performing system at the biocreative iv chemdner track by achieving an f-score of 87.95 percent .
title : a subjectivity detection method for opinion_mining based on lexical_resources ; abstract : extraction and monitoring of opinions about a product or service is much valuable to users of social_media and to companies as a feedback mechanism and as a source of information to define marketing campaigns . opinion_mining is a subfield of data_mining which aims at automatically_classify opinions with respect to their polarity , i.e. , to infer if the author of an opinion has either a positive or negative_sentiment about some subject ( e.g. , a movie , a car model , an appliance , etc ) . a common approach in the opinion_mining task is to preprocess the collection of opinion texts used as training_set in order to remove the sentences of an opinion that do not contain subjective content . these so called subjective extracts , one for each opinionated_text , are then used in a second step to train a polarity classifier , that is used to predict the orientation of the original opinion ( positive or negative ) . in this paper , we propose a new method for the subjectivity detection step of the opinion_mining task . our method is based on part-of-speech ( pos ) tagging each sentence of an opinionated_text , and on the use of lexical_resources to better generate the corresponding subjective extract . we take advantage of wordnet and sentiwordnet , two publically available lexical_resources , to calculate the association degrees between sentences of an opinionated document in the subjectivity detection step . we use well-known movie review datasets ( from the internet_movie_database ) to provide comparative_experiments and we show a statistically_significant increase in classification_accuracy of the resulting opinion_mining system that can be up to 9 % .
title : proceedings of the 2nd workshop on natural_language_processing and computational social_science , nlp+css 2017 at the 55th annual meeting of the association_for_computational_linguistics , acl 2017 ; abstract : the proceedings contain 13 papers . the topics discussed include : language-independent gender prediction on twitter ; when does a compliment become sexist ? analysis and classification of ambivalent sexism using twitter data ; personality driven differences in paraphrase preference ; community2vec : vector representations of online_communities encode semantic relationships ; telling apart tweets associated with controversial versus noncontroversial topics ; cross-lingual classification of topics in political texts ; mining social_science publications for survey variables ; linguistic markers of influence in informal interactions ; non-lexical_features encode political affiliation on twitter ; and modelling participation in small group social sequences with markov rewards analysis .
title : linguistic feature-based praise or complaint classification from customer_reviews ; abstract : online_reviews are very important in the customer ’ s decision-making process in selecting the appropriate products in the online_shopping portal . these reviews are then analyzed by business organizations to understand customer sentiment w.r.t . product/service . traditional sentiment_analysis techniques identify only positive , negative or neutral sentiment w.r.t . reviews and does not consider informativeness of reviews while analyzing sentiment . the extreme opinions like praise and complaint sentences are considered as a subset of positive and negative sentences and becomes difficult to find . praise sentences are more descriptive in nature . praises contain more nouns , adjectives , intensifiers as compared to plain positive sentences and complaint sentences contain more connectives and adverbs rather than the plain negative sentences . this paper proposes a linguistic feature-based approach for review sentences filtering and hybrid feature_selection method for classifying review sentence as praise or complaint . these praise and complaint sentences can be further analyzed by business organizations to identify the reasons for customer_satisfaction or dissatisfaction . it can also be used for creating automatic product description from online_reviews in terms of pro and con of the product/service . the performance of the four different supervised_machine_learning classifiers , namely random_forest , svc , kneighbors , mlp with hybrid feature_selection method is evaluated on three domains reviews using the parameters accuracy , precision , recall , and f1-score . the proposed method showed excellent_results as compared to the state of art classifiers .
title : context-aware misinformation_detection : a benchmark of deep_learning architectures using word_embeddings ; abstract : new mass_media paradigms for information distribution have emerged with the digital age . with new digital-enabled mass_media , the communication process is centered around the user , while multimedia content is the new identity of news . thus , the media landscape has shifted from mass_media to personalized social_media . while this progress brings advantages , it also carries the risk of being detrimental to society through the emergence of misinformation ( false or inaccurate information ) and disinformation ( intentionally spreading misinformation ) in the form of fake_news . fake_news is a tool used to manipulate public opinion on particular topics , distort public perceptions , and generate social unrest while lacking the rigor of traditional journalism . driven by this current and real-world problem , in this paper , we train multiple deep_learning architectures for multi-class classification and compare their performance in detecting the veracity of the news articles . to achieve accurate models in detecting misinformation , we employ a large dataset containing 100 000 news articles labeled with ten classes ( one with real_news and the rest with different types of fake_news ) . we use two preprocessing_techniques , i.e. , one simple and another very aggressive , to clean the dataset . we also employ three word_embeddings that preserve the word context , i.e. , word2vec , fasttext , and glove , pre-trained and trained on our dataset to vectorize the preprocessed dataset . for the misinformation task , we train a logistic_regression as a baseline and compare its results with the performance of ten deep_learning architectures . we obtain the best results using a recurrent_convolutional_neural_network based architecture . the experimental results show that the models are highly dependable on text_preprocessing and the word_embedding employed .
title : computational_linguistics literature and citations oriented citation linkage , classification and summarization ; abstract : scientific_literature is currently the most important resource for scholars , and their citations have provided researchers with a powerful latent way to analyze scientific trends , influences and relationships of works and authors . this paper is focused on automatic citation_analysis and summarization for the scientific_literature of computational_linguistics , which are also the shared_tasks in the 2016 workshop of the 2nd computational_linguistics scientific document_summarization at birndl 2016 ( the joint workshop on bibliometric-enhanced information_retrieval and natural_language_processing for digital_libraries ) . each citation linkage between a citation and the spans of text in the reference_paper is recognized according to their content similarities via various computational methods . then the cited text span is classified to five pre-defined facets , i.e. , hypothesis , implication , aim , results and method , based on various features of lexicons and rules via support_vector_machine and voting_method . finally , a summary of the reference_paper from the cited text_spans is generated within 250 words . hlda ( hierarchical latent_dirichlet_allocation ) topic_model is adopted for content modeling , which provides knowledge about sentence clustering ( subtopic ) and word_distributions ( abstractiveness ) for summarization . we combine hlda knowledge with several other classical features using different weights and proportions to evaluate the sentences in the reference_paper . our systems have been ranked top one and top two according to the evaluation results published by birndl 2016 , which has verified the effectiveness of our methods .
title : lexicon-pointed hybrid n-gram features extraction model ( lenfem ) for sentence_level sentiment_analysis ; abstract : sentiment_analysis of social_media textual posts can provide information and knowledge that is applicable in social settings , business_intelligence , evaluation of citizens ' opinions in governance , and in mood triggered devices in the internet of things . feature_extraction and selection is a key determinant of accuracy and computational_cost of machine_learning models for such analysis . most feature_extraction and selection techniques utilize bag of words , n-grams , and frequency-based algorithms especially term_frequency-inverse_document_frequency . however , these approaches do not consider relationships between words , they ignore words ' characteristics and they suffer high feature dimensionality . in this paper we propose and evaluate a feature_extraction and selection approach that utilizes a fixed hybrid n-gram window for feature_extraction and minimum_redundancy maximum relevance feature_selection algorithm for sentence_level sentiment_analysis . the approach improves the existing features extraction techniques , specifically the n-gram by generating a hybrid vector from words , part of speech ( pos ) tags , and word semantic orientation . the vector is extracted by using a static trigram window identified by a lexicon where a sentiment word appears in a sentence . a blend of the words , pos_tags , and the sentiment_orientations of the static trigram are used to build the feature_vector . the optimal features from the vector are then selected using minimum_redundancy maximum relevance ( mrmr ) algorithm . experiments were carried out using the public yelp dataset to compare the performance of the proposed model and existing feature_extraction models ( bow , normal n-grams and lexicon-based bag of words semantic orientations ) . using supervised_machine_learning classifiers the experimental results showed that the proposed model had the highest f-measure ( 88.64 % ) compared to the highest ( 83.55 % ) from baseline approaches . wilcoxon test carried out ascertained that the proposed approach performed significantly better than the baseline approaches . comparative performance analysis with other datasets further affirmed that the proposed approach is generalizable .
title : improved and efficient text adversarial_attacks using target information ; abstract : there has been recently a growing interest in studying adversarial_examples on natural_language models in the black-box setting . these methods attack natural_language classifiers by perturbing certain important words until the classifier label is changed . in order to find these important words , these methods rank all words by importance by querying the target model word by word for each input sentence , resulting in high query inefficiency . a new interesting approach was introduced that addresses this problem through interpretable learning to learn the word ranking instead of previous expensive search . the main advantage of using this approach is that it achieves comparable attack rates to the state-of-the-art methods , yet faster and with fewer queries , where fewer queries are desirable to avoid suspicion towards the attacking agent . nonetheless , this approach sacrificed the useful information that could be leveraged from the target classifier for that sake of query efficiency . in this paper we study the effect of leveraging the target model outputs and data on both attack rates and average number of queries , and we show that both can be improved , with a limited overhead of additional queries .
title : generating tailored classification schemas for german patents ; abstract : patents and patent applications are important parts of a company ’ s intellectual_property . thus , companies put a lot of effort in designing and maintaining an internal structure for organizing their own patent portfolios , but also in keeping track of competitor ’ s patent portfolios . yet , official classification schemas offered by patent offices ( i ) are often too coarse and ( ii ) are not mappable , for instance , to a company ’ s functions , applications , or divisions . in this work , we present a first step towards generating tailored classification . to automate the generation process , we apply key term_extraction and topic_modelling algorithms to 2.131 publications of german patent applications . to infer categories , we apply topic_modelling to the patent collection . we evaluate the mapping of the topics found via the latent_dirichlet_allocation method to the classes present in the patent collection as assigned by the domain expert .
title : progressively improving supervised emotion classification through active_learning ; abstract : recognizing human emotions from pieces of natural_language has been a challenging task in artificial_intelligence , for the difficulty of acquiring high-quality training_examples . in this paper , we propose a novel method based on active_learning to progressively improve the performance of supervised text emotion classification models , with as few human labor as possible in annotating the training_examples . specifically , the active_learning algorithm interactively communicates with the supervised emotion classification model to find the potentially most effective training_examples from a huge set of unlabeled_data and increases the training_data by acquiring emotion labels for these examples from the human experts . our experiment of multi-label_emotion_classification on japanese tweets suggests that the proposed method is effective in steadily improving the supervised classification results by incrementally feeding a classification model with the new tweets of well-balanced emotion labels .
title : a recommender system for open educational_videos based on skill requirements ; abstract : in this paper , we suggest a novel method to help learners find relevant open educational_videos to master skills demanded on the labour market . we have built a prototype , which 1 ) applies text_classification and text_mining methods on job vacancy announcements to match jobs and their required skills ; 2 ) predicts the quality of videos ; and 3 ) creates an open educational video recommender system to suggest personalized learning content to learners . for the first evaluation of this prototype we focused on the area of data_science related jobs . our prototype was evaluated by in-depth , semi-structured interviews . 15 subject_matter experts provided feedback to assess how our recommender prototype performs in terms of its objectives , logic , and contribution to learning . more than 250 videos were recommended , and 82.8 % of these recommendations were treated as useful by the interviewees . moreover , interviews revealed that our personalized video recommender system , has the potential to improve the learning experience .
title : artificial_intelligence to organize patient_portal messages : a journey from an ensemble deep_learning text_classification to rule-based named_entity_recognition ; abstract : since the turn of the millennium , numerous healthcare venues all over the world have made a standard of communication transition from the classic telephone call to a sophisticated online patient_portal system . more and more , a majority of patients prefer using portal-style communication for clinician contact , checking lab results , and other informational transactions , in which hundreds of thousands of patient_portal messages ( ppms ) are daily generated as free-text data with multiple requests often buried in one single message . thus , there is a pressing need to design and implement artificial_intelligence ( ai ) algorithms to accurately organize this wealth of data in a timely fashion . with the present contribution , an attempt was made to first develop an ensemble deep_learning text_classification component and then integrate it with rule-based named_entity_recognition to categorize free-text ppms submitted under the 'non-urgent medical question ' subject in the patient_portal as either containing active symptom descriptions or logistical requests ( e.g. , appointment rescheduling ) .
title : rls-mars to feature_selection for text_classification ; abstract : feature_selection and feature_extraction will help to remove noisy features and reduce the dimensionality of data sets . in this paper a new rls-mars ( regularized least squares-multi angle regression and shrinkage ) feature_selection method for text_classification is proposed , while rls with lars can be viewed as an efficient approach for a modified rls formulation which uses both l2 regularizer and l1 regularizer . the present method is to find a series of direction in multidimensional space , leading the gradient vectors to change along those directions which would make the gradient matrix 's gradient descent during the procedure , and the feature in this direction can be easily selected . the experiments on reuters-21578 demonstrate the effectiveness of the new feature_selection method for text_classification in several classical algorithms : knn and svmlight .
title : personality_disorders identification in written_texts ; abstract : this article describes a method for dealing with the detection of possible personality_disorder without the necessary presence of specialists and using the patient ’ s self-essays . written text is analyzed by using nlp techniques and is categorized into one of the three main groups of personality_disorders we chosen— fear , procrastination and intolerance of uncertainty . we customized approach based on features extraction , sentiment_analysis and classification by well-known classifiers : naive_bayes , multi-class support_vector_machine , k-nearest_neighbors and decision_tree . the first experiential , based on real data consulted with specialists , have shown promising_results . greater or lesser personality_disorders are due to a stressful and time-titch way of life quite frequent nowadays . in the cases of restrictions or complications in the suffering individual ’ s life is early identification and problem_solving more than desirable . but some people consider visiting a specialist as a personal failure a and due a shame they do not solve the problem even if it suspects themselves . psychologists and psychiatrists on the other hand use several methods to detect personality_disorders today , either by observation during the interview , a questionnaire and a written self-essay .
title : notice of retraction : semi-supervised short_text categorization based on random_subspace ; abstract : in order to solve the `` labeling bottleneck '' problem of short_text categorization , a novel semi-supervised expectation-maximization short_text categorization method based on random_subspace ( rs-em ) is used in this paper . rsem performs an iterative em style training where multiple models are trained on subsets by using random_subspace method . this combination of the stochastic discrimination theory and semi-supervised em_algorithm is used to compensate for the weaknesses of the standard em_algorithm to over-training . experimental on real corpus show that the proposed method is more effectively exploit unlabeled_data to enhance the learning performance , and is superior to standard semi-supervised em_algorithm in the learning efficiency and the classification generalization . © 2010 ieee .
title : senti-cs : building a lexical_resource for sentiment_analysis using subjective feature_selection and normalized chi-square-based feature_weight generation ; abstract : sentiment_analysis involves the detection of sentiment content of text using natural_language_processing . natural_language_processing is a very challenging task due to syntactic ambiguities , named_entity_recognition , use of slangs , jargons , sarcasm , abbreviations and contextual sensitivity . sentiment_analysis can be performed using supervised as well as unsupervised approaches . as the amount of data grows , unsupervised approaches become vital as they cut down on the learning time and the requirements for availability of a labelled_dataset . sentiment_lexicons provide an easy application of unsupervised algorithms for text_classification . sentiwordnet is a lexical_resource widely employed by many researchers for sentiment_analysis and polarity_classification . however , the reported performance levels need improvement . the proposed research is focused on raising the performance of sentiwordnet3.0 by using it as a labelled corpus to build another sentiment_lexicon , named senti-cs . the part of speech information , usage based ranks and sentiment_scores are used to calculate chi-square-based feature_weight for each unique subjective term/part-of-speech pair extracted from sentiwordnet3.0 . this weight is then normalized in a range of −1 to +1 using min–max normalization . senti-cs based sentiment_analysis framework is presented and applied on a large dataset of 50000 movie reviews . these results are then compared with baseline sentiwordnet , mutual_information and information gain techniques . state of the art comparison is performed for the cornell movie review dataset . the analyses of results indicate that the proposed approach_outperforms state-of-the-art classifiers .
title : evaluating efficiency and effectiveness of federated_learning approaches in knowledge extraction tasks ; abstract : federated_learning is a valuable instrument for building ai-based systems that preserve the privacy and security of sensitive data , based on the main concept of shifting no more the data to the edges but moving computations to data , avoiding the collection , sharing , and use of such data by third parties . more robust federated_learning systems should be able of preventing malicious inference over both data exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive_accuracy . this study proposes a preliminary analysis to investigate and evaluate the effectiveness and efficiency of a federated approach to ensure valid classification_accuracy and data_security . a real case_study from the androids project , concerning the application of machine_learning-based systems for supporting mental-health disorders detection , was considered . large amounts of sensitive patient information are collected , which must be obfuscated or anonymized to provide a preliminary level of protection . unfortunately , the real bottleneck lies in the difficulty of extracting all sensitive data for anonymization , due to a lot of data to handle as well as the considerable effort required . we propose a natural_language_processing approach for sensitive knowledge detection and classification , performed by adopting a federated approach . accuracy decay and latency introduced by applying a decentralized learning approach compared to the same task and data performed in a centralized way were evaluated . preliminary_results proved that effectiveness can be reached by a correct tuning of the federated algorithm and by choosing the right number of participants to the federation .
title : applications of logistic_regression and naive_bayes in commodity sentiment_analysis ; abstract : sentiment_analysis is popular research which helps people perceive the trend of public opinion commented in separate social_networking platforms . the aim of the paper is to investigate the real evaluation of goods when marketing tweets are eliminated and estimate the performance of two classifiers in sentiment_analysis . the initial process is data collection used crawler with user_comments on the series of ĝ€ ? for verification > xiaomi 11 ' in weibo . by collected data , pre-processing and features extracting can be instituted sequentially . we use term_frequency-inverse_document_frequency to evaluate the significant of words then filter out common words , and bag of word is used to convert the texts into numerical_features . finally , we use naive_bayes and logistic_regression for the classification of twitters reviews and get the results that logistic_regression classifier preforms better than naive_bayes_classifier . the analysis with former gives 3.572 % more accuracy , 6.632 % more precision and 2.81 % more recall .
title : optimal partitioning for classification and regression trees ; abstract : in designing a decision_tree for classification or regression , one selects at each node a feature to be tested , and partitions the range of that feature into a small number of bins , each bin corresponding to a child of the node . when the feature 's range is discrete with n unordered outcomes , the optimal partition , that is , the partition minimizing an expected loss , is usually found by an exhaustive search through all possible partitions . since the number of possible partitions grows exponentially in n , this approach is impractical when n is larger than about 10 or 20 . in this paper , we present an iterative algorithm that finds a locally optimal partition for an arbitrary loss_function , in time linear in n for each iteration . the algorithm is a k-means like clustering_algorithm that uses as its distance_measure a generalization of kullback 's information divergence . moreover , we prove that the globally optimal partition must satisfy a nearest_neighbor condition using divergence as the distance_measure . these results generalize similar results of breiman et al . to an arbitrary number of classes or regression variables and to an arbitrary number of bins . we also provide experimental results on a text-to-speech example , and we suggest additional applications of the algorithm , including the design of variable combinations , surrogate splits , composite nodes , and decision graphs . © 1991 ieee
title : a novel heuristic text_classification algorithm based on support_vector_machines ; abstract : support_vector_machines ( svm ) , one of the new techniques for text_classification , have been widely used in many application_areas . svm try to find an optimal hyperplane within the input_space so as to correctly classify the binary classification problem . we present a novel heuristic text_classification approach based on genetic_algorithm ( ga ) and svm . simulation results demonstrate that ga and svm are integrated effectively , and have good classification_accuracy . ©2010 ieee .
title : a feature based opinion_mining for product_reviews using naive_bayes and k-nearest_neighbor classifiers ; abstract : the explosive_growth in the technology of web like social_media , directs more number of people to express their sentiments , feedbacks or opinions towards the service , events , individuals , topics , products or social_issues . since the data get bigger extensively in everyday_life , this makes so difficult for customers , manufactures as well as for end_users of online websites to derive a conclusion on the accessibility of big_data available as reviews . opinion_mining is referred to as a natural_language_processing technique that gives out the knowledge extraction of viewpoints or attitudes from the review texts . feature-based opinion_mining system aims to find the main aspects or features of a specified entity and the sentiment expressed on that entity by using natural_language_processing and ai techniques . most of the studies have been conducted on aspect_based_opinion_mining but not any of the particular works have justified to be adequate for assessing the critical or majorfactors . the major factors with respect to aspect_based_opinion_mining are implicit or implied aspects , explicit or direct aspects and multiple aspect based . the aspect_based_opinion_mining by considering all these critical factors helps in analyzing the aspect of a particular entity and its sentiment more accurately . in this paper , an explicit aspect_based_opinion_mining is carried out using naive_bayes and k-nearest neighborclassifiers to generate more accurate opinions for product_reviews . the end_user can easily get opinions based on the particular aspect of the entity and the proposed work proves that the sentiment_classification using naive_bayes_classifier provide with more accuracy than using k-nearest_neighbor classifier .
title : sentiment_analysis of mobile phone products reviews using classification_algorithms ; abstract : sentiment_analysis or opinion_mining is a process of analyzing opinions and emotions to infer the tendencies and impressions shown on the analyzed data and classify them as positive or negative . sentiment_analysis is extremely important because it helps companies and institutions to measure the scope of their customer 's satisfaction with a specific product based on reviews in a very fast way . as the manual analysis of these reviews is very difficult because of the increase in the numbers of reviews on products day after day . this paper proposes a sentiment_analysis model to classify product_reviews as positive , neutral and negative . it applies five popular machine_learning classifiers namely : naive_bayes , support_vector_machine , decision_tree , k-nearest_neighbor and maximum_entropy with the aim to come up with the most efficient classifier . the dataset used consists of 82 , 815 reviews about mobile phone products , collected from kaggle website . in order to evaluate the five classifiers , we used recall , precision , f1-mesaure and accuracy to measure the performance of each algorithm . the results show that maximum_entropy and naïve_bayes outperforms the other classifiers in term of accuracy in all experiments . decision_tree algorithm gave the lowest results across all experiments in terms of accuracy .
title : aspect-based sentiment_analysis on mobile application reviews ; abstract : with the popularity of smartphones , mobile application ( a.k.a mobile app ) development has become a booming industry all across the world . one of the main hurdles that app developers are facing , is understanding users ' needs and catering their products to satisfy the users . though users are one of the main stakeholders of the app development process it is harder to incorporate them into the requirement elicitation process . numerous studies have shown that incorporating user_reviews in the requirement elicitation process paves the way to a better understanding of user needs which , in turn , helps developers develop better apps that satisfy the targeted audience of the app . in this paper , we introduce a cnn-based approach to analyze user_reviews using aspect-based sentiment_analysis ( absa ) . the results show that our approach could achieve 87.88 % , 93.75 % , and 31.25 % improvements in aspect category classification and 16.43 % , 23.35 % , and 3.72 % improvements in aspect_sentiment_classification over the baseline results for awer dataset in productivity , social_networking , and game domains respectively .
title : 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection ; incorporating syntactic information into relation representations for enhanced relation_extraction .
title : an adaptive , semistructured language model approach to spam_filtering on a new corpus ; abstract : motivated by current efforts to construct more realistic spam_filtering experimental corpora , we present a newly assembled , publicly available corpus of genuine and unsolicited ( spam ) email , dubbed genspam . we also propose an adaptive model for semi-structured document_classification based on language model component interpolation . we compare this with a number of alternative classification models , and report promising_results on the spam_filtering task using a specifically assembled test set to be released as part of the genspam corpus .
title : baksa at semeval-2020 task 9 : bolstering cnn with self-attention for sentiment_analysis of code_mixed text ; abstract : sentiment_analysis of code-mixed text has diversified applications in opinion_mining ranging from tagging user_reviews to identifying social or political sentiments of a sub-population . in this paper , we present an ensemble architecture of convolutional neural_net ( cnn ) and self-attention_based lstm for sentiment_analysis of code-mixed tweets . while the cnn component helps in the classification of positive and negative tweets , the self-attention_based lstm , helps in the classification of neutral_tweets , because of its ability to identify correct sentiment among multiple sentiment_bearing units . we achieved f1 scores of 0.707 ( ranked 5th ) and 0.725 ( ranked 13th ) on hindi-english ( hinglish ) and spanish-english ( spanglish ) datasets , respectively . the submissions for hinglish and spanglish tasks were made under the usernames ayushk and harsh_6 respectively .
title : a simple and efficient ensemble_classifier combining multiple neural_network models on social_media datasets in vietnamese ; abstract : text_classification is a popular topic of natural_language_processing , which has currently attracted numerous research efforts worldwide . the significant increase of data in social_media requires the vast attention of researchers to analyze such data . there are various studies in this field in many languages but limited to the vietnamese_language . therefore , this study aims to classify vietnamese texts on social_media from three different vietnamese benchmark_datasets . advanced deep_learning models are used and optimized in this study , including cnn , lstm , and their variants . we also implement the bert , which has never been applied to the datasets . our experiments find a suitable model for classification_tasks on each specific dataset . to take advantage of single models , we propose an ensemble model , combining the highest-performance models . our single models reach positive results on each dataset . moreover , our ensemble model achieves the best performance on all three datasets . we reach 86.96 % of f1- score for the hsd-vlsp dataset , 65.79 % of f1-score for the uit-vsmec dataset , 92.79 % and 89.70 % for sentiments and topics on the uit-vsfc dataset , respectively . therefore , our models achieve better performances as compared to previous_studies on these datasets .
title : domain_independent sentiment_classification with many lexicons ; abstract : sentiment_lexicons are language resources widely used in opinion_mining and important tools in unsupervised sentiment_classification . we present a comparative study of sentiment_classification of reviews on six different domains using sentiment_lexicons from different sources . our results highlight the tendency of a lexicon 's performance to be imbalanced towards one class , and indicate lexicon accuracy varies with the target domain . we propose an approach that combines information from different lexicons to make classification decisions and achieve more robust results that consistently improve our baseline across all domains tested . these are further refined by a domain_independent score adjustment that mitigates the effect of the precision imbalance seen on some of the results . © 2011 ieee .
title : a survey on dimension reduction techniques in text_classification ; abstract : dimension reduction is one of the key_points for text_classification . feature_selection and feature_extraction are the two common methods of dimension reduction . in this paper , we mainly discussed some dimension reduction techniques from two aspects including traditional_methods ( information gain , mutual_information , document_frequency , correlation_coefficient ) and new methods ( optimization mutual_information based on word_frequency , cdf ( concentration , dispersion and frequency ) , semantic relatedness ) . then analyzed the principle of these methods and illustrated their advantages as well as disadvantages .
title : classification and optimization scheme for text data using machine_learning naïve_bayes classifier ; abstract : text_classification is an essential advance in characteristic dialect processing . it very well may be performed utilizing different classification_algorithms . hadoop map_reduce is widely_utilized in text_classification to perform classification on colossal measure of text data . however , map_reduce required a ton of time to perform the tasks thereby increasing latency and since the data is distributed over the cluster it builds time and thus reducing processing speed . also hadoop utilizes long queue of code . motivated by this , we propose a basic yet compelling machine_learning method which uses naïve_bayes classifier for text data . in machine_learning approach , the classifier is built automatically by learning the properties of categories from a set of pre-defined training_data . hence , it can process complex furthermore , multi assortment information in dynamic situations . here we propose a naïve_bayes classifier which scales directly with number of indicators and data points which can be used for both binary and multiclass_classification problems . we implemented the presented schemes using machine_learning tool . the experimental results demonstrate the performance_improvement in the classification technique .
title : native language identification and writing proficiency ; abstract : this study evaluates the impact of writing proficiency on native language identification ( nli ) , a topic that has important implications for the generalizability of nli models and detection-based arguments for cross-linguistic influence ( jarvis 2010 , 2012 ; cli ) . the study uses multinomial_logistic_regression to classify the first language ( l1 ) group membership of essays at two proficiency levels based on systematic lexical and phrasal choices made by members of five l1 groups . the results indicate that lower proficiency essays are significantly easier to classify than higher proficiency essays , suggesting that lower proficiency writers make lexical and phrasal choices that are more similar to other lower proficiency writers that share an l1 than higher proficiency writers that share an l1 . a close analysis of the findings also indicates that the relationship between nli accuracy and proficiency differed across l1 groups .
title : machine reading of arabic manuscripts using knn and svm classifiers ; abstract : in this paper , feature_extraction techniques like histogram of oriented gradients ( hog ) and local_binary_pattern ( lbp ) have been applied for feature_extraction based on the structure of the arabic manuscript texts . further , the two most popular classifiers namely , k-nearest_neighbor ( knn ) and support_vector_machine ( svm ) have been used to classify these texts and the results of both the classifiers have been compared . for comparison , we have used 1155 images of arabic words and 1100 kufic words for the purpose of training and testing and then the results of both the classifiers have been compared . in this work , we have used the arabic ahdb dataset and kufic dataset for experimentation and for selecting , training and testing the data ; the partition approach has been used . it is found that svm performs better than knn and achieved maximum recognition_accuracy of 97.05 % ( with kufic dataset ) and 97.80 % ( with ahdb dataset ) .
title : hybrid approach for telugu handwritten_character_recognition using k-nn and svm classifiers ; abstract : research in optical_character_recognition ( ocr ) had started more than five decades ago . the recognition_accuracy for printed_characters is above 90 % , whereas for handwritten_characters is very low and less than 60 % as reported in the literature . handwritten_character_recognition of indian languages is still at nascent stage of research and hence captivated our attention for further analysis . this paper describes the handwritten_character_recognition of telugu_language using two-stage classifiers . k-nearest_neighbor ( k-nn ) and support_vector_machines ( svm ) were used for classification in this work . the use of these two classifiers one after the other in two subsequent stages increases the recognition_accuracy of the system . various features extracted from the images are block pixel count , block based directions , histograms and boundaries for both the training and test images . in the first stage k-nn classifier was used and subsequently the wrongly recognized characters were tested with svm classifier . again the classifiers were interchanged in the first and second stages to check the improvement of accuracy . it was found that the recognition_accuracy increased to a great extent by cascading the two different classifiers . using these two classifiers the best recognition_accuracy obtained was 90.2 % .
title : optimization driven actor-critic neural_network for sentiment_analysis in social_media ; abstract : purpose : sentiment_analysis is the subfield of data_mining , which is profusely used for studying the opinions of the users by analyzing their suggestions on the web platform . it plays an important role in the daily decision-making process , and every decision has a great impact on daily life . various techniques including machine_learning algorithms have been proposed for sentiment_analysis , but still , they are inefficient for extracting the sentiment features from the given text . although the improvement in sentiment_analysis approaches , there are several problems , which make the analysis inefficient and inaccurate . this paper aims to develop the sentiment_analysis scheme on movie reviews by proposing a novel classifier . design/methodology/approach : for the analysis , the movie reviews are collected and subjected to pre-processing . from the pre-processed review , a total of nine sentiment related features are extracted and provided to the proposed exponential-salp swarm algorithm based actor-critic neural_network ( essa-acnn ) classifier for the sentiment_classification . the essa algorithm is developed by integrating the exponentially weighted moving_average ( ewma ) and ssa for selecting the optimal weight of acnn . finally , the proposed classifier classifies the reviews into positive or negative class . findings : the performance of the essa-acnn classifier is analyzed by considering the reviews present in the movie review database . from , the simulation results , it is evident that the proposed essa-acnn classifier has improved performance than the existing_works by having the performance of 0.7417 , 0.8807 and 0.8119 , for sensitivity , specificity and accuracy , respectively . originality/value : the proposed classifier can be applicable for real-world problems , such as business , political activities and so on .
title : sentiment-aware automatic_speech_recognition pre-training for enhanced speech_emotion_recognition ; abstract : we propose a novel multi-task pre-training method for speech_emotion_recognition ( ser ) . we pre-train ser model simultaneously on automatic_speech_recognition ( asr ) and sentiment_classification tasks to make the acoustic asr model more “ emotion aware ” . we generate targets for the sentiment_classification using text-to-sentiment model trained on publicly available data . finally , we fine-tune the acoustic asr on emotion annotated speech data . we evaluated the proposed approach on msp-podcast dataset , where we achieved the best reported concordance correlation_coefficient ( ccc ) of 0.41 for valence prediction .
title : classification of legal articles based on bio ethics related to machine_learning ; abstract : in order to solve the problem of long execution time of traditional legal text_classification , this paper designs a legal text_classification method based on machine_learning . based on the combination of support_vector_machine ( svm ) and naive_bayes_classifier in machine_learning , the feature value of legal text is filtered , the legal text_classification matrix is established , and the legal text_classification results are accessed directly through embedded system to realize the legal text_classification . the experimental results show that the implementation time of the design method is more than twice as fast as that of the control group , which can solve the problem of long implementation time of the traditional method . ai and machine_learning have the potential to transform the delivery of healthcare . however , creating decision_support_systems based on machine_learning requires more than just a technological undertaking . as a result , bioethical standards must be considered . as ai and machine_learning progress , bioethical frameworks must be adapted to solve the difficulties that these growing systems may bring , and the creation of these automated systems also has to be tailored to embrace bioethical concepts .
title : arabic text_classification using master-slaves technique ; abstract : text_classification ( tc ) is an essential field in both text_mining ( tm ) and natural_language_processing ( nlp ) . humans have a tendency to organize and categorize everything as they want to make things easier to understand . therefore , text_classification is an important step to achieve this goal . arabic text_classification ( atc ) is a difficult process because the arabic_language has complications and limitations resulting from the nature of its morphology . in this paper , a proposed approach called the master-slaves technique ( mst ) was used to improve arabic text_classification . it consists of two main phases : in the first phase , a new arabic corpus of 16757 text files was collected . these text files were classified into five categories manually . in the second phase , four different classifiers were implemented on the collected corpus . these classifiers are naïve_bayes ( nb ) , k-nearest_neighbour ( knn ) , multinomial_logistic_regression ( mlr ) and maximum weight ( mw ) . naïve_bayes classifier was implemented as master and the others as slaves . the results of these slave classifiers were used to change the probability of the naïve_bayes classifier ( master ) . the four classifiers used were implemented individually and the simple voting technique was implemented among them too on the collected corpus to check the effectiveness and efficiency of the proposed technique . all the tests were applied after the pre-processing of arabic text documents ( tokenization , stemming , and stop-word_removal ) and each document was represented as vector of weights . for the reliability of the results , 10-fold_cross-validation was used in this paper . the results showed that the master-slaves technique gives a good improvement in accuracy of text document_classification with accepted algorithm complexity compared to other techniques .
title : sentiment_analysis and emotion recognition : evolving the paradigm of communication within data classification ; abstract : the process of sentiment_analysis and emotion recognition ( saer ) entails using artifcial intelligence components and algorithms to extract emotions and sentiments from online texts , such as tweets . the information extracted can then be used by marketing , customer_support and public_relations teams to foster positive consumer attitudes . advances in this discipline , however , are being hindered by two signifcant obstacles . first , although ‘ emotion ’ and ‘ sentiment ’ are distinct entities that require distinct analysis , there is no agreed defnition to distinguish between the two . secondly , the nature of language within the electronic medium has evolved to include much more than textual statements , including ( but not limited to ) acronyms , emojis and other visuals , such as video ( in its many forms ) . as visual_communication lacks universal interpretation , this can lead to erroneous analysis and conclusions , even where there is a differentiation between emotion and sentiment . this paper uses examples and case_studies to explain the theoretical basis of the problem . it also offers conceptual direction regarding how to make saer more accurate .
title : exploring data augmentation for classification of climate_change_denial : preliminary study ; abstract : in order to address the growing need of monitoring climate-change_denial narratives in online_sources , nlp-based methods have the potential to automate this process . here , we report on preliminary_experiments of exploiting data_augmentation techniques for improving climate_change_denial classification . we focus on a selection of both known techniques , and augmentation transformations not reported elsewhere that replace certain type of named_entities with high probability of preserving labels . we also introduce a new benchmark_dataset consisting of text_snippets extracted from online_news labeled with fine-grained climate_change_denial types .
title : news article classification based on a vector representation including words ’ collocations ; abstract : in this paper we present a proposal including collocations into the preprocessing of the text_mining , which we use for the fast news article recommendation and experiments based on real data from the biggest slovak newspaper . the news article section can be predicted based on several article ’ s characteristics as article name , content , keywords etc . we provided experiments aimed at comparison of several approaches and algorithms including expressive vector representation , with considering most popular words collocations obtained from slovak national corpus .
title : fast and precise prediction of non-coding rnas ( ncrnas ) using sequence_alignment and k-mer counting ; abstract : non-coding ribonucleic acids ( ncrnas ) are functional rna molecules that transcribe from dna , but can not be translated into proteins . these ncrnas function to control the gene expressions at both the transcriptional level and post-transcriptional level , for many diseases and biological activities . they are also identified as regulators of various risk factors and cell functions , and thus improve diagnosis , prognosis and therapeutic assessments . the classification and prediction of ncrna classes is the recent diagnostic advancement that provides information about complementary bases with target messenger rnas . in this paper , a novel method , “ ncrnlp ” has been proposed for the classification of different ncrna families . to facilitate the classification , a natural_language_processing ( nlp ) model has been proposed , that applies k-mers and bag-of-words approaches to ncrna sequences . the proposed model has been assessed on the standard dataset ‘ rfam ’ , and it outperforms existing classification methods available in the literature . the paper also focuses on the classification of three major classes of short ncrnas , which are , piwi-interacting rnas ( pirnas ) , short interfering rnas ( sirnas ) and micrornas ( mirnas ) as an application of ncrnlp . these three categories are expressed divergently and associated with various types of cancer such as lung_cancer , breast_cancer , gastric_cancer and hepatocellular_carcinoma .
title : document_classification utilising ontologies and relations between documents ; abstract : two major types of relational information can be utilized in automatic document_classification as background information : relations between terms , such as ontologies , and relations between documents , such as web links or citations in articles . we introduce a model where a traditional bag-of-words type classifier is gradually extended to utilize both of these information types . the experiments with data from the finnish national_archive show that classification_accuracy improves from 70 % to 74 % when the general finnish ontology yso is used as background information , without using relations between documents . © 2010 acm .
title : deep_learning solutions based on fixed contextualized_embeddings from pubmedbert on bioasq 10b and traditional ir in synergy ; abstract : this paper presents the participation of the university of aveiro biomedical informatics and technologies group ( bit ) in the tenth edition of the bioasq challenge for document_retrieval ( task b phase a and synergy ) and ‘ yes or no ’ answering ( task b phase b ) . for the synergy task , we adopted a relevance_feedback approach that leveraged the traditional bm25 retrieval model combined with query_expansion based on the positive documents . regarding task b phase a , we adopted a two-stage retrieval pipeline that consists of the traditional bm25 retrieval with pseudo-relevance_feedback followed by a neural retrieval model . for the neural_models , we experimented with the publicly available transformer-upwm already trained on last year ’ s bioasq data and with a local implementation of the parade-cnn model . lastly , for the ‘ yes or no ’ questions , we trained a binary classifier over the pretrained pubmedbert model , and also studied the impact of data augmentation and dataset balancing_techniques . in terms of results , our synergy and task b phase b systems underperformed , scoring below the average . our best results came from the task b phase a systems that achieved above-average results , being in the top three in terms of teams . furthermore , after the challenge , we also conducted additional experiments to evaluate the impact of the transformer-upwm when trained on 10b data . this trial produced an improvement of 1 to 5 map percentage_points in all official evaluation batches . code to reproduce our submissions are available on https : //github.com/bioinformatics-ua/bioasq-10.git .
title : classification of chinese texts based on recognition of semantic topics ; abstract : for machine_learning methods , processing and understanding chinese texts are difficult , for that the basic unit of chinese texts is not character but phrases , and there is no natural delimiter in chinese texts to separate the phrases . the processing of a large number of chinese web texts is more difficult , because such texts are often less topic focused , short , irregular , sparse , and lacking in context . it poses a challenge for mining , clustering , and classification of chinese web texts . typically , the recognition_accuracy of the real meaning of such texts is low . in this paper , we propose a method that recognizes stable and abstract semantic topics that express the highly hierarchical_relationship behind the chinese texts from baidubaike . then , based on these semantic topics , a discrete distribution model is established to convert analysis to a convex_optimization problem by geometric programming . our experiments demonstrated that the proposed approach_outperforms many conventional machine_learning methods , such as knn , svm , wiki , crfs , and lda , regarding the recognition of mini training_data and short chinese web texts .
title : context-aware sarcasm detection using bert ; abstract : in this paper , we present the results obtained by bert , bilstm and svm classifiers on the shared_task on sarcasm detection held as part of the second workshop on figurative_language processing . the shared_task required the use of conversational_context to detect sarcasm . we experimented by varying the amount of context used along with the response ( response is the text to be classified ) . the amount of context used includes ( i ) zero context , ( ii ) last one , two or three utterances , and ( iii ) all utterances . it was found that including the last utterance in the dialogue along with the response improved the performance of the classifier for the twitter data set . on the other hand , the best performance for the reddit data set was obtained when using only the response without any contextual_information . the bert classifier obtained f-score of 0.743 and 0.658 for the twitter and reddit data set respectively .
title : comparison of bert and xlnet accuracy with classical methods and algorithms in text_classification ; abstract : the aim of this publication is to compare the accuracy of the bidirectional_encoder_representations_from_transformers ( bert ) and generalized autoregressive pretraining for language understanding ( xlnet ) models in text_classification with the accuracy of classical machine_learning methods and algorithms . analyzed : bidirectional_encoder_representations_from_transformers ( bert ) , generalized autoregressive pretraining for language understanding ( xlnet ) , bernoulli naive_bayes_classifier , gaussian_naive_bayes classifier , multinomial_naive_bayes classifier , support_vector_machines . the results show that when classifying 50,000 reviews in english , xlnet ranks with the highest_accuracy-96 % , which is nearly 8 % more than the best-performing classic classifier support_vector_machines .
title : are bert embeddings able to infer travel patterns from twitter efficiently using a unigram approach ? ; abstract : public opinion is nowadays a valuable data source for many sectors . in this study , we analysed the transportation sector using messages extracted from twitter . contrasting with the traditional surveying methods that are high-cost and inefficient used in transportation sector , social_media are popular sources of crowdsensing . this work used bert embeddings , an unsupervised pre-trained model released in 2018 , to classify travel-related terms using tweets collected from three distinct cities : new york , london , and melbourne . in order to understand if a simple model can have a good performance , we used unigrams . a list of 24 travel-related words was used to classify the messages . popular words are train , walk , car , station , street , and avenue . between 3 % to 5 % of all messages are classified as traffic-related , while along the typical working hours of the day the values is around 5-6 % . a high model performance was obtained , with precision and accuracy higher than 0.80 and 0.90 , respectively . the results are consistent for all the three cities assessed .
title : classification of semantic paraphasias : optimization of a word_embedding model ; abstract : in clinical assessment of people with aphasia , impairment in the ability to recall and produce words for objects ( anomia ) is assessed using a confrontation naming task , where a target stimulus is viewed and a corresponding label is spoken by the participant . vector_space word_embedding models have had inital results in assessing semantic similarity of target-production pairs in order to automate scoring of this task ; however , the resulting models are also highly dependent upon training parameters . to select an optimal family of models , we fit a beta regression model to the distribution of performance_metrics on a set of 2,880 grid_search models and evaluate the resultant first- and second-order effects to explore how parameterization affects model performance . comparing to simlex-999 , we show that clinical data can be used in an evaluation task with comparable optimal parameter_settings as standard nlp evaluation datasets .
title : a model for auto-tagging of research papers based on keyphrase_extraction methods ; abstract : tagging provides a convenient means to assign tokens of identification to research papers which facilitate recommendation , search and disposition process of research papers . this paper contributes a document centered approach for auto-tagging of research papers . the auto-tagging method mainly comprises of two processes : -classification and tag selection . the classification process involves automatic_keyword_extraction using rapid automatic_keyword_extraction ( rake ) algorithm which uses the keyword - score matrix . the generated top scored keywords are added to the train dataset dynamically , which can be used further . this add-on feature of the system is considered to be one of the main advantages of the system since adding new born phrases is time-consuming and error_prone . cosine_similarity is used for classifying the research paper into corresponding domain utilizing the extracted_keywords . tag selection concentrates on the selection of proper tags for the research paper . tagging facilitates better search facility and determines the dynamics of research areas in terms of number of publications in a domain by each author . the system generates reports for statistical_analysis of research papers in each domain and expertise of each faculty in the research community .
title : not all negatives are equal : label-aware contrastive_loss for fine-grained text_classification ; abstract : fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them . guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks . in this work , we analyse the contrastive fine-tuning of pre-trained_language_models on two fine-grained text_classification tasks , emotion classification and sentiment_analysis . we adaptively embed class relationships into a contrastive objective_function to help differently weigh the positives and negatives , and in particular , weighting closely confusable negatives more than less similar negative_examples . we find that label-aware contrastive_loss outperforms previous contrastive methods , in the presence of larger number and/or more confusable classes , and helps models to produce output distributions that are more differentiated .
title : effective printed tamil text segmentation and recognition using bayesian classifier ; abstract : text segmentation and recognition of indian languages have gained a lot of research interest in the recent_years . the existence of a huge number of symbols and varying characteristics in these languages makes segmentation and extraction of text a challenging task . the tamil_language has a wide variety of the literature , and printed_text is available in various forms such as newspaper , books , and magazines . in this paper , extraction of printed tamil text from an image is done irrespective of the characteristics of the text such as font style , color , and size . the proposed work uses scanned printed tamil text as the input image . this input image is binarized since text is always available in the foreground , and histograms can be used to segment them into lines and words . the morphological operator , dilation , is used to remove outliers such as dots and commas present in an underlying object and segment the printed_text into words to facilitate text detection . further , each character is identified using bounding box technique . classification of tamil letters is done by extracting features such as gradient information and curvature-based information obtained from grayscale and binary images . these features are trained , and characters are classified using bayesian classifier . the recognized characters are documented as text using unicode format . the performance of the approach is evaluated using precision , recall , and f-measure .
title : multi-domain dialogue success classifiers for policy training ; abstract : we propose a method for constructing dialogue success classifiers that are capable of making accurate predictions in domains unseen during training . pooling and adaptation are also investigated for constructing multi-domain models when data is available in the new domain . this is achieved by reformulating the features input to the recurrent_neural_network models introduced in [ 1 ] . importantly , on our task of main interest , this enables policy training in a new domain without the dialogue success classifier ( which forms the reinforcement_learning reward function ) ever having seen data from that domain before . this occurs whilst incurring only a small reduction in performance relative to developing and using an in-domain dialogue success classifier . finally , given the motivation with these dialogue success classifiers is to enable policy training with real users , we demonstrate that these initial policy training results obtained with a simulated user carry over to learning from paid human users .
title : automated lexicon and feature_construction using word_embedding and clustering for classification of asd diagnoses using ehr ; abstract : using electronic_health_records of children evaluated for autism_spectrum_disorders , we are developing a decision_support system for automated diagnostic criteria extraction and case classification . we manually_created 92 lexicons which we tested as features for classification and compared with features created automatically using word_embedding . the expert annotations used for manual lexicon creation provided seed terms that were expanded with the 15 most similar terms ( word2vec ) . the resulting 2,200 terms were clustered in 92 clusters parallel to the manually_created lexicons . we compared both sets of features to classify case status with a ff\bp neural_network ( nn ) and c5.0 decision_tree . for manually_created lexicons , classification_accuracy was 76.92 % for the nn and 84.60 % for c5.0 . for the automatically created lexicons , accuracy was 79.78 % for the nn and 86.81 % for c5.0 . automated lexicon creation required a much shorter development time and brought similarly high quality outcomes .
title : holistic approaches to identifying the sentiment of blogs using opinion_words ; abstract : sentiment_analysis aims to identify the orientation ( positive or negative ) of opinions or emotions expressed in documents . opinion lexicons comprise opinion_words expressing prior positive or negative_sentiments . in most previous work documents are represented as bags of words and sentiment_analysis has been cast a classification problem , where opinion lexicons are only used to enhance the classification models . in this paper we aim to establish the direct connection between document sentiment and opinion_words in the documents . we propose two holistic approaches that consider the probability_distribution of both opinion_words and their polarity for analyzing document sentiment . our extensive_experiments on blogs of 12 topics show that our holistic models significantly_improve baseline_models using words and their polarity information separately , and is also superior to an existing approach combining both types of information . ©_2011_springer-verlag .
title : on algebraic approach of r. wille and b. ganter in the investigation of texts ; abstract : the statement of the problem of a binary classification by precedents using formal concept lattices is given , in which the initial data are two binary contexts . it is specified that this problem is intractable due to the high computational_complexity of discovery process of the formal concept and constructing for them of the lattices . the decomposition reception , which allows reducing the computational_complexity of this process is proposed and theoretically justified . the reduction of computational_complexity is achieved by separation of every initial context on polynomial number of boxes ( subcontexts ) , followed by a search of the formal concepts in each selected box . the results of computational experiments are presented and they confirm the effectiveness of the proposed of reception of the reducing computational_complexity .
title : chinese text_classification based on attention_mechanism and feature-enhanced fusion neural_network ; abstract : owing to the uneven distribution of key features in chinese texts , key features play different roles in text recognition in chinese text_classification tasks . we propose a feature-enhanced fusion model based on attention_mechanism for chinese text_classification , a long short-term_memory ( lstm ) network , a convolutional_neural_network ( cnn ) , and a feature-difference enhancement attention algorithm model . the chinese text is digitized into a vector form containing certain semantic context information into the embedding_layer to train and test the neural_network by preprocessing . the feature-enhanced fusion model is implemented by double-layer lstm and cnn modules to enhance the fusion of text features extracted from the attention_mechanism for classifying the classifiers . the feature-difference enhancement attention algorithm model not only adds more weight to important text features but also strengthens the differences between them and other text features . this operation can further improves the effect of important features on chinese text recognition . the two models are classified by the softmax_function . the text_classification experiments are conducted based on the chinese text_corpus . the experimental results show that compared with the contrast model , the proposed algorithm can significantly_improve the recognition ability of chinese text features .
title : local latent_semantic_analysis based on support_vector_machine for imbalanced text_categorization ; abstract : many text_categorization tasks involve imbalanced training_examples . we tackle this problem by using improved local latent_semantic_analysis . lsa has been shown to be extremely useful but it is not an optimal representation for text_categorization because this unsupervised method ignores class_discrimination while only concentrating on representation . some local lsi methods have been proposed to improve the classification by utilizing class_discrimination information . in this paper , we choose support_vector_machine ( svm ) to generate imbalanced_dataset as the local regions for local lsa . experimental results show that our method is better than global lsa and traditional local lsa methods on classification within a much smaller lsa dimension . ©_2011_springer-verlag .
title : effect of negation in sentiment_analysis ; abstract : sentiment_analysis is the process to study of people opinion , emotion and way of considering a matter and take decision into different categorizes like positive , negative and neutral in data_mining . the sentiment_analysis is used to find out negation within the text using natural_language_processing rules . our aim is to detect negation affect on consumer reviews which looks like positive but exactly negative in sense . a number of different approaches have been used , but these approaches do not provide efficient and appropriate way of calculating negation sense in sentiment_analysis . the proposed modified negation approach presents a way of calculating negation identification and is helpful to improve classification_accuracy . main achievement of this approach is that it is helpful for calculating the negation in sentiment_analysis without the words not , no , n't , never etc . this method produced a significant result for review classification by accuracy , precision and recall .
title : sentiment_analysis of review datasets using naive_bayes and k-nn classifier ; abstract : the advent of web_2.0 has led to an increase in the amount of sentimental content available in the web . such content is often found in social_media web_sites in the form of movie or product_reviews , user_comments , testimonials , messages in discussion_forums etc . timely discovery of the sentimental or opinionated web_content has a number of advantages , the most important of all being monetization . understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements , recommendation_systems and analysis of market_trends . the focus of our project is sentiment focussed web_crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same . we use statistical_methods to capture elements of subjective style and the sentence polarity . the paper elaborately discusses two supervised_machine_learning_algorithms : k-nearest_neighbour ( k-nn ) and naive_bayes and compares their overall accuracy , precisions as well as recall values . it was seen that in case of movie reviews naive_bayes gave far better results than k-nn but for hotel reviews these algorithms gave lesser , almost same accuracies .
title : fuzzy_c-means for english sentiment_classification in a distributed system ; abstract : sentiment_classification plays a significant role in everyday_life , in political activities , in activities relating to commodity production , and commercial activities . finding a solution for the accurate and timely classification of emotions is a challenging task . in this research , we propose a new model for big_data sentiment_classification in the parallel network environment . our proposed model uses the fuzzy_c-means ( fcm ) method for english sentiment_classification with hadoop map ( m ) /reduce ( r ) in cloudera . cloudera is a parallel network environment . our proposed model can classify the sentiments of millions of english documents in the parallel network environment . we tested our model using the testing data set ( which comprised 25,000 english reviews , 12,500 being positive and 12,500 negative ) and achieved 60.2 % accuracy . our english training_data set has 60,000 english sentences , comprising 30,000 positive english sentences and 30,000 negative english sentences .
title : document_classification method based on graphs and concepts of non-rigid 3d models approach ; abstract : text document_classification is an important research topic in the field of information_retrieval , and so it is how we represent the information extracted from the documents to be classified . there exists document_classification methods and techniques based on the vector_space_model , which does n't capture the relation between words , which is considered of importance to make a better comparison and therefore classification . for this reason , two significant contributions were made , the first one is the way to create the feature_vector for document comparison , which uses adapted concepts of non-rigid 3d models comparison and graphs as a data structure to represent such documents . the second contribution is the classification method itself , which uses the representative feature_vectors of each category to classify new documents .
title : a novel approach of sentiment_classification using emoticons ; abstract : sentiment_analysis is a technique that analyzes the attitudes and emotions of people towards some product , service etc . sentiment_analysis of some product or service can be beneficial in predicting future_scope of it . however , manually analyzing a large number of documents in a limited time can be a tedious and challenging task . hence , several attempts have been made in the literature to solve this problem and several sentiment_analysis techniques have been proposed . however , these approaches do not consider or do not give much weighted to'emoticons ' present in the sentence . emotions are very popular these days and have become an integral part of written communication . hence , in this paper , we propose a novel algorithm , based on 'emoticon score learning ' for identifying sentiment of a given sentence . we test the proposed algorithm on 1000 tweets . experimental results show that the proposed algorithm is effective in sentiment_classification and give accuracy of 91.1 % . additionally , the proposed algorithm is able to detect sentences consisting of both positive and negative_sentiments .
title : natural_disaster on twitter : role of feature_extraction method of word2vec and lexicon based for determining direct eyewitness ; abstract : researchers have collected twitter data to study a wide range of topics , one of which is a natural_disaster . a social_network sensor was developed in existing research to filter natural_disaster information from direct eyewitnesses , none eyewitnesses , and non-natural_disaster information . it can be used as a tool for early_warning or monitoring when natural_disasters occur . the main component of the social_network sensor is the text tweet classification . similar to text_classification research in general , the challenge is the feature_extraction method to convert twitter text into structured_data . the strategy commonly used is vector_space representation . however , it has the potential to produce high dimension data . this research focuses on the feature_extraction method to resolve high dimension data issues . we propose a hybrid approach of word2vec-based and lexicon-based feature_extraction to produce new features . the experiment result_shows that the proposed method has fewer features and improves classification performance with an average auc value of 0.84 , and the number of features is 150 . the value is obtained by using only the word2vec-based method . in the end , this research shows that lexicon-based did not influence the improvement in the performance of social_network sensor predictions in natural_disasters .
title : sentiment_analysis based on word-emoticon clusters ; abstract : sentiment_analysis is a process of identifying and extracting subjective_information in source materials by performing text analysis and natural_language_processing . it aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual_polarity of a document . conventionally , a machine_learning algorithm is applied to classify the polarity of a given text into positive , negative or neutral and this classification is done based on emotional_states such as „ angry‟ , „ sad‟ , „ happy‟ etc . a better classification can be achieved by considering emoticons along with emotional_states . in many past_studies , the emoticons played an important role in building sentiment_lexicons and in training machine_learning classifiers and also they are considered to be the reliable indicators of sentiment . but , real meaning of all emoticons is not known to many of the social_media users . clustering of words and emoticons in the context of social_media will give a good insight about the meaning conveyed by the emoticons . emoticons are labeled as positive , negative or neutral based on the respective cluster of words they come under . emoticons are identified from the text and sentiment_analysis is performed by using emotional_states in the text and emoticons . such an analysis will result in better classification . this paper focuses on clustering of words and emoticons to know the meaning conveyed by the emoticons and compare the results of sentiment_analysis before and after the emoticons are removed from the text .
title : `` did you buy it already '' , detecting users purchase-state from their product-related questions ; abstract : in this study we address the problem of identifying the purchase-state of users , based on product-related questions they ask on an ecommerce website . we differentiate between questions_asked before buying a product ( pre-purchase ) and after ( post-purchase ) . at first , we study the ambiguity that exists in purchase-states ' definition , and then investigate the linguistic characteristics of the questions in each state . we analyze the discrepancy between the language models of pre- and post-purchase questions , and offer two classification schemes for this task , both outperform human judgments . we additionally show the effectiveness of our classification models in improving real_world applications for both consumers and sellers .
title : reproducible analysis and intelligent scientific criteria in engineering papers classification using data_science ; abstract : in this paper , we discuss , any work must be reproducible in order to influence research and contribute to our profession 's knowledge . nevertheless , studies show that 70 % of university laboratory work can not reproduce . reproducible work with not always available data sets or procedures not clearly specified is uncommon in software_engineering and complex specifications engineering . the lack of reproducible research prevents development , which means that researchers must replicate a scratch study . the re researcher will read conference papers , find empirical articles and then review data that can ( if available ) be replicated in the empirical report . this paper deals with two aspects of the problem and discusses in re articles , re documents and theoretical documents . in learning and development of an automatic classification in re and empirical document identification , recent conference papers and re documents have been utilized . they use the errc approach for performing supervised lecture classifications using natural_language therapy and machine_learning . our software is equated with a fundamental keyword approach . we study the paper collections from the engineering conference sponsored by ieee on requirements and the international software_testing and evaluation symposium sponsored by ieee in order to test our process . in all but a few cases , we examine that the procedure of errc worked superior than the reference procedure .
title : aspect-based_sentiment_classification model employing whale-optimized adaptive neural_network ; abstract : nowadays in e-commerce applications , aspect-based sentiment_analysis has become vital , and every consumer started focusing on various aspects of the product before making the purchasing decision on online portals like amazon , walmart , alibaba , etc . hence , the enhancement of sentiment_classification considering every aspect of products and services is in the limelight . in this proposed research , an aspect-based_sentiment_classification model has been developed employing sentiment whale-optimized adaptive neural_network ( swoann ) for classifying the sentiment for key_aspects of products and services . the accuracy of sentiment_classification of the product and services has been improved by the optimal selection of weights of neurons in the proposed model . the promising_results are obtained by analyzing the mobile phone review dataset when compared with other existing sentiment_classification approaches such as support_vector_machine ( svm ) and artificial_neural_network ( ann ) . the proposed work uses key features such as the positive opinion score , negative opinion score , and term_frequency-inverse_document_frequency ( tf-idf ) for representing each aspect of products and services , which further improves the overall effectiveness of the classifier . the proposed model can be compatible with any sentiment_classification problem of products and services .
title : a next step towards automated modelling of sources of law ; abstract : the ultimate_goal of the research line described here is support for automated modelling of sources of law . one of the first steps is the automatic recognition of norms . in earlier work we presented a categorization of norms or provisions in legislation . we claimed that the categories are characterized by the use of typical sentence_structures and that this would enable automatic detection and classification . in this paper we present the results of experiments in such automatic classification of provisions . we have defined fourteen different categories of provisions , and compiled a list of 88 sentence_structures for those categories from twenty dutch laws . based on these structures , a parser was used to classify the sentences in fifteen different dutch laws , classifying 91 % of 592 sentences correctly . it compares well with other , statistical_approaches . an important improvement of our classifier will be the distinction of principal and auxiliary sentences .
title : polarity_classification of hotel reviews : lexicon-based method ; abstract : customer_reviews are documentation of their experiences about products , places , services , etc. , and these reviews are the focus of interest for both organizations and individuals because they are the main factor for customers in their purchasing choices . this study aims to compare the performance of three well-known dictionaries in rating the polarity of hotel and restaurant reviews .
title : statistical comparison of opinion_spam detectors in social_media with imbalanced_datasets ; abstract : sentiment_analysis is a growing research area that analyzes people ’ s opinions towards a specific target using posts shared in social_media . however , spammers can inject false opinions to change sentiment-oriented decisions , e.g . low_quality products or policies can be promoted or advocated over others . therefore , identifying and removing spam posts in social_media is a crucial data cleaning operation for text_mining tasks including sentiment_analysis . an inherent problem related to spam_detection is the imbalanced-class problem . in this paper , we explore the impact of imbalance ratio on the performance of twitter spam_detection using multiple approaches of single and ensemble_classifiers . besides ensemble-based learning ( bagging and random_forest ) , we apply the smote oversampling technique to improve detection performance especially for classifiers sensitive to imbalanced_datasets .
title : a multiple-layer machine_learning architecture for improved accuracy in sentiment_analysis ; abstract : twitter is an online micro-blogging platform through which one can explore the hidden valuable and delightful information about the current context at any point of time , which also serves as a data source to carry out sentiment_analysis . in this paper , the sentiments of large amount of tweets generated from twitter in the form of big_data have been analyzed using machine_learning algorithms . a multi-tier architecture for sentiment_classification is proposed in this paper , which includes modules such as tokenization , data cleaning , preprocessing , stemming , updated lexicon , stopwords and emoticon dictionaries , feature_selection and machine_learning classifier . unigram and bigrams have been used as feature_extractors together with χ2 ( chi-squared ) and singular_value_decomposition for dimensionality_reduction together with two model types ( binary and reg ) , with four types of scaling methods ( no scaling , standard , signed and unsigned ) and represented them in three different vector formats ( tf-idf , binary and int ) . accuracy is considered as the evaluation standard for random_forest and bagged trees classification methods . sentiments were analyzed through tokenization and having several stages of pre-processing and several combinations of feature_vectors and classification methods . through which it was possible to achieve an accuracy of 84.14 % . obtained results conclude that , the proposed scheme gives a better accuracy when compared with existing schemes in the literature .
title : enhanced document_classification using noun verb ( nv ) terms extraction approach ; abstract : the exponential_growth in digital documents and the constantly increasing online information have called for the necessity and lead to classify the documents . document_classification is increasingly vital and indispensable for modern applications . generally , documents comprise multiple terms of extraction . here , the main concentration of the most important words is on verbs and nouns , which signify the topics and events . however , nouns and verbs technique or simply called noun verb ( nv ) as an extraction method will greatly enhance the performance of document_classification . the aim and the implication of this research is to improve document_classification performance by using and utilizing nv extraction to detect the class of a document . three classifiers namely , k-nearest_neighbor ( knn ) , naive_bayes ( nb ) , and support_vector_machine ( svm ) are used to compare the results . nine benchmark_datasets were employed in testing the proposed document_classification . the anticipated classification was verified by evaluating its accuracy . the results exhibit that the verbs as extraction affect document_classification . this encouraged the research work to combine verbs with nouns as extraction . the nv method extraction outperformed other extraction methods ( e.g. , nouns , bag of word ( bow ) , and verbs ) .
title : stemming versus light stemming as feature_selection techniques for arabic_text_categorization ; abstract : this paper compares and contrasts two feature_selection techniques when applied to arabic corpus ; in particular ; stemming , and light stemming were employed . with stemming , words are reduced to their stems . with light stemming , words are reduced to their light stems . stemming is aggressive in the sense that it reduces words to their 3-letters roots . this affects the semantics as several words with different meanings might have the same root . light stemming , by comparison , removes frequently used prefixes and suffixes in arabic words . light stemming does n't produce the root and therefore does n't affect the semantics of words ; it maps several words , which have the same meaning to a common syntactical form . the effectiveness of above two feature_selection techniques was assessed in a text_categorization exercise for arabic corpus . this corpus consists of 15000 documents that fall into three categories . the k-nearest_neighbors ( knn ) classifier was used in this work . several experiments were carried out using two different representations of the same corpus ; the first version uses stem-vectors ; and the second uses light stem-vectors as representatives of documents . these two representations were assessed in terms of size , time and accuracy . the light stem representation was superior in terms of classifier accuracy when compared with stemming . ©2008 ieee .
title : applying a culture dependent emotion triggers database for text valence and emotion classification ; abstract : this paper presents a method to automatically spot and classify the valence and emotions present in written text , based on a concept we introduced - of emotion triggers . the first step consists of incrementally building a culture dependent lexical_database of emotion triggers , emerging from the theory of relevance from pragmatics , maslow 's theory of human needs from psychology and neefs theory of human needs in economics . we start from a core of terms and expand them using lexical_resources such as wordnet , completed by nomlex , sense number disambiguated using the relevant domains concept . the mapping among languages is accomplished using eurowordnet and the completion and projection to different cultures is done through language-specific commonsense knowledgebases . subsequently , we show the manner in whichthe constructed database can be used to mine texts for valence ( polarity ) and affective meaning . an evaluation is performed on the semeval task no . 14 : affective text test data and their corresponding translation to spanish . the results and improvements are presented together with an argument on the strong and weak points of the method and the directions for future work .
title : multimodal attention-based learning for imbalanced corporate documents classification ; abstract : the corporate document_classification process may rely on the use of textual approach considered separately of image features . on the opposite , some methods only use the visual_content of documents while ignoring the semantic information . this semantic corresponds to an important part of corporate documents which make some classes of document impossible to distinguish effectively . the recent state-of-the-art deep_learning methods propose to combine the textual_content and the visual_features within a multi-modal approach . in addition , corporate document_classification processes offer a particular challenge for deep_learning-based systems with an imbalanced corpus . indeed the neural_network performances strongly depend on the corpus used to train the network , and an imbalanced set generally entails bad final system performances . this paper proposes a multi-modal deep convolutional_network with an attention model designed to classify a large variety of imbalanced corporate documents . our proposed approach is compared to several state-of-the-art methods designed for document_classification task using the textual_content , the visual_content and some multi-modal approaches . we obtained higher performances on our two testing datasets with an improvement of 2 % on our private dataset and a 3 % on the public rvl-cdip dataset .
title : semi-supervised text_classification algorithm based on a feature_mapping ; abstract : there are many algorithms based on data distribution to effectively_solve the problem of semi-supervised text_categorization . however , they may perform badly when the labeled_data distribution is different from the unlabeled_data . to solve the problem , semi-supervised text_classification algorithm based on feature_mapping was proposed . first , three sets of features were selected respectively from labeled_data , unlabeled_data and test data by using different feature_selection methods , and their values were initialize . second , three feature_mapping functions were studied , and the weight of each feature was recalculated by them . finally , the em_algorithm was employ to classify the text data . experiments of standard data sets show that the proposed algorithm is effective .
title : classifying the level of bid_price volatility based on machine_learning with parameters from bid documents as risk factors ; abstract : the purpose of this study is to classify the bid_price volatility level with machine_learning and parameters from bid documents as risk factors . to this end , we studied project-oriented risk_factors affecting the bid_price and pre-bid clarification document as the uncertainty of bid documents through preliminary research . the authors collected caltrans ’ s bid summary and pre-bid clarification document from 2011-2018 as data samples . to train the classification model , the data were preprocessed to create a final dataset of 269 projects consisting of input and output parameters . the projects in which the bid inquiries were not resolved in the pre-bid clarification had higher bid averages and bid ranges than the risk-resolved projects . besides this , regarding the two classification models with neural_network ( nn ) algorithms , model 2 , which included the uncertainty in the bid documents as a parameter , predicted the bid average risk and bid range risk more accurately ( 52.5 % and 72.5 % , respectively ) than model 1 ( 26.4 % and 23.3 % , respectively ) . the accuracy of model 2 was verified with 40 verification test datasets .
title : constructing im/migrants and ethnic minority groups as ‘ carriers of disease ’ : power effects of categorization practices in tuberculosis health reporting in the uk and germany ; abstract : migration- and ethnicity-related categories are a core feature of public_health systems internationally , particularly in health reporting on communicable infectious_diseases . the specific categories and classifications used differ from country to country and are subject to controversy and change . the article compares categorization practices in health reporting in the uk and germany with regard to tuberculosis . tuberculosis has been framed as a ‘ migrants ’ disease ’ in recent decades and new categories were introduced to collect and report epidemiological data . we reconstruct the genesis , change and power effects of categories related to im/migrants and ethnic minority groups . in both countries , migration-related categorizations entail constructions of im/migrants as ‘ carriers of disease ’ . however , the categories also connect with discourses on human_rights , prevention , treatment and care for migrants as vulnerable groups . while this ambivalent role of migration-related categories is not unique to health statistics , the potential contribution to processes of ‘ othering ’ and politics of exclusion seem particularly imminent in the context of communicable diseases such as tuberculosis . ethnicity categories used in the uk , but not in germany , also contribute to othering through racialization and culturalization , yet at the same time provide opportunities for community participation in the discourse .
title : a simple study of webpage text_classification algorithms for arabic and english languages ; abstract : webpage text_classification is an important problem that has been studied through different approaches and algorithms . it aims to assign a predefined_category to a webpage based on its content and linguistic features . it has many applications such as word_sense_disambiguation , document_indexing , text filtering , webpages hierarchical categorization and document_organization . this study is a part of a work in progress , in which we are targeting to develop bi-languages algorithm for classifying arabic and english webpage text and can perform accurate and efficient in both languages . it aims at providing a simple overview of many approaches that constructed for classifying arabic and english webpage documents . in this survey , the widely used algorithms for text_classification are given with a comparison of the recent_researches in classification field for arabic and english languages to conclude which is the best algorithm that we can apply for both arabic and english languages . © 2013 ieee .
title : sentiment_analysis using learning techniques ; abstract : this paper depicts sentiment_analysis classification as an efficient process for analysing textual_data coming from various internet resources . one of the most prominent research subjects in recent_years has been sentiment_analysis . it is a critical challenge for both companies and users to provide an accurate and useful overview of the network , which requires a large amount of data in terms of configuration , usage and content ; hence , the sentiment_analysis principle is proposed to address this problem . methods of sentiment_analysis seek to reveal any feelings , subjectivity and opinions in the text . it is virtually difficult to analyse such a vast number of reviews manually . as a result , sa is used for extracting the general polarity or sentiment of opinions from documents . sentiment research is used in twitter , movie reviews , blogs and consumer comments , among other places . for sentiment_analysis , three methods are commonly used : machine_learning-based techniques , lexicon-based techniques and hybrid techniques , but the machine_learning approach is more efficient and accurate . many variations and extensions of machine_learning methods and software have recently been available in recent_times . this paper presents a brief introduction to the sentiment_analysis , its classification and levels of sentiment_analysis . further , machine_learning techniques for sentiment_analysis are also mentioned in detail in this paper . this paper aims to provide a brief knowledge of the sentiment_analysis process , including standard sa methods , from the viewpoint of ml methods , in which machines interpret and identify human sentiments conveyed in speech and text .
title : firebert : hardening bert classifiers against adversarial_attack ; abstract : we present firebert , a set of three proof-of-concept nlp classifiers hardened against textfooler-style word-perturbation by producing diverse alternatives to original samples . in one approach , we co-tune bert against the training_data and synthetic adversarial_samples . in a second approach , we generate the synthetic samples at evaluation time through substitution of words and perturbation of embedding_vectors . the diversified evaluation results are then combined by voting . a third approach replaces evaluation-time word_substitution with perturbation of embedding_vectors . we evaluate firebert for mnli and imdb movie review datasets , in the original and on adversarial_examples generated by textfooler . we also test whether textfooler is less successful in creating new adversarial_samples when manipulating firebert , compared to working on unhardened classifiers . we show that it is possible to improve the accuracy of bert-based models in the face of adversarial_attacks without significantly reducing the accuracy for regular benchmark samples . we present co-tuning with a synthetic_data generator as a highly effective method to protect against 95 % of pre-manufactured adversarial_samples while maintaining 98 % of original benchmark performance . we also demonstrate evaluation-time perturbation as a promising_direction for further research , restoring accuracy up to 75 % of benchmark performance for pre-made adversarials , and up to 65 % ( from a baseline of 75 % orig./12 % attack ) under active attack by textfooler .
title : job pre-interview system with artificial_intelligence ; abstract : with the study ; the interviews , which are the preliminary stage of the recruitment process , are translated into text by the job_seekers ' answers . then , it is ensured that the answer to the question is checked by associating it with the flow of dialogue and the system can decide the next question according to the answer given by the job seeker . according to these results , artificial_intelligence interview system which provides reporting of interview results is presented . in the system , studies on natural_language_processing , semantic analysis , the formation and use of ontologies , classification_algorithms , machine_learning and technological expertise are carried out .
title : a double channel cnn-lstm model for text_classification ; abstract : the cnn-lstm model has the advantages of combining convolutional_neural_network ( cnn ) and long-short_term_memory ( lstm ) . it can perform timing analysis while extracting abstract features . it is widely used in computer vision and natural_language_processing ( nlp ) fields and has achieved satisfactory_results . however , for a large number of samples of complex text data , especially for words with ambiguous meanings , the word-level cnn-lstm model is insufficient . therefore , in order to solve this issue , this paper presents an improved double channel ( dc ) mechanism as a significant enhancement to cnn-lstm . in this dc mechanism , two channels are used to receive word-level and char-level embedding respectively , at the same time . hybrid attention is proposed to combine the current time output with the current time unit state , and then using attention to calculate the weight . by calculating the probability_distribution of each timestep input data weight , the weight score is obtained , and then weighted summation is performed , and the data input by each timestep is subjected to trade-off learning to improve the generalization_ability of the model learning . after experimental comparison , the dc cnn-lstm model proposed in this paper has significantly superior accuracy and f1-score compared with the basic cnn-lstm model .
title : isomer : transfer enhanced dual-channel heterogeneous_dependency attention_network for aspect-based_sentiment_classification ; abstract : aspect-based_sentiment_classification aims to predict the sentiment_polarity of a specific aspect in a sentence . however , most existing_methods attempt to construct dependency_relations into a homogeneous dependency_graph with sparsity and ambiguity , which only considers one type of node and one type of edge , thus can not cover the comprehensive contextualized features of short texts or consider any additional node types or semantic relation information . to solve those issues , we present a sentiment_analysis model , isomer , which performs dual-channel attention on heterogeneous_dependency graphs incorporating external_knowledge to integrate additional_information effectively . specifically , a transfer-enhanced dual-channel heterogeneous_dependency attention_network is designed in isomer for modeling short texts by heterogeneous_dependency graphs . these heterogeneous_dependency graphs not only consider different types of information but also incorporate external_knowledge . experiments studies show that isomer outperforms the-state-of-arts on diverse datasets . furthermore , the results suggest that isomer captures the importance of various information features to focus on informative contextual words .
title : a novel approach of sensitive data classification using convolution neural_network and logistic_regression ; abstract : text_classification is a basic approach of text_mining and natural_language_processing . in previous use , classifiers use human interface features like frequency base and n-gram features which are not able to find non-linearity in features and increase overlapping in features which directly impacts the performance of classifiers . in this paper , proposed convolution based approach refines the traditional features in layered approach by activation_function . this process increase the effective pattern for learning which is learn by logistic_regression and optimized by boosting approach . in experiment , there is comparison of machine_learning approach which uses traditional features and deep_learning approach which refine the traditional approach for increasing non-linearity pattern . the results showed that proposed approach cnn-logistic_regression improves the accuracy significantly because of the improving pattern of features .
title : biomedical semantic indexing by deep_neural_network with multi-task_learning ; abstract : background : biomedical semantic indexing is important for information_retrieval and many other research fields in bioinformatics . it annotates biomedical citations with medical_subject_headings . in face of unbalanced category_distribution in the training_data , sampling_methods are difficult to apply for semantic indexing task . results : in this paper , we present a novel deep serial multi-task_learning model . the primary task treats the biomedical semantic indexing as a multi-label_text_classification issue that considers the relations of the labels . the auxiliary task is a regression task that predicts the mesh number of the citation and provides hints for the network to make it converge faster . the experimental results on the bioasq-task5a open dataset show that our model outperforms the state-of-the-art solution `` mti '' , proposed by the us national_library_of_medicine . further , it not only achieves the highest precision among all the solutions in bioasq-task5a but also has faster_convergence speed compared with some naive deep_learning methods . conclusions : rather than parallel in an ordinary multi-task structure , the tasks in our model are serial and tightly coupled . it can achieve satisfied performance without any handcrafted_feature .
title : enhanced malay sentiment_analysis with an ensemble classification machine_learning approach ; abstract : sentiment_analysis is one of the challenging and important tasks that involves natural_language_processing , web_mining and machine_learning . this study aims to propose an enhanced ensemble of machine_learning classification methods for malay sentiment_analysis . three classification approaches ( naive_bayes , support_vector_machine and k-nearest_neighbour ) and five ensemble classification_algorithms ( bagging , stacking , voting , adaboost and metacost ) were experimented to achieve the best possible ensemble model for malay sentiment_classification . a wide range of ensemble experiments are conducted on a malay opinion corpus ( moc ) . this study demonstrates that ensemble approaches improve the performance of malay sentiment-based classification , however , the results depend on the classifier used and the ensemble algorithm as well as the number of classifiers in the ensemble approach . the experiments also show that the ensemble classification approaches achieve the best result with an f-measure of 85.81 % .
title : evidence and presumptions for analyzing and detecting misunderstandings ; abstract : the detection and analysis of misunderstandings are crucial aspects of discourse_analysis , and presuppose a twofold investigation of their structure . first , misunderstandings need to be identified and , more importantly , justified . for this reason , a classification of the types and force of evidence of a misunderstanding is needed . second , misunderstandings reveal differences in the interlocutors ’ interpretations of an utterance , which can be examined by considering the presumptions that they use in their interpretation . this paper proposes a functional approach to misunderstandings grounded on presumptive reasoning and types of presumptions , in which incompatible interpretations or interpretative failures are examined as defaults of the underlying interpretative reasoning , caused by overlooked evidence or conflicting presumptions . moreover , it advances a classification of the types and the probative weights of the evidence that can be used to detect misunderstandings . the proposed methodology and its implications are illustrated through the analysis of doctor–patient communication in diabetes care .
title : reducing misclassification due to overlapping classes in text_classification via stacking classifiers on different feature_subsets ; abstract : correct classification of customer_support tickets or complaints can help companies to improve the quality of their services to the customers . one of the challenges in text_classification is when certain classes tend to share the same vocabulary . this can result in misclassification by the machine_learning algorithm used . the problem is worsened when the dataset is imbalanced . to address this issue , we propose a stacking algorithm based on combining different selected classifiers that operate on different feature_subsets ; depending on those features that tend to improve the recall and the precision of the overlapped classes . in our approach , first , we train different linear and non-linear_classifiers on the full feature_set . second , we use the chi2 test to determine the best feature_set for all our pre-trained classifiers that improve the f1-score for the overlapped class ( es ) . finally , we train a two-layered stacked model composed of the best base_learners obtained from the first step as layer-1 and combine it with a strong meta-learner for the second layer . the experimental results on a real-world dataset from a large it organization and a public consumer complaint database show an improvement in the overall accuracy as well as a reduction in the misclassification_rate for the overlapped classes .
title : a study of interrelation between ratings and user_reviews in light of classification ; abstract : natural_language_processing ( nlp ) is a broad area of study where human language and their emotion can be characterized and analyzed by computer . types of nlp problems can be , but not limited to , categorization , classification , analytical , etc . a major application of nlp is at sentiment_analysis of rich texts . in this study , we have worked on sentiment_analysis and categorization aspects of nlp . here , we have performed analysis , lemmatization , sampling , and classification on yelp text reviews , which is an open-source dataset , and then compared the results of the different types of classifiers . categorization of different texts has been considered as a response of classification problems . after validation , we have achieved a significant level of robustness in these methods . an attempt was made to study nlp classification problems under various situations .
title : batch active_learning for text_classification and sentiment_analysis ; abstract : supervised_learning of classifiers for text_classification and sentiment_analysis relies on the availability of labels that may be either difficult or expensive to obtain . a standard procedure is to add labels to the training dataset sequentially by querying an annotator until the model reaches a satisfactory_performance . active_learning is a process that optimizes unlabeled_data records selection for which the knowledge of the label would bring the highest discriminability of the dataset . batch active_learning is a generalization of a single instance active_learning by selecting a batch of documents for labeling . this task is much more demanding because plenty of different factors come into consideration ( i. e. batch_size , batch evaluation , etc. ) . in this paper , we provide a large_scale study by decomposing the existing algorithms into building_blocks and systematically comparing meaningful combinations of these blocks with a subsequent evaluation on different text datasets . while each block is known ( warm start weights initialization , dropout mc , entropy sampling , etc . ) , many of their combinations like bayesian strategies with agglomerative_clustering are first proposed in our paper with excellent_performance . particularly , our extension of the warm start method to batch active_learning is among the top performing strategies on all datasets . we studied the effect of this proposal comparing the outcomes of varying distinct factors of an active_learning algorithm . some of these factors include initialization of the algorithm , uncertainty representation , acquisition function , and batch selection_strategy . further , various combinations of these are tested on selected nlp problems with documents encoded using roberta embeddings . datasets cover context integrity ( gibberish wackerow ) , fake_news_detection ( kaggle fake_news_detection ) , categorization of short texts by emotional context ( twitter sentiment140 ) , and sentiment_classification ( amazon reviews ) . ultimately , we show that each of the active_learning factors has advantages for certain datasets or experimental settings .
title : performance evaluation of hybrid feature_selection technique for sentiment_classification based on food reviews ; abstract : this paper presents an evaluation of the performance efficiency of sentiment_classification using a hybrid feature_selection technique . this technique is able to overcome the issue of lack in evaluating features importance by using a combination of tf-idf+svm-rfe ( term_frequency-inverse_document_frequency ( tf-idf ) and supports vector machine ( svm-rfe ) ) . feature_importance is measured and significant features are selected recursively based on the number of significant features known as k-top features . we tested this technique with a food reviews dataset from kaggle to classify a positive and negative review . finally , svm has been deployed as a classifier to evaluate the classification performance . the performance is observed based on the accuracy , precision , recall and f-measure . the highest_accuracy is 80 % , precision is 82 % , recall is 76 % and f-measure is 79 % . consequently , 24.5 % of the features to be classified in this technique have been reduced in obtaining these highest results . thus , the computational_resources are able to be utilized optimally from this reduction and the classification performance efficiency is able to be maintained .
title : short-text feature_expansion and classification based on non-negative_matrix_factorization ; abstract : in this paper , a non-negative_matrix_factorization feature_expansion ( nmffe ) approach was proposed to overcome the feature-sparsity issue when expanding features of short-text . firstly , we took the internal relationships of short texts and words into account when segmenting words from texts and constructing their relationship matrix . secondly , we utilized dual regularization non-negative matrix tri-factorization algorithm ( dnmtf ) to obtain the words clustering indicator matrix , which was used to get the feature_space by dimensionality_reduction methods . thirdly , words with close relationship were selected out from the feature_space and added into the short-text in order to solve the sparsity_issue . the experimental results showed that the accuracy of short_text_classification of our nmffe algorithm increased 25.77 % , 10.89 % and 1.79 % on three datasets : web snippets , twitter sports and agnews respectively compared with word2vec algorithm and char-cnn algorithm . it indicated that the nmffe algorithm was better than bow algorithm and the char-cnn algorithm in terms of classification_accuracy and algorithm robustness .
title : a comprehensive survey and classification of approaches for community question_answering ; abstract : community question-answering ( cqa ) systems , such as yahoo ! answers or stack_overflow , belong to a prominent group of successful and popular web_2.0 applications , which are used every day by millions of users to find an answer on complex , subjective , or context-dependent questions . in order to obtain answers effectively , cqa systems should optimally harness collective_intelligence of the whole online_community , which will be impossible without appropriate collaboration support provided by information technologies . therefore , cqa became an interesting and promising subject of research in computer science and now we can gather the results of 10 years of research . nevertheless , in spite of the increasing number of publications emerging each year , so far the research on cqa systems has missed a comprehensive state-of-the-art survey . we attempt to fill this gap by a review of 265 articles_published between 2005 and 2014 , which were selected from major conferences and journals . according to this evaluation , at first we propose a framework that defines descriptive attributes of cqa approaches . second , we introduce a classification of all approaches with respect to problems they are aimed to solve . the classification is consequently employed in a review of a significant number of representative approaches , which are described by means of attributes from the descriptive framework . as a part of the survey , we also depict the current trends as well as highlight the areas that require further attention from the research community .
title : archinet : a concept-token_based approach for determining architectural change categories ; abstract : causes of software architectural change are classified as perfective , preventive , corrective , and adaptive . change classification is used to promote common approaches for addressing similar changes , produce appropriate design documentation for a release , construct a developer 's profile , form a balanced team , support code_review , etc . however , automated architectural change classification techniques are in their infancy , perhaps due to the lack of a benchmark_dataset and the need for extensive human involvement . to address these shortcomings , we present a benchmark_dataset and a text classifier for determining the architectural change rationale from commit descriptions . first , we explored source_code properties for change classification independent of project activity descriptions and found poor outcomes . next , through extensive analysis , we identified the challenges of classifying architectural change from text and proposed a new classifier that uses concept tokens derived from the concept analysis of change samples . we also studied the sensitivity of change classification of various types of tokens present in commit_messages . the experimental outcomes employing 10-fold and cross-project validation techniques with five popular open-source systems show that the f1 score of our proposed classifier is around 70 % . the precision and recall are mostly consistent among all categories of change and more promising than competing methods for text_classification .
title : polarity_classification of arabic sentiments ; abstract : sentiment_analysis/opinion_mining is associated with social_media and usually aims to automatically identify the polarities of different points of views of the users of the social_media about different aspects of life . the polarity of a sentiment reflects the point view of its author about a certain issue . this study aims to present a new method to identify the polarity of arabic reviews and comments whether they are written in modern_standard_arabic ( msa ) , or one of the arabic dialects , and/or include emoticons . the proposed method is called detection of arabic sentiment_analysis polarity ( dasap ) . a modest dataset of arabic comments , posts , and reviews is collected from online social_network websites ( i.e . facebook , blogs , youtube , and twitter ) . this dataset is used to evaluate the effectiveness of the proposed method ( dasap ) . receiver operating characteristic ( roc ) prediction quality measurements are used to evaluate the effectiveness of dasap based on the collected dataset .
title : minimum_description_length principles for detection and classification of ftp exploits ; abstract : in this paper we build on the principle of `` conservation i of complexity , '' analyzed in [ 5 ] , to measure protocol redundancy and pattern content as a metric for information_assurance . we first analyze complexity estimators as a tool for detecting ftp exploits . results showing the utility of complexity-based information_assurance to detect exploits over the file transfer protocol are presented and analyzed . we show that complexity metrics are able to distinguish between ftp exploits and normal sessions within some margin of error . we then derive a new heuristic for complexity estimation using minimum_description_length principles and develop a new complexity estimator and compression algorithm based on grammar inference using this heuristic . this estimator is used to provide meaningful models of unknown data sets . finally we demonstrate the capability of our complexity-based approach to classify protocol behavior based on similarity distance_metrics from known behaviors . ©2004 ieee .
title : covid-19 vaccine hesitancy : a social_media analysis using deep_learning ; abstract : hesitant attitudes have been a significant issue since the development of the first vaccines—the who sees them as one of the most critical global_health threats . the increasing use of social_media to spread questionable information about vaccination strongly impacts the population ’ s decision to get vaccinated . developing text_classification methods that can identify hesitant messages on social_media could be useful for health campaigns in their efforts to address negative influences from social_media platforms and provide reliable information to support their strategies against hesitant-vaccination sentiments . this study aims to evaluate the performance of different machine_learning models and deep_learning methods in identifying vaccine-hesitant tweets that are being published during the covid-19_pandemic . our concluding remarks are that long short-term_memory and recurrent_neural_network models have outperformed traditional_machine_learning models on detecting vaccine-hesitant messages in social_media , with an accuracy_rate of 86 % against 83 % .
title : referring object manipulation of natural_images with conditional classifier-free guidance ; abstract : we introduce the problem of referring object manipulation ( rom ) , which aims to generate photo-realistic image edits regarding two textual_descriptions : 1 ) a text referring to an object in the input image and 2 ) a text describing how to manipulate the referred object . a successful rom model would enable users to simply use natural_language to manipulate images , removing the need for learning sophisticated image_editing software . we present one of the first approach to address this challenging multi-modal problem by combining a referring image_segmentation method with a text-guided diffusion model . specifically , we propose a conditional classifier-free guidance scheme to better guide the diffusion process along the direction from the referring expression to the target prompt . in addition , we provide a new localized ranking method and further improvements to make the generated edits more robust . experimental results show that the proposed framework can serve as a simple but strong_baseline for referring object manipulation . also , comparisons with several baseline text-guided diffusion models demonstrate the effectiveness of our conditional classifier-free guidance technique .
title : active_learning in automated text_classification : a case_study exploring bias in predicted model performance_metrics ; abstract : machine_learning has emerged as a cost-effective innovation to support systematic_literature reviews in human health risk assessments and other contexts . supervised_machine_learning approaches rely on a training dataset , a relatively small set of documents with human-annotated labels indicating their topic , to build models that automatically_classify a larger set of unclassified_documents . “ active ” machine_learning has been proposed as an approach that limits the cost of creating a training dataset by interactively and sequentially focussing on training only the most informative documents . we simulate active_learning using a dataset of approximately 7000 abstracts from the scientific_literature related to the chemical arsenic . the dataset was previously annotated by subject_matter experts with regard to relevance to two topics relating to toxicology and risk_assessment . we examine the performance of alternative sampling approaches to sequentially expanding the training dataset , specifically looking at uncertainty-based sampling and probability-based sampling . we discover that while such active_learning methods can potentially reduce training dataset size compared to random_sampling , predictions of model performance in active_learning are likely to suffer from statistical bias that negates the method ’ s potential benefits . we discuss approaches and the extent to which the bias resulting from skewed sampling can be compensated . we propose a useful role for active_learning in contexts in which the accuracy of model performance_metrics is not critical and/or where it is beneficial to rapidly create a class-balanced training dataset .
title : an overview of alternative typologies of slavic_languages ; abstract : the typology of slavic_languages has been frequently dealt with in different publications since the late 19th century . in this paper , the author reviews some of the most significant attempts aimed at the phonological , morphological , and syntactic levels of this typological research . it appears that the phonological classification first elaborated by baudouin de courtenay has remained reliable to this day . in morphology , however , the only method for categorization seems to be the identification of certain grammatical markers . syntactic typology is still a young field of linguistics ; nevertheless , there exist promising ventures in it , too . it is remarkable that the typological findings for the modern slavic_languages to a large extent coincide with the results of areal studies . based on the information presented in the paper , the following implications can be made with reference to the typology of the specific linguistic levels in the slavic_languages . the most uniform level is that of phonological typology because in all the models presented here , a key role is played by two prosodic_features : the opposition of long and short vowels , on the one hand , and the character of word stress , on the other . thus , the phonological typology first elaborated by baudouin de courtenay has proved to be reliable up to the present . at least no competing theories in this field can be seen for the time being . as to morphological typology , it is not possible to identify features or criteria similar to the phonological models which could be applied for the differentiation of whatever morphological types . the slavic_languages , even bulgarian and macedonian , which have no nominal declension , have remained fusional ( inflectional ) languages , within which it is not easy to delineate further subtypes . so far , the only way of morphological categorization seems to be the identification and comparison of individual grammatical_features of the different slavic_languages , as it is illustrated tentatively in section 2 . the syntactic typology of the slavic_languages is still a very young field of typological research . therefore , it is impossible to arrive at any general conclusions on this matter ( besides the ones mentioned in section 3 ) . the model offered by haspelmath for the european languages looks quite promising but it is necessary to work out further details and specific methods so that it could be successfully applied specifically for the slavic_languages . one can not fail to notice that the typological regularities specified by way of the morphological and syntactic observations in sections 2 and 3 , to a marked extent coincide with the facts of the areal ( geographical ) classification of the slavic_languages , as it was sharply noticed by bogoroditsky , janda , tommola , and other researchers .
title : a survey on sentiment_analysis_and_opinion_mining in greek social_media ; abstract : as the amount of content that is created on social_media is constantly increasing , more and more opinions and sentiments are expressed by people in various subjects . in this respect , sentiment_analysis_and_opinion_mining techniques can be valuable for the automatic analysis of huge textual corpora ( comments , reviews , tweets etc. ) . despite the advances in text_mining algorithms , deep_learning techniques , and text_representation models , the results in such tasks are very good for only a few high-density languages ( e.g. , english ) that possess large training corpora and rich linguistic resources ; nevertheless , there is still room for improvement for the other lower-density languages as well . in this direction , the current work employs various language models for representing social_media texts and text classifiers in the greek_language , for detecting the polarity of opinions_expressed on social_media . the experimental results on a related dataset collected by the authors of the current work are promising , since various classifiers based on the language models ( naive_bayesian , random_forests , support_vector_machines , logistic_regression , deep feed-forward neural_networks ) outperform those of word or sentence-based embeddings ( word2vec , glove ) , achieving a classification_accuracy of more than 80 % . additionally , a new language model for greek social_media has also been trained on the aforementioned dataset , proving that language models based on domain_specific corpora can improve the performance of generic language models by a margin of 2 % . finally , the resulting models are made freely available to the research community .
title : improving multi-label_text_classification models with knowledge graphs ; abstract : multi-label_text_classification ( mltc ) is a variant of classification problem where multiple_labels are assigned to each instance . most existing mltc methods ignore the relationship between the target labels . since the hierarchical_relationship for addressing these problems is significant , a semantic network approach with the help of knowledge graphs can be used . this paper proposes a knowledge_graph-based approach together with gru ( gated_recurrent_unit ) neural_network model to solve an mltc problem on a research text dataset . in particular , we leverage the tax2vec approach to extract hypernyms from the wordnet knowledge_graph and enrich the dataset . the enrichment results in following a tree-like structure to identify the relationship between the semantic concepts . the result_shows that the enriched dataset outperforms the traditional gru neural_network-based model based on different evaluation_metrics .
title : automatic call section segmentation for contact-center calls ; abstract : this paper presents a svm ( support_vector_machine ) classification system which divides contact-center call transcripts into `` greeting '' , `` question '' , `` refine '' , `` research '' , `` resolution '' , `` closing '' and `` out-of-topic '' sections . this call section segmentation is useful to improve search and retrieval functions and to provide more detailed statistics on calls . we use an off-the-shelf automatic_speech_recognition ( asr ) system to generate call transcripts from recorded calls between customers and service representatives . we first classify an individual utterance into a call section by applying the svm classifier and then merge adjacent utterances classified into a same call section . we experiment with the proposed system on 100 automatically transcribed calls . the 10-fold_cross_validation shows 87.2 % classification_accuracy . we also compare the proposed algorithm with two other approaches - the most frequent section only method and a maximum_entropy-based segmentation . the evaluation shows that our system 's accuracy is 12 % higher than the first baseline system and 6 % higher than the second baseline system respectively . copyright 2007 acm .
title : arabic aspect_based_sentiment_classification using bert ; abstract : aspect-based sentiment_analysis ( absa ) is a textual analysis methodology that defines the polarity of opinions on certain aspects related to specific targets . the majority of research on absa is in english , with a small amount of work available in arabic . most previous arabic research has relied on deep_learning models that depend primarily on context-independent word_embeddings ( e.g.word2vec ) , where each word has a fixed representation independent of its context . this article explores the modeling capabilities of contextual_embeddings from pre-trained_language_models , such as bert , and making use of sentence_pair input on arabic aspect_sentiment polarity_classification task . in particular , we develop a simple but effective bert-based neural baseline to handle this task . our bert architecture with a simple_linear classification layer surpassed the state-of-the-art works , according to the experimental results on three different arabic datasets . achieving an accuracy of 89.51 % on the arabic hotel reviews dataset , 73 % on the human annotated book reviews dataset , and 85.73 % on the arabic news dataset .
title : enhanced bag-of-words model for phrase-level sentiment_analysis ; abstract : we propose a novel rule-based model to incorporate contextual_information and effect of negation that enhances the performance of sentiment_classification performed using bag-of-words models . we employed morphological_analysis in feature_extraction to ensure feature_vector contains only opinionated words in a textual review . also it reduces the dimensionality of feature_vector and , eventually improves the efficiency of the classification algorithm . further , we consider grammatical relationships to incorporate the context of adjectives and scope of negations within a phrase , to the feature_vector . this enables our model to capture contextual_polarity of adjectives and impact of negation words . for the morphological_analysis we mainly employ part of speech taggers ( pos taggers ) and grammatical relationships which are obtained using typed dependency parsers . by using dependency-based rules , we relax the conditional independent assumption of bag-of-words models by way of combining adjectives and negations to identified target words and , hence obtain a sentiment_classification accuracy that significantly better than baseline performance .
title : ontology-based enriched concept graphs for medical document_classification ; abstract : the rapidly_increasing volume of medical text data , including biomedical_literature and clinical_records , presents difficulties to biomedical researchers and clinical practitioners . automatic text_classification is an important means for managing medical text data . the main challenge in medical text_classification is the complex terminology used in these documents . therefore , it is critical to handle synonymy , polysemy , and multi-word concepts so that classification is based on the meaning of these documents . the solution to this problem of complex terminology helps in building systems with better access to relevant data , resulting in more effective utilisation of the existing information . in this paper , we present a simple and effective approach to address this challenge . a concept graph is automatically_constructed and enriched for each medical text document with the help of a domain-specific similarity_matrix that is built using unified medical language system ( umls ) concepts in the training documents . medical text documents are compared based on their enriched concept graphs using a graph kernel . classification is then done based on the comparison result . the benefit of this approach is that it allows the incorporation of domain_knowledge into the classification framework . the experiments on biomedical_abstracts and clinical_reports classification show the effectiveness of the proposed approach . based on evaluation_metrics of precision , recall and f1-scores , our method_achieves a significantly higher classification performance than other widely used similarity_measures for similarity-based text_classification .
title : rankt5 : fine-tuning t5 for text ranking with ranking losses ; abstract : recently , substantial progress has been made in text ranking based on pretrained_language_models such as bert . however , there are limited studies on how to leverage more powerful sequence-to-sequence models such as t5 . existing attempts usually formulate text ranking as classification and rely on postprocessing to obtain a ranked_list . in this paper , we propose rankt5 and study two t5-based ranking model structures , an encoder-decoder and an encoder-only one , so that they not only can directly output ranking scores for each query-document pair , but also can be fine-tuned with `` pairwise '' or `` listwise '' ranking losses to optimize ranking performances . our experiments show that the proposed models with ranking losses can achieve substantial ranking performance_gains on different public text ranking data sets . moreover , when fine-tuned with listwise ranking losses , the ranking model appears to have better zero-shot ranking performance on out-of-domain data sets compared to the model fine-tuned with classification losses .
title : increasing the sensitivity of parts classification system ; abstract : this paper demonstrates the application of fuzzy_set theory to increase the sensitivity of group technology ( gt ) , the most popular parts classification system in use today . it is shown that subjective descriptors of part shape can be quantified with a reasonable degree of reliability and , in reduced form , provide that portion of the part code which describes the part geometry . the proposed procedure is not only objective , it is more sensitive than the existing method of assigning codes for part geometry . moreover , it can lead to superior sequencing of parts when such sequencing is based on similarity of geometrical shapes . © 1988 .
title : identifying entity properties from text with zero-shot learning ; abstract : we propose a method for identifying a set of entity properties from text . identifying entity properties is similar to a relation_extraction task that can be cast as a classification of sentences . normally , this task can be achieved by distant_supervised learning by automatically preparing training sentences for each property ; however , it is impractical to prepare training sentences for every property . therefore , we describe a zero-shot learning problem for this task and propose a neural_network-based model that does not rely on a complete training_set comprising training sentences for every property . to achieve this , we utilize embeddings of properties obtained from a knowledge_graph embedding using different components of a knowledge_graph structure . the embeddings of properties are combined with the model to enable identification of properties with no available training sentences . by using our newly constructed dataset as well as an existing dataset , experiments revealed that our model achieved a better performance for properties with no training sentences , relative to baseline results , even comparable to that achieved for properties with training sentences .
title : da-bert : enhancing part-of-speech_tagging of aspect sentiment_analysis using bert ; abstract : with the development of internet , text-based data from web have grown exponentially where the data carry large amount of valuable_information . as a vital branch of sentiment_analysis , the aspect sentiment_analysis of short text on social_media has attracted interests of researchers . aspect_sentiment_classification is a kind of fine-grained textual sentiment_classification . currently , the attention_mechanism is mainly combined with rnn ( recurrent_neural_network ) or lstm ( long short-term_memory ) networks . such neural_network-based sentiment_analysis model not only has a complicated computational structure , but also has computational dependence . to address the above problems and improve the accuracy of the target-based sentiment_classification for short text , we propose a neural_network model that combines deep-attention with bidirectional_encoder_representations_from_transformers ( da-bert ) . the da-bert model can fully mine the relationships between target words and emotional words in a sentence , and it does not require syntactic analysis of sentences or external_knowledge such as sentiment_lexicon . the training_speed of the proposed da-bert model has been greatly_improved while removing the computational dependencies of rnn structure . compared with lstm , td-lstm , tc-lstm , at-lstm , atae-lstm , and pat-lstm , the results of experiments on the dataset semeval2014 task4 show that the accuracy of the da-bert model is improved by 13.63 % on average where the word_vector is 300 dimensions in aspect_sentiment_classification .
title : albert over match-lstm network for intelligent questions classification in chinese ; abstract : this paper introduces a series of experiments with an albert over match-lstm network on the top of pre-trained_word_vectors , for accurate classification of intelligent_question_answering and thus the guarantee of precise information service . to improve the performance of data classification , a short_text_classification method based on an albert and match-lstm model was proposed to overcome the limitations of the classification process , such as few vocabularies , sparse features , large amount of data , lots of noise and poor normalization . in the model , jieba word_segmentation tools and agricultural dictionary were selected to text segmentation , glove algorithm was then adopted to expand the text characteristic and weighted word_vector according to the text of key vector , bi-directional_gated_recurrent_unit was applied to catch the context feature information and multiconvolutional neural_networks were finally established to gain local multidimensional characteristics of text . batch_normalization , dropout , global average pooling and global max_pooling were utilized to solve overfitting problem . the results showed that the model could classify questions accurately , with a precision of 96.8 % . compared with other classification models , such as multi-svm model and cnn model , albert+match-lstm had obvious advantages in classification performance in intelligent agri-tech information service .
title : is evalita done ? on the impact of prompting on the italian nlp evaluation_campaign ; abstract : prompt-based learning is a recent paradigm in nlp that leverages large pre-trained_language_models to perform a variety of tasks . with this technique , it is possible to build classifiers that do not need training_data ( zero-shot ) . in this paper , we assess the status of prompt-based learning applied to several text_classification tasks in the italian_language . the results indicate that the performance gap towards current supervised methods is still relevant . however , the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for nlp .
title : numerical irony identification : a pattern-based approach ; abstract : irony is a type of metaphorical language where the literal meaning of words ca n't hold , rather the contrary interpretation is planned in a content . this purposeful uncertainty makes irony recognition a significant job of opinion_mining . in general irony discovery is viewed as a binary classification issue wherein both rule-based and deep_learning models have been effectively worked to foresee wry remarks . these current strategies will in general focus on recognizing irony in a content . however a specific type of irony communicated through numbers stays neglected . in this manner , perceiving numerically ironic statements can be extremely valuable to improve the performance of opinion_mining of information gathered from microblogging sites or interpersonal organizations . in this paper , we proposed a pattern-based methodology for recognizing irony communicated through numerical qualities in a tweet . we dissect the difficulties of the issue and present a machine_learning way to deal with irony in numerical bits of text . four sets of highlights that cover the various sorts of irony are characterized and used to classify the tweets as ironic and non-ironic . we likewise study the significance of every one of the proposed sets of highlights and assess its additional incentive to the classification . the exploratory outcomes show that our model can clearly beat other cutting-edge strategies and further , we underline the significance of pattern-based highlights for the recognition of ironic statements expressed through numbers .
title : an ensemble technique to detect fabricated news article using machine_learning and natural_language_processing techniques ; abstract : fake_news or fabricated news , refers to false_information published under the guise of being authentic news , often to influence political views . fabricated news articles are a threat to people 's trust in the government and in effect , one of the biggest threats that modern-day democracies are facing . as the menace of fake_news is growing with each passing day , so is the research community getting more actively involved in curbing this issue . this paper reviews the current progress of the advancements done to solve the issue . the paper also presents various ensemble_techniques to perform the binary classification of news articles . additionally , the paper focuses on sources of articles to widen misclassification tolerance and make more accurate predictions . evaluation_metrics such as accuracy score , precision , recall , f-1 score have been used to make comparison among various models . the best performing model has been an ensemble of decision_tree , logistic_regression , bagging classifier used with a hard-voting_ensemble technique , which gives the accuracy of over 88 % .
title : big_data and machine_learning for evaluating machine_translation ; abstract : human evaluation of machine_translation is the most important aspect of improving accuracy of translation output which can be used for text_categorization ahead . in this article we describe approach of text_classification based on parallel_corpora and natural_language_processing techniques . a text classifier is built on multilingual texts by translating different features of the model using the expectation_maximization_algorithm . cross-lingual text_classification is the process of classifying text into different languages during translation by using training_data . the main idea underlying this mechanism is using training_data from parallel_corpus and applying classification_algorithms for reducing the distortion and alignment errors in machine_translation . in this chapter a classification model is trained which directs source_language to target_language on the basis of translation knowledge and parameters defined . the algorithm adopted here is expectation_maximization_algorithm which removes ambiguity in parallel_corpora by aligning source sentence to target sentence . it considers possible translations from source to target_language and selects the one that fits the model on the basis of bleu ( bilingual evaluation understudy ) score . the only requirement of this learning is unlabelled_data in the target_language . the algorithm can be evaluated accurately by running a separate classifier on different parallel_corpora . we use monolingual corpora and machine_translation in our study to see the effect of both the models on our parallel_corpora .
title : analyzing news sentiments and their impact on stock_market trends using pos and tf-idf based approach ; abstract : since the dawn of time , investors are looking into different schemes in determining the stock trends to earn profit . several studies have been conducted that could potentially help the investors predict the rise and fall of stocks . most of them looked into past market pricing history in order to foresee the future . while many factors influence the fluctuation of stock_market , it can be argued that the sentiments of the investors influenced by unfolding of current happenings or events has a huge impact on the stock trend . in this paper , we propose a new method in interpreting the sentiment of a given news . through a fine-grained analysis of syntactic sentence_patterns using different part of speech ( pos ) combinations , the news data inputs are preprocessed . these are then fed into term_frequency_-_inverse_document_frequency ( tf-idf ) to filter only significant text in the corpus . we then conduct_experiments using various classifiers to predict the sentiments . results are fed into k-nearest_neighbor ( k-nn ) classifier , along with historical stock price , to determine adjusted closing price over various time intervals . it can be observed that the results of proposed model are compatible with current researches stating about existing correlation between financial_news and stock prices .
title : cnn-lstm neural_network model for fine-grained negative emotion computing in emergencies ; abstract : in the web_2.0 era , governments are facing the challenge of analyzing the emotional_tendency of online public opinion during emergencies to regulate people 's emotions more effectively and maintain social stability . when dealing with large-scale short , unordered texts and extracting text features , the existing_studies often face the problem of sparse features , ignoring fine-grained negative_emotions . aiming at those drawbacks and inspired by the dependency relationship among chinese words , an emotion computing algorithm based on a binary_tree is designed to assign words with emotional intensity . then , the paper proposes a cnn-lstm model for chinese_language sentiment_classification to conduct local feature_extraction and maintain long-term dependencies . the proposed model is validated using different traditional models and classifiers . the results show that the cnn-lstm model achieved competitive classification performance . finally , our approach was applied to practical emergency_management problems , exploring the impact of government information release on negative emotion regulation to test its reliability . the experimental results validated that compared with traditional_methods , this approach improved the accuracy of sentiment_classification and possesses higher classification performance . the empirical_analysis demonstrated that the cnn-lstm method was rapid , effective and feasible and could be more suitable for optimizing emotion regulation policies .
title : classification as decoder : trading flexibility for control in medical dialogue ; abstract : generative seq2seq dialogue_systems are trained to predict the next word in dialogues that have already occurred . they can learn from large unlabeled conversation datasets , build a deeper understanding of conversational_context , and generate a wide variety of responses . this flexibility comes at the cost of control , a concerning tradeoff in doctor/patient interactions . inaccuracies , typos , or undesirable content in the training_data will be reproduced by the model at inference time . we trade a small amount of labeling_effort and some loss of response variety in exchange for quality_control . more specifically , a pretrained_language_model encodes the conversational_context , and we finetune a classification head to map an encoded conversational_context to a response class , where each class is a noisily labeled group of interchangeable responses . experts can update these exemplar responses over time as best practices change without retraining the classifier or invalidating old training_data . expert evaluation of 775 unseen doctor/patient conversations shows that only 12 % of the discriminative model 's responses are worse than the what the doctor ended up writing , compared to 18 % for the generative_model .
title : deep unfolding inference for supervised_topic_model ; abstract : conventional supervised_topic_model for multi-class classification is inferred via the variational_inference algorithm where the model parameters are estimated by maximizing the lower bound of the logarithm of marginal likelihood_function over input documents and labels . the classification_accuracy is constrained by the variational lower bound . in this study , we aim to improve the classification_accuracy by relaxing this constraint through directly maximizing the negative cross_entropy error_function via a deep unfolding inference ( dui ) . the inference procedure for class posterior is treated as the layer-wise learning in a deep_neural_network . the classification_accuracy in dui is accordingly increased by using the estimated topic parameters according to the exponentiated updates . deep_learning of supervised_topic_model is achieved through an error back-propagation algorithm . experimental results show the superiority of dui to variational_bayes inference in supervised_topic_model .
title : method of chinese text_categorization based on variable precision rough_set ; abstract : text_categorization is an important research direction of current information_retrieval . the traditional text_classification method use the support_vector_machine ( svm ) and the bayes classification algorithm ( etc ) .on the basis of rough_set on text_categorization , this paper put forward the idea of variable precision rough_set model for chinese text_categorization , which use the attribute reduct algorithm based on the importance of attributes as heuristic information to reduct the feature_subset of the text , and analyses the influence of error classification rate on text_classification it can increase the flexibility of text_categorization and improve the accuracy of text_classification by setting different value to find the the best . © 2009 ieee .
title : classification analysis of kouji uno ’ s novels using topic_model ; abstract : kouji uno is a prominent japanese littérateur , whose creative activity was subjected to disruption twice . literary critics take the view that uno ’ s writing_style underwent changes when he resumed writing . this paper aims at revealing the partition of uno ’ s creative phase using statistical_methods to conduct an investigation into the stylistic characteristics of his novels . for this purpose , a topic-model was applied to classifying uno ’ s novels and to comparing the characteristics of each group . as revealed by the results , uno ’ s novels can be classified into three groups separated approximately by the two non-productive periods and there are different stylistic characteristics displayed by novels in each group . moreover , one interesting observation is that his stylistic characteristics have changed even prior to the interruptions caused to writing . it is more reasonable that uno ’ s writing_style started to change beforethe interruptions with achievements made to some extent after the resumption .
title : mixtures of biased sentiment analysers ; abstract : modelling bias is an important consideration when dealing with inexpert annotations . we are concerned with training a classifier to perform sentiment_analysis on news_media articles , some of which have been manually_annotated by volunteers . the classifier is trained on the words in the articles and then applied to non-annotated articles . in previous work we found that a joint estimation of the annotator biases and the classifier parameters performed better than estimation of the biases followed by training of the classifier . an important question follows from this result : can the annotators be usefully clustered into either predetermined or data-driven clusters , based on their biases ? if so , such a clustering could be used to select , drop or otherwise categorise the annotators in a crowdsourcing task . this paper presents work on fitting a finite_mixture model to the annotators ' bias . we develop a model and an algorithm and demonstrate its properties on simulated data . we then demonstrate the clustering that exists in our motivating dataset , namely the analysis of potentially economically relevant news articles from irish online_news sources . ©_2013_springer-verlag_berlin_heidelberg .
title : sentiment_score analysis for opinion_mining ; abstract : sentiment_analysis has been widely used as a powerful tool in the era of predictive mining . however , combining sentiment_analysis with social_network analytics enhances the predictability power of the same . this research work attempts to provide the mining of the sentiments extracted from twitter social app for analysis of the current trending topic in india , i.e. , goods and services tax ( gst ) and its impact on different sectors of indian economy . this work is carried out to gain a bigger perspective of the current sentiment based on the live reactions and opinions of the people instead of smaller , restricted polls typically done by media corporations . a variety of classifiers are implemented to get the best possible accuracy on the dataset . a novel method is proposed to analyze the sentiment of the tweets and its impact on various sectors . further the sector trend is also analyzed through the stock_market analyses and the mapping between the two is made . furthermore , the accuracy of stated approach is compared with state of art classifiers like svm , naïve_bayes , and random_forest and the results demonstrate accuracy of stated approach outperformed all the other three techniques . also , a detailed analysis is presented in this manuscript regarding the effect of gst along with time series analysis followed by gender-wise analysis .
title : mooc video classification using natural_language_processing and machine_learning model ; abstract : mooc opens up the doors for universal access to education remotely and serves as a constructive approach to acquire formal education informally by negating the traditional practices . in recent_years , the number of mooc video resources has increased exponentially . therefore , the need is a fully_automated system that would be proficient enough to store , analyze and manage such an immensity of videos while sustaining the quality in response . an automatic classification/prediction of videos is a challenging and complex aspect , although supervised_machine_learning can effectively achieve this task in an effective way . many applications use text_classification to categorize documents like , e.g . spam_filtering , email routing , sentiment_analysis , etc . in this study , we present a clever and adaptive technique for autonomous classification of mooc videos transcription using natural_language_processing and machine_learning model . our approach can predict the category of a targeted video ; the data_mining algorithms such as svm , random_forest , and naive_bayesian will be engaged to organize the mooc videos . experiments reveal that our approach outperformed other approaches in the field of transcription classification and supervised_learning .
title : evading obscure communication from spam_emails ; abstract : spam is any form of annoying and unsought digital communication sent in bulk and may contain offensive_content feasting viruses and cyber-attacks . the voluminous increase in spam has necessitated developing more reliable and vigorous artificial_intelligence-based anti-spam filters . besides text , an email sometimes contains multimedia content such as audio , video , and images . however , text-centric email_spam filtering employing text_classification techniques remains today ’ s preferred choice . in this paper , we show that text pre-processing techniques nullify the detection of malicious contents in an obscure communication framework . we use spamassassin corpus with and without text pre-processing and examined it using machine_learning ( ml ) and deep_learning ( dl ) algorithms to classify these as ham or spam_emails . the proposed dl-based approach consistently_outperforms ml_models . in the first stage , using pre-processing techniques , the long-short-term_memory ( lstm ) model achieves the highest results of 93.46 % precision , 96.81 % recall , and 95 % f1-score . in the second stage , without using pre-processing techniques , lstm achieves the best results of 95.26 % precision , 97.18 % recall , and 96 % f1-score . results show the supremacy of dl algorithms over the standard ones in filtering spam . however , the effects are unsatisfactory for detecting encrypted communication for both forms of ml_algorithms .
title : multi-label maximum_entropy model for social emotion classification over short text ; abstract : social_media provides an opportunity for many individuals to express their emotions online . automatically classifying user emotions can help us understand the preferences of the general public , which has a number of useful applications , including sentiment retrieval and opinion_summarization . short text is prevalent on the web , especially in tweets , questions , and news headlines . most of the existing social emotion classification models focus on the detection of user emotions conveyed by long documents . in this paper , we introduce a multi-label maximum_entropy ( mme ) model for user emotion classification over short text . mme generates rich features by modeling multiple emotion labels and valence scored by numerous users jointly . to improve the robustness of the method on varied-scale corpora , we further develop a co-training algorithm for mme and use the l-bfgs algorithm for the generalized mme model . experiments on real-world short_text collections validate the effectiveness of these methods on social emotion classification over sparse features . we also demonstrate the application of generated lexicons in identifying entities and behaviors that convey different social emotions .
title : bilingual code-mixing in indian social_media texts for hindi and english ; abstract : code_mixing ( cm ) is an important in the area of natural_language_processing ( nlp ) but it is more challenging technique . there are many techniques available for code-mixing but till now less work has been done for code_mixing . in this paper we discussed the various approaches used for code_mixing and classifying existing code_mixing algorithm according to their techniques . most of people do not always use the unicode that means only one language during chatting on facebook , gmail , twitter , etc . if some people do not understand the hindi_language , then it is very difficult task for these people to understanding the meaning of code-mixedsentences . for correct hindi words we used the converter form hindi words to english words . but most of the words are not correct words according to dictionary and also the code-mixed sentences contained the short form , abbreviation words , phonetic typing , etc . so we have used the character n-gram pruning which is one of the most popular and successful technique of natural_language_processing ( nlp ) with dictionary based approaches for language identification of social_media text . this paper proposed a scheme which improve the translation by removing the phonetic typing , abbreviation words , shortcut , hindi word and emotions .
title : misogynous text_classification using svm and lstm ; abstract : discrimination and manipulation are becoming predominant in social_network activities . comments bearing attitudes , such as distress , hate , and aggression in social_networking sites ( sns ) add fuel to the process of discrimination . this research aims to classify texts , which are misogynous in nature using support_vector_machine ( svm ) and long-short_term_memory ( lstm ) for user-generated texts of english and hindi languages written using the roman script . approximately 87 % accuracy was achieved while svm was trained with term_frequency-inverse_document_frequency ( tf-idf ) feature and for hindi comments approximately 93.43 % accuracy was achieved for english using bidirectional_lstm ( bi-lstm ) .
title : attack-words guided sentence generation for textual adversarial_attack ; abstract : deep_neural_networks are vulnerable to carefully crafted adversarial_examples and many adversarial_attack methods have been proposed in computer vision tasks , such as image_classification , object_detection , etc . generating_adversarial_examples for textual tasks is more challenging since the lexical correctness , grammatical correctness and semantics similarity should be maintained . in this paper , we introduce an attack-words guided sentence generation ( agsg ) method to attack text_classification models . we first determine words ' attack ability by the ensemble strategy , then we add perturbation by inserting a short attack sentence . we conduct_extensive_experiments on two popular datasets imdb and amazon comments against textcnn , lstm and rcnn models . the results show that the agsg method greatly_reduces the classification_accuracy with a low word_substitution rate . specifically , the accuracy is reduced by 94.5 % and 90.1 % when disturbance rate is 13.3 % and 25.1 % for imdb and amazon comments respectively . the similarity evaluation study shows that our adversarial_attack method guarantees semantic similarity and grammatical correctness . compared with two baseline adversarial_attack methods , the agsg method can generate adversarial_texts that are harder for humans to perceive .
title : an analog vlsi implementation of a feature_extractor for real time optical_character_recognition ; abstract : the architecture , the design , and the analog very_large_scale_integration ( vlsi ) implementation of a feature_extractor chip for optical_character_recognition ( ocr ) systems are described . the chip extracts a set of 112 feature_values coded by current signals from a 32 × 24 digital pixel matrix , representing the input character . such features are applied to a classifier ( for example , a neural classifier ) performing the recognition task . the measurements performed on that chip confirm its functionality . the chip can be used with a segmented and nonsegmented string of characters . a throughput of about 140 kchar/s is achieved for the segmented case , while a throughput of about 450 kchar/s is achieved for the nonsegmented case . the ocr architecture has been functionally validated . a set of numerical handwritten_characters has been processed by the chip and the measured output features ( after a normalization operation ) have been used as input for neural_network classifiers implemented by a software simulator which performs the recognition task . the resulting classification error_rate ( 4.3 % ) has been successfully compared with those obtained by a high level model of this chip , and the results validate the entire architecture .
title : semantic search approach based on multi-classification semantic analysis and personalization ; abstract : to further enhance the accuracy of semantic search and improve the user_experience , a novel approach for semantic search based on multi-classification semantic analysis ( msa ) and personalization is presented . first , documents are transformed into vectors and stored in term vector database ( tvdb ) by using the modified msa method . then , documents are classified by support_vector_machine ( svm ) and wrote into index with categories . in the search process , users ' search history and personal_information are used to optimize the search_results with the help of tvdb . the experiment results show that the average precision , the average discounted cumulative gain ( dcg ) and the average normalized discounted cumulative gain ( ndcg ) otained by using this approach are 0.7 , 7.267 and 0.890 , respectively , which are 31 % , 36 % and 19 % higher than the average of the results calculated by the lucene method and the yahoo directory method . and the time complexity per query is 0.669 s , which is only 0.326 s more than that by using the lucene method . therefore , this approach can improve the relevance and precision of semantic search with a rational time cost .
title : an information theoretic approach to detection of minority subsets ; abstract : unsupervised_learning techniques , e.g . clustering , is useful for obtaining a summary of a dataset . however , its application to large databases can be computationally_expensive . alternatively , useful information can also be retrieved from its subsets in a more efficient yet effective manner . this paper addresses the problem of finding a small subset of minority instances whose distribution significantly differs from that of the majority . generally , such a subset can substantially overlap with the majority , which is problematic for conventional estimation of distribution . this paper proposes a new approach for estimating a minority distribution based on information theoretic framework , an extension of the rate distortion theory for unsupervised_learning tasks . specifically , the proposed method ( a ) estimates parameters which maximize the divergence between the minority and majority distributions , ( b ) penalizes the redundancy of data expression based on the mutual_information between the observed and hidden variables , and ( c ) employs a hard assignment approximation to avoid computation of trivial conditional_probabilities . the algorithm of the proposed method has no problem-dependent parameter and its time and space complexities are linear to the size of the minority subset . experiments using artificial datasets show the proposed method yields significantly high precision and sensitivity in detecting minority subsets which substantially overlaps with the majority . the proposed method also substantially_outperforms one-class classification and mixture estimation methods in real-world benchmark_datasets for text and satellite_imagery classification .
title : multi-classification cluster_analysis of large data based on knowledge element in microblogging short text ; abstract : in order to improve performance of personalized label recommendation , the recommendation algorithm of implied relation topic_model ( rtm ) for microblog personalized label based on gibbs_sampling inference is proposed . firstly , imaging form is used to express potential local information in microblog and to conduct top-k similar user discovery for users represented as user topic_distribution , and then the frequency of all labels in these users is calculated to recommend the label mostly related to the users . secondly , to dig potential topic information , enhancement cosine_similarity rtm model with penalty term is used to name the microblog label , which greatly_improves the influence of joint_modeling on potential topic generation label , and the relationship between overall label and topic can be found ; finally , it can be seen from real experimental result that the proposed recommendation method is superior to the selected tf–idf , rtmsa and other classic label recommendation algorithm , so as to verify effectiveness of the algorithm .
title : a new sentiment_classification method based on hybrid classification in twitter ; abstract : social_media including twitter create a space for expression and dissemination of thoughts and opinions on various topics and various events and they have created opportunity to apply theories and technology leading to search and explore the trends . mining stream data needs to be balanced in three different branches : accuracy , time and memory . the optimal accuracy_rate has obtained in the stream data using stochastic_gradient_descent algorithm . in this paper , we show that replacing the stochastic_gradient_descent algorithm in tree leaves causes improvement and reliability of the forecast . the proposed algorithm , hoeffding stochastic_gradient_descent , has not changed the time while increasing the accuracy . studies on the tweets that have been randomly_selected shows that the proposed algorithm outperforms when assessing the sentiment stream data .
title : classification of short texts based on nld-svm-rf model ; abstract : [ objective ] this paper addresses the issue of data sparseness due to short texts , which also improves the performance of short texts classification . [ methods ] we proposed a multi-channel text model for the input of short text classifier by integrating the semantics , word_order features and topic features . then , we created the classification method named nld-svm-rf with the help of svm and random_forest algorithms . finally , we examined the new model with short text of complaints . [ results ] we compared the performance of our new model with the svm and rf single classifiers using doc2vec as the feature . when n =5 , the accuracy of the nld-svmrf method increased by 9 . 70 % and 6 . 25 % , respectively . [ limitations ] the experimental data size needs to be expanded . [ conclusions ] the nld-svm-rf model provides a practical solution for the business community to analyse short texts and improve decision-making .
title : classification of dialogue_acts using prosodic_features in chinese spontaneous_speech ; abstract : dialogue_acts ( das ) indicate the meanings of the utterances at the aspect of speech_act , e.g . whether an utterance is a question , statement , confirmation and so forth , which helps to understand the utterances . the understanding component of a dialogue system can be improved by knowing the dialogue_acts of utterances . in this paper , we investigate the classification of the dialogue_acts using prosodic_features in easyflight , a chinese spoken_dialogue system for querying and booking flight tickets . the experiments are based on the domain-specified chinese spontaneous_speech corpus , which is collected for evaluating the performance of the dialogue system . dialogue_acts are labeled by hand . according to more than thirty prosodic_features two types of das , question and statement are classified in the experiments , and the classification_accuracy of 83 % is obtained .
title : crossoie : cross-lingual classifier for open_information_extraction ; abstract : open_information_extraction ( open ie ) is the task of extracting open-domain assertions from natural_language sentences . considering the low availability of datasets and tools for this task in languages other than english , recently it has been proposed that multilingual resources can be used to improve open ie methods for different languages . in this work , we present the crossoie , a multilingual publicly available relation tuple validity classifier that scores open ie systems ’ extractions based on their estimated quality and can be used to improve open ie systems and assist in the creation of open ie benchmarks for different languages . experiments show that our model trained using a small corpus in english , spanish , and portuguese can trade recall performance for up to 27 % improvement in precision . this result was also archived in a zero-shot scenario , demonstrating a successful knowledge_transfer across the languages .
title : monitoring mental_health using smart_devices with text analytical tool ; abstract : the emerging of information_technology integrated with healthy human lifestyle has been contributing to improving the overall quality of life . smart_devices have facilitated the digital transformation in different domains , primarily , in the health sector for the fields of management , diagnosis , and treatments . the aim of maintaining mental_health is to improve human productivity and social functionality . this paper intends to study the benefits of using an intelligent application that uses a text analytical tool to support mental_health . it uses innovative sensors and different technologies that are built-in smart_devices . it detects anxiety and depression using the camera sensors , and by performing self-testing scales . it is a user-friendly platform providing simple bits of advice , animated breath exercises , and online text-based therapy with registered psychologists supported by the text analytical tool . we tested different machine_learning classifications , and the svm selected showed the best performance with a score of 79.81 % in the text analytical tool .
title : comparison between decision-level and feature-level_fusion of acoustic and linguistic features for spontaneous emotion recognition ; abstract : detection of affective states in speech could improve the way users interact with electronic devices . however the analysis of speech at the acoustic level could be not enough to determine the emotion of a user speaking in a realistic scenario . in this paper we analysed the spontaneous_speech recordings of the fau aibo corpus at the acoustic and linguistic levels to extract two sets of acoustic and linguistic features . the acoustic set was reduced by a greedy procedure selecting the most relevant features to optimize the learning stage . we experimented with three classification approaches : naïve-bayes , a support_vector_machine and a logistic model tree , and two fusion schemes : decision-level fusion , merging the hard-decisions of the acoustic and linguistic classifiers by means of a decision_tree ; and feature-level_fusion , concatenating both sets of features before the learning stage . despite the low performance achieved by the linguistic data , a dramatic improvement was achieved after its combination with the acoustic information , improving the results achieved by this second modality on its own . the results achieved by the classifiers using the parameters merged at feature level outperformed the classification results of the decision-level fusion scheme , despite the simplicity of the scheme . © 2012 aisti .
title : interactive intelligent_agent architecture ; abstract : this paper proposes a conversational creature model . it is based on multi-agent subsumption_architecture . behaviours agents are represented by classifiers . we use an ethological hierarchy of agents from reflexive agent to cognitive agent . a strategic agent is also developed to control the whole dialogue , into a proactive architecture . to make this conversational creature evolve , we use a classifier system based on lisp-like s-expressions . © 2006 ieee .
title : is machine_translation ripe for cross-lingual sentiment_classification ? ; abstract : recent advances in machine_translation ( mt ) have brought forth a new paradigm for building nlp applications in low-resource scenarios . to build a sentiment classifier for a language with no labeled resources , one can translate labeled_data from another language , then train a classifier on the translated text . this can be viewed as a domain_adaptation problem , where labeled translations and test data have some mismatch . various prior work have achieved positive results using this approach . in this opinion piece , we take a step back and make some general statements about cross-lingual adaptation problems . first , we claim that domain mismatch is not caused by mt errors , and accuracy degradation will occur even in the case of perfect mt . second , we argue that the cross-lingual adaptation problem is qualitatively different from other ( monolingual ) adaptation problems in nlp ; thus new adaptation algorithms ought to be considered . this paper will describe a series of carefully-designed experiments that led us to these conclusions . © 2011 association_for_computational_linguistics .
title : a linguistic system for predicting sentiment in arabic tweets ; abstract : the term sentiment_analysis is considered very important in our current era , especially with widespread of social_media , as it helps understand people 's feelings , behavior and opinions about a specific behavior or entity , individuals , organizations and any related topic . recently , with the development of machine_learning , there have been many studies concerned with analyzing feelings . still , most of these researches are concerned with the english_language more than other languages . this paper proposes a model for working with standard_arabic and some other arabic dialects such as levantine , egyptian , and gulf . working with the arabic_language poses several challenges due to the complex structure of the language , the large number of dialects used and the lack of associated resources . the data collected was divided into positive , negative , and neutral . several algorithms were used to predict sentiment in arabic texts such as naive_bayes classifiers ( nb ) , support_vector_machine ( svm ) , random_forest classifier , and bert model ( bidirectional_encoder_representations_from_transformers ) . the results obtained are very encouraging , especially with the bert model ( bidirectional_encoder_representations_from_transformers ) that gave very accurate results during the test , reaching more than 83 % .
title : key-phrase based classification of public_health web_pages ; abstract : this paper describes and evaluates the public_health web_pages classification model based on key_phrase_extraction and matching . easily extendible both in terms of new classes as well as the new language this method proves to be a good solution for text_classification faced with the total lack of training_data . to evaluate the proposed solution we have used a small collection of public_health related web_pages created by a double blind manual classification . our experiments have shown that by choosing the adequate threshold value the desired value for either precision or recall can be achieved . © 2013 imia and ios press .
title : a hybrid approach for web_pages classification ; abstract : currently , the internet is growing at an exponential_rate and can cover just some required data . however , the immense amount of web_pages makes the discovery of the target data more difficult for the user . therefore , an efficient method to classify this huge amount of data is essential where web_pages can be exploited to their full potential . in this paper , we propose an approach to classify web_pages based on their textual_content . this approach is based on an unsupervised statistical technique ( tf-idf ) for keyword_extraction ( textual_content ) combined with a supervised_machine_learning approach , namely recurrent_neural_networks .
title : sentiment_analysis of insomnia-related_tweets via a combination of transformers using dempster-shafer theory : pre– and peri–covid-19_pandemic retrospective study ; abstract : background : the covid-19_pandemic has imposed additional stress on population health that may result in a change of sleeping behavior . objective : in this study , we hypothesized that using natural_language_processing to explore social_media would help with assessing the mental_health conditions of people experiencing insomnia after the outbreak of covid-19 . methods : we designed a retrospective study that used public social_media content from twitter . we categorized insomnia-related_tweets based on time , using the following two intervals : the prepandemic ( january 1 , 2019 , to january 1 , 2020 ) and peripandemic ( january 1 , 2020 , to january 1 , 2021 ) intervals . we performed a sentiment_analysis by using pretrained transformers in conjunction with dempster-shafer theory ( dst ) to classify the polarity of emotions as positive , negative , and neutral . we validated the proposed pipeline on 300 annotated_tweets . additionally , we performed a temporal analysis to examine the effect of time on twitter users ’ insomnia experiences , using logistic_regression . results : we extracted 305,321 tweets containing the word insomnia ( prepandemic tweets : n=139,561 ; peripandemic tweets : n=165,760 ) . the best combination of pretrained transformers ( combined via dst ) yielded 84 % accuracy . by using this pipeline , we found that the odds of posting negative tweets ( odds_ratio [ or ] 1.39 , 95 % ci 1.37-1.41 ; p < .001 ) were higher in the peripandemic interval compared to those in the prepandemic interval . the likelihood of posting negative tweets after midnight was 21 % higher than that before midnight ( or 1.21 , 95 % ci 1.19-1.23 ; p < .001 ) . in the prepandemic interval , while the odds of posting negative tweets were 2 % higher after midnight compared to those before midnight ( or 1.02 , 95 % ci 1.00-1.07 ; p=.008 ) , they were 43 % higher ( or 1.43 , 95 % ci 1.40-1.46 ; p < .001 ) in the peripandemic interval . conclusions : the proposed novel sentiment_analysis pipeline , which combines pretrained transformers via dst , is capable of classifying the emotions and sentiments of insomnia-related_tweets . twitter users shared more negative tweets about insomnia in the peripandemic interval than in the prepandemic interval . future_studies using a natural_language_processing framework could assess tweets about other types of psychological distress , habit changes , weight_gain resulting from inactivity , and the effect of viral_infection on sleep .
title : a novel rs attribute_reduction technique for chinese classification in cqa ; abstract : with the rapid development of the question and answer services which are based on community , like sina iask , baidu zhidao and yahoo ! answers , the community-based question_answering , cqa , service has become a new knowledge-sharing model , which has characteristic of interactivity , openness etc . increasing more people use the services which are provided on these sites to meet the information needs . in order to accurately understand the user ’ s query and provide useful information , it is necessary to deal with the questions in the community , and the question_classification in the cqa is the key component in this step . however , the difficulty of the question_classification is high-dimensional feature_vector , which usually uses the feature_selection as the primary method of dimensionality_reduction , and in this paper , a combined extract features method is presented to screen the question features for the first time to obtain a feature_subset . then the importance and dependence of the rough_set is used as the heuristic information for the feature_selection to further screen the useful features . experiments show that the algorithm is effective , and it not only make the question of feature dimensions reduced to some extent , but also improve the classification_accuracy of the question .
title : research on word-level contextual paraphrase retrieving with five-features ; abstract : to solve the weakness of chinese synonym dictionary tongyici-cilin 's , which ca n't be used as a context-dependent paraphrase_corpus , a word-level paraphrase method was presented to improved the chinese paraphrase extraction accuracy . based on its contextual sentence , the target word 's paraphrase candidates were identified and extracted from large-size corpuses . the target word was then paired up with each candidate , and a five-feature probability model captured the information of the target word , the context sentence , and the paraphrase candidates were established . values of those five features were inputted to train a binary classifier which subsequently filtered out the paraphrase candidates . the experiment proved that through data mining the method for retrieving candidate paraphrases from large-size corpuses had pragmatic value , and on average 3.1 correct paraphrases were obtained for a word . binary classifier was efficient in filtering out the paraphrases , with an accuracy_rate of 0.65.32 % of the retrieved paraphrases could not be found in the expanded chinese synonym dictionary .
title : a unified approach to discourse_relation_classification in nine languages ; abstract : this paper presents efforts to solve the shared_task on discourse_relation_classification ( disrpt task 3 ) . the intricate prediction task aims to predict a large number of classes from the rhetorical structure theory ( rst ) framework for nine target languages . labels include discourse_relations such as background , condition , contrast and elaboration . we present an approach using euclidean distance between sentence_embeddings that were extracted using multlingual sentence bert ( sbert ) and directionality as features . the data was combined into five classes which were used for initial prediction . the second classification step predicts the target classes . we observe a substantial difference in results depending on the number of occurrences of the target label in the training_data . we achieve the best results on chinese , where our system achieves 70 % accuracy on 20 labels .
title : keyword_extraction algorithms for emotion recognition from uyghur text ; abstract : this paper describes sentiment_classification research on uyghur text using different keyword_extraction methods to recognize common emotions like anger and happiness . the keywords expressing happiness and anger are extracted using the textrank , sparse discriminant analysis ( sda ) and sparse support_vector_machine ( sparse svm ) methods to train feature_extraction and sentiment models . a sentiment text database was built by excerpting the anger and happiness sentiments from uyghur movies and novels with several validation experiments based on those text databases . the tests show that the keyword_extraction methods presented in this paper are effective for emotion classification from uyghur sentences . the sparse svm method is robustness and has higher_accuracy in recognition tests with a smaller number of keywords extracted .
title : combining lexical and syntactic features for detecting content-dense texts in news ; abstract : content-dense news report important factual information about an event in direct , succinct manner . information seeking applications such as information_extraction , question_answering and summarization normally assume all text they deal with is content-dense . here we empirically test this assumption on news articles from the business , u.s. international_relations , sports and science_journalism domains . our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector . we heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features . on manually_annotated data , we compare the performance of domain-specific classifiers , trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together . our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label . domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer . domain_independent classifiers reproduce better naive crowdsourced judgements . classification prediction is high across all conditions , around 80 % .
title : deep_learning based network news text_classification system ; abstract : today , text information includes all the information in the form of natural_language text , among which text information occupies an important position in life and becomes an important part of people 's use of social information resources . the main purpose of this paper is to study a deep_learning-based online message classification system . based on deep_learning and related text segmentation theory , this paper proposes modules such as segmentation result evaluation for the first time . then , the first implementation and experimental results of network message segmentation are introduced and analyzed in depth . the research shows that text_classification is a multidisciplinary field . among all the literatures in the statistics , computer science has the largest number of related books in the field of text_classification , with a total of 1827 books published , accounting for 87.88 % of the total . in addition , it can be seen from the topic_distribution map that the vocabulary has many applications and development prospects in education , news_broadcasting , business management , information dissemination , mathematics and other fields .
title : robust document_representations using latent_topics and metadata ; abstract : task_specific fine-tuning of a pre-trained neural language model using a custom softmax output_layer is the de facto approach of late when dealing with document_classification problems . this technique is not adequate when labeled_examples are not available at training time and when the metadata artifacts in a document must be exploited . we address these challenges by generating document_representations that capture both text and metadata artifacts in a task agnostic manner . instead of traditional auto-regressive or auto-encoding based training , our novel self-supervised approach learns a soft-partition of the input_space when generating text embeddings . specifically , we employ a pre-learned topic_model distribution as surrogate labels and construct a loss_function based on kl divergence . our solution also incorporates metadata explicitly rather than just augmenting them with text . the generated document_embeddings exhibit compositional characteristics and are directly used by downstream_classification_tasks to create decision boundaries from a small number of labeled_examples , thereby eschewing complicated recognition methods . we demonstrate through extensive_evaluation that our proposed cross-model fusion solution outperforms several competitive_baselines on multiple datasets .
title : evaluation of service_quality for express industry through sentiment_analysis of online_reviews ; abstract : the evaluation method based on questionnaire survey has limited respondents with low_quality , therefore , an evaluation method of express service_quality was put forward in this paper through sentiment_analysis of massive online_reviews by selecting online_reviews of express companies sf and st from dianping.com for experiment analysis . first , the servqual model of service_quality evaluation and the related theory of quality_evaluation of logistics service was applied combined with text analysis , to establish an index system of quality_evaluation of express service through sentiment_analysis . then online_reviews such as capturing , phrasing and marking were preprocessed , and the recall_ratio and precision ratio under different feature_selection algorithm and different classification algorithm were compared . 614 features were extracted and the useful text were identified by choose ig and svm as the best combination . furthermore , the polarity analysis and strength analysis of online_reviews were conducted based on semantic_similarity calculation of hownet and adverb classification method . finally , tf-idf with the weight of evaluation_index was applied to evaluate the express service_quality . compared with the rating scores from dianping.com , the experimental results indicate that the proposed method can effectively and better compare differences between sf and st in each express service_quality index . besides , the general evaluation scores are in alignment with dianping.com .
title : sinica_bow : integrating bilingual wordnet and sumo ontology ; abstract : starting_point : the lexicon lexicons can perform the bridging function between documents and conceptual categorisation ( calzolari , this panel ) . this position is motivated by both language engineering concerns as well as psychological felicity . language engineering adopts linguistic models where a word is the unified atom for both form and meaning . psycho- and neurc-linguistics , on the other hand , assumes the paradigm where word_forms are concrete units that are manipulated for conceptual access . in addition , when the issues and needs of multi-linguality are taken into consideration , it becomes obvious that the lexicon is the only level where generalizations as well as variations across different languages can be captured efficiently and comprehensively . in this talk , we report our preliminary work on integrating a lexical structure such that the linguistic-to-conceptual_representation and language-to-language gaps can be bridged simultaneously . sinica_bow the sinica_bow ( academia_sinica bilingual ontological wordnet ) is intended as a linguistic infrastructure for knowledge_representation and knowledge_engineering . it is built upon the relation-based structure of wordnet . on one hand , a bilingual wordnet is constructed with ure crucial design feature of treating bilingual translation correspondences as lexical_semantic relations [ 1 ] . on the other hand , sumo ( suggested upper merged ontology ) is adopted as thee shared system of conceptual_categorization [ 2 ] . sumo is also one of the first conceptual_categorization systems to be mapped to an english lexicon [ 3 ] , since sumo is mapped to wordnet 1.6 , the english wordnet has become the cornerstone for linking across languages and between a language and its conceptual system . in addition , domain tags are assigned to lemmas when necessary in order to ensure domain inter-operability . by the combination of ontology and wordnet , we hope that sinica_bow will 1 ) give each linguistic form a rigorous conceptual location , 2 ) clarify the relation between conceptual classification and linguistic instantiation , and 3 ) facilitate genuine cross_-lingual access of knowledge . the sinica_bow allows lexical searches in either language to return ontological information ( in either language ) . searches on sinica_bow can return the following iriforination : sense-based english-chinese translation equivalency , english word-sense-based ontology and inference , chinese word-based ontology and inference , word-sense-based domain specification ( under construction ) . in addition to the integration of wordnet and ontology , it is also an important goal of sinica_bow to integrate lexical_resources . sinica_bow 's design is lemma-driven , a lexical_database of word_forms is first compiled by integrating multiple lexical_resources . this becomes the central database for lexical management for sinica_bow . making use of this lexical_database , a lexical search may link to either the main bow knowledgebase or any of the corresponding entries in an online lexicon . hie multilingual and cross-domain properties of ( semantic ) relations in addition to relying on lemmas as retrieval keys , a crucial step in establishing synergy between language and knowledge resources is to identify the conceptual atoms that apply equally effectively to knowledge and language resources . lexical_semantic relations are exactly , such a set of atoms . sinica_bow implements this idea by encoding the lexical_semantic relations between english-chinese translation equivalent pairs . in addition to more precisely describing the relationship between two translation equivalents , this also allows better cross-lingual inferences . explicitly allowing lexical_semantic relations to be coded cross-lingually also will facilitate the transferring to a structured set of tree relations from one language to the other .
title : stacked residual recurrent_neural_networks with cross-layer attention for text_classification ; abstract : text_classification is a fundamental task in natural_language_processing and is essential for many tasks like sentiment_analysis and question_classification etc . as we all know , different nlp tasks require different linguistic features . tasks such as text_classification requires more semantic features than other tasks such as dependency_parsing requiring more syntactic features . most existing_methods focus on improving performance by mixing and calibrating features , without distinguishing the types of features and corresponding effects . in this paper , we propose a stacked residual recurrent_neural_networks with cross-layer attention model to filter more semantic features for text_classification , which named srcla . firstly , we build a stacked network structure to filter different types of linguistic features , and then propose a novel cross-layer attention_mechanism that exploits higher-level features to supervise the lower-level features to refine the filtering process . based on this , more semantic features can be selected for text_classification . we conduct_experiments on eight text_classification tasks , including sentiment_analysis , question_classification and subjectivity classification and compare with a broad range of baselines . experimental results show that the proposed approaches achieve the state-of-the-art results on 5 out of 8 tasks .
title : an enhanced sentiment_analysis framework based on pre-trained_word_embedding ; abstract : sentiment_analysis ( sa ) is a technique that lets people in different fields such as business , economy , research , government , and politics to know about people 's opinions , which greatly affects the process of decision-making . sa techniques are classified into : lexicon-based techniques , machine_learning techniques , and a hybrid between both approaches . each approach has its limitations and drawbacks , the machine_learning approach depends on manual feature_extraction , lexicon-based approach relies on sentiment_lexicons that are usually unscalable , unreliable , and manually_annotated by human experts . nowadays , word-embedding techniques have been commonly used in sa classification . currently , word2vec and glove are some of the most accurate and usable word_embedding techniques , which can transform words into meaningful semantic vectors . however , these techniques ignore sentiment information of texts and require a huge corpus of texts for training and generating accurate vectors , which are used as inputs of deep_learning models . in this paper , we propose an enhanced ensemble_classifier framework . our framework is based on our previously_published lexicon-based method , bag-of-words , and pre-trained_word_embedding , first the sentence is preprocessed by removing_stop-words , pos_tagging , stemming and lemmatization , shortening exaggerated word . second , the processed sentence is passed to three modules , our previous lexicon-based method ( sum votes ) , bag-of-words module and semantic module ( word2vec and glove ) and produced feature_vectors . finally , the previous features vectors are fed into 11 different classifiers . the proposed framework is tested and evaluated over four datasets with five different lexicons , the experiment results show that our proposed model outperforms the previous lexicon based and the machine_learning methods individually .
title : integration of n-gram language models in multiple classifier systems for offline_handwritten text_line recognition ; abstract : current multiple classifier systems for unconstrained handwritten_text_recognition do not provide a straightforward way to utilize language model information . in this paper , we describe a generic method to integrate a statistical n-gram language model into the combination of multiple offline_handwritten text_line recognizers . the proposed method first builds a word transition network and then rescores this network with an n-gram language model . experimental evaluation conducted on a large dataset of offline_handwritten text_lines shows that the proposed approach improves the recognition_accuracy over a reference system as well as over the original combination method that does not include a language model . © 2008 world_scientific_publishing_company .
title : recognition of animal drug pathogenicity named_entity based on att-aux-bert-bilstm-crf ; abstract : in order to solve the problems that traditional_methods of veterinary drug named_entity_recognition rely on artificial design features , which is time-consuming and labor-consuming , and the amount of veterinary drug pathogenic corpus data is less in the process of building veterinary drug pathogenic knowledge_graph , a method based on att-aux-bert-bilstm-crf of veterinary drug text named_entity_recognition model was proposed , which combined bert-bilstm-crf models by introducing attention_mechanism and auxiliary classification layer.the text was vectorized by the bert preprocessing model , and then connected to bi-directional_long-short_term_memory network.the auxiliary classification mechanism was introduced , the output of the bert layer was used as the auxiliary classification layer , and the output of the bilstm_layer was used as the main classification layer . the attention_mechanism was proposed to combine auxiliary classification layer with main classification layer to improve the overall performance.finally , it was sent to conditional_random_field to construct an end-to-end deep_learning model framework suitable for veterinary drug name entity recognition.in the experiment , totally 10 643 sentences and 485 711 characters of veterinary drug text were selected to identify four kinds of entities : drug , adverse effect , intake mode , aimal . the results showed that the model can effectively identify the entities in the veterinary drug pathogenic text , and the f1 value of recognition was 96.7 % .
title : making sense of large_volumes of unstructured email responses ; abstract : unstructured_textual_data has increased to unprecedented levels with the rapid spread of user_generated content via social_media . handling large_volumes of unstructured_text data is a challenging task that is increasingly becoming needed in a variety of situations . reading and extracting important information from large collections of text manually is impractical and often impossible . this research concerns making sense of 500,000 email submissions received in response to a debate on net_neutrality in india . the large_volume of html format data was preprocessed using natural_language_processing ( nlp ) before analyzing it . topic_modeling and sentiment_analysis were used to explore the nature and scope of the submissions so received , and provided a framework for dealing with this increasingly recurring problem . interesting topics were identified from the submissions by using the latent_dirichlet_allocation ( lda ) topic_model , which included themes such as the speed of internet , security , privacy and internet_traffic among others . word clouds were then generated to visualize key_concepts within these email responses . a lexicon-based approach was used to classify the sentiment_polarity of responses as positive , negative and neutral . two methods were used to classify polarity of responses . in the first method , negative and positive word_lists were used to classify polarity . there was considerable accuracy in the classification results . in the second method , the sentiwordnet lexical_resource was used for classification and resulted in improved performance .
title : cross-domain_sentiment_classification with bidirectional contextualized transformer language models ; abstract : cross-domain_sentiment_classification is an important natural_language_processing ( nlp ) task that aims at leveraging knowledge obtained from a source_domain to train a high-performance learner for sentiment_classification on a target domain . existing transfer_learning methods applied on cross-domain_sentiment_classification mostly focus on inducing a low-dimensional feature_representation shared across domains based on pivots and non-pivots , which is still a low-level representation of sequence data . recently , there have been great_progress in the nlp literature in developing high-level representation language models based on transformer architecture , which are pre-trained on large text_corpus and fine-tuned for specific task with an additional layer on top . among such language models , the bidirectional contextualized transformer language models of bert and xlnet have greatly impacted nlp research field . in this paper , we fine-tune bert and xlnet for the cross-domain_sentiment_classification . we then explore their transferability in the context of cross-domain_sentiment_classification through in-depth analysis of two models ' performances and update the state-of-the-arts with a significant margin of improvement . our results show that such bidirectional contextualized language models outperform the previous state-of-the-arts methods for cross-domain_sentiment_classification while using up to 120 times less data .
title : how the classification of mansi dialects was changed ( on the material of the first cyrillic books and dictionaries of the 18th and 19th centuries ) ; abstract : the first books and dictionaries written in mansi dialects in the 18th and 19th centuries that were never described before have been found in the archive of the russian academy of sciences ( st. petersburg branch ) and the national_library of finland . this paper presents the data on six dialect features identified by l. honti in seven 18th-century dictionaries and three 19th-century books . these archival sources show that the proto-mansi phonemes to be differentiated in the 19th century were usually realized by doublet archaic and innovative reflexes in the 18th century . apparently , there were no clearly distinct dialect differences between the northern and western mansi dialects in the 18th century . this situation changed in the middle of the 19th century when the proto-mansi * ā became o in all the southern , eastern , and northern dialects under discussion . proto-mansi * ( * a according to the reconstruction proposed ) has no doublets and is reflected as a in all the dialects . the reflexes of the proto-mansi * and * š make it possible to differentiate the eastern and , presumably , northern and southern subdialects of the tobolsk province . the reflex χ- of the proto-mansi * k- before back vowels appears not only in the northern and eastern dialects but also in , presumably , the southern dialects . thus , the study shows that the vowel phonemes had practically no doublet reflexes in the 19th century and coincided in the four dialects examined . the consonant phonemes , on the other hand , make it possible to differentiate between the southern , northern , and eastern dialects .
title : measuring the validity of algorithms to automatically categorize facebook content ; abstract : in this experience report , we experiment with a text_classification pipeline , attempting to validate a model that will automatically_classify texts culled from facebook . we performed 10-fold_cross-validation on the training_set using a variety of parameter_values . for each fold we used 9/10 of the training_data to train a model and then tested it with the remaining 1/10 of the data , repeating 10 times and averaging the performance . we offer four recommendations for researchers facing similar challenges in semi-automating text extraction and classification .
title : classification method comparison on indonesian social_media sentiment_analysis ; abstract : sentiment_analysis from social_media has turned out to be essential since individuals are normally genuine with their sentiment on giving their perspective . however , in turning social_media into a sentiment_analysis possess challenges such as comments are usually ambiguous , language_barrier problem , slang words , redundant comment , and sentiment_classification . this study attempted to distinguish the issues of sentiment_classification from indonesian social_media on jakarta governor election . several steps are taken to overcome those problems that include preprocessing . the preprocessing strategy used are removing the unrelated tweet , removing url , deleting duplicate lines , deleting similar lines , removing the unrelated word , removing hashtag , removing twitter username , removing number in the comment , removing punctuation , checking slang words , and converting the slang word into appropriate word . the preprocessed sentiment is then classified into positive , negative , and neutral . the classification method used in this study are summation method , average on tweet , average on tweet with the threshold on objective score , weighted_average , and naïve_bayes method . the experimental results show that naïve_bayes produce the highest precision , highest recall , and highest_accuracy for neutral and positive_sentiment . but , naïve_bayes does not produce good results for negative_sentiment .
title : improving question_classification with hybrid networks ; abstract : question_classification is a basic work in natural_language_processing , which has an important influence on question_answering . due to question sentences are complicated in many specific_domains contain a large number of exclusive vocabulary , question_classification becomes more difficult in these fields . to address the specific challenge , in this paper , we propose a novel hierarchical hybrid deep network for question_classification . specifically , we first take advantages of word2vec and a synonym dictionary to learn the distributed_representations of words . then , we exploit bi-directional_long_short-term_memory networks to obtain the latent_semantic representations of question sentences . finally , we utilize convolutional_neural_networks to extract question sentence features and obtain the classification results by a fully-connected network . besides , at the beginning of the model , we leverage the self-attention_layer to capture more useful features between words , such as potential relationships , etc . experimental results show that our model outperforms common classifiers such as svm and cnn . our approach achieves up to 9.37 % average accuracy improvements over baseline method across our agricultural dataset .
title : cross-modal complementary network with hierarchical fusion for multimodal_sentiment_classification ; abstract : multimodal_sentiment_classification ( msc ) uses multimodal data , such as images and texts , to identify the users ' sentiment_polarities from the information posted by users on the internet . msc has attracted_considerable attention because of its wide applications in social_computing and opinion_mining . however , improper correlation strategies can cause erroneous fusion as the texts and the images that are unrelated to each other may integrate . moreover , simply concatenating them modal by modal , even with true correlation , can not fully capture the features within and between modals . to solve these problems , this paper proposes a cross-modal complementary network ( cmcn ) with hierarchical fusion for msc . the cmcn is designed as a hierarchical_structure with three key modules , namely , the feature_extraction module to extract features from texts and images , the feature attention_module to learn both text and image attention features generated by an image-text correlation generator , and the cross-modal hierarchical fusion module to fuse features within and between modals . such a cmcn provides a hierarchical fusion framework that can fully integrate different modal features and helps reduce the risk of integrating unrelated modal features . extensive experimental results on three public datasets show that the proposed approach significantly_outperforms the state-of-the-art methods .
title : context_free frequently_asked_questions detection using machine_learning techniques ; abstract : faqs are the lists of common questions and answers on particular topics . today one can find them in almost all web_sites on the internet and they can be a great tool to give information to the users . questions in faqs are usually identified by the site administrators on the basis of the questions that are asked by their users . while such questions can respond to required information about a service , topic , or particular subject , they can not easily be distinguished from non-faq questions . this paper describes machine_learning based parsing and question_classification for faqs . we demonstrate that questions for faqs can be distinguished from other types of questions . identification of specific features is the key to obtaining an accurate faq classifier . we propose a simple yet effective feature_set including bag of words , lexical , syntactical , and semantic features . to evaluate our proposed methods , we gathered a large data set of faqs in three different contexts , which were labeled by humans from real data . we showed that the svm and naive_bayes reach the accuracy of 80.3 % , which is an outstanding result for the early stage research on faq classification . experimental results show that the proposed approach can be a practical tool for question_answering systems . to evaluate the accuracy of our classifier we have conducted an evaluation process and built the questionnaire . therefore , we compared our classifier ranked questions with user rates and almost 81 % similarity of the question ratings gives some confidence .
title : impact of instance_selection on knn-based text_categorization ; abstract : with the increasing use of the internet and electronic documents , automatic_text_categorization becomes imperative . several machine_learning algorithms have been proposed for text_categorization . the k-nearest_neighbor algorithm ( knn ) is known to be one of the best state of the art classifiers when used for text_categorization . however , knn suffers from limitations such as high computation when classifying new instances . instance_selection techniques have emerged as highly competitive methods to improve knn through data reduction . however previous_works have evaluated those approaches only on structured datasets . in addition , their performance has not been examined over the text_categorization domain where the dimensionality and size of the dataset is very high . motivated by these observations , this paper investigates and analyzes the impact of instance_selection on knn-based text_categorization in terms of various aspects such as classification_accuracy , classification efficiency , and data reduction .
title : focus location extraction from political news reports with bias correction ; abstract : automatic identification of geolocation mentioned in online_news articles provide vital information for understanding associated events . while numerous open-source and commercial tools exist for geolocation extraction , they lack in reliable identification of fine-grained location , i.e. , they identify location at country-level rather than a fine-grained city or locality level . the problem of location identification has been widely_studied . yet , most techniques depend on external_knowledge-base or view the problem only in terms of named_entity_recognition ( ner ) , only to identify country-level location_information . in this paper , we focus on news articles describing an event . a set of locations directly associated with the event are called focus locations . however , an event can occur only at a single location . therefore , we aim to extract this location among focus locations , and call this as primary_focus location . we propose a mechanism that utilizes the named_entities to identify potential sentences containing focus locations , and then employ a supervised classification mechanism over sentence_embedding to predict the primary focused geolocation . however , the main issue with such an approach is the unavailability of ground_truth ( i.e. , whether words in a sentence is focus or non-focus ) for training a classifier . in practice , labels from only a small number of news articles may be available for training due to high cost of manual_labeling . if these articles are not a good representation of news articles in the wild , the classifier may not perform well . therefore , we utilize an adaptation mechanism to overcome sampling_bias in training_data . particularly , we train a classifier by using bias-corrected training_data obtained from news_articles published by an agency , while testing it on news_articles published by a different agency . our empirical results show superior performance compared to baseline approaches on real-world datasets consisting of news articles .
title : a comprehensive framework for ontology based classifier using unstructured_data ; abstract : the knowledge contained within the natural_language data can be used to build expert_systems . classifying unstructured_data using ontology and text_classification algorithms to extract information is one way of approaching the problem of building intelligent_systems . one major problem with text processing is most data generated is unstructured and ambiguous , as , data with a structure helps to identify meaningful patterns and eventually exhibit the latent knowledge . ambiguity in natural_language affects accuracy of categorization . also , natural_language_processing techniques when combined with semantic data modeling through ontological knowledge will also solve the problem of domain_knowledge representation thereby enabling improved data classification facilities , particularly in large_datasets where number of features scale to unmanageable proportions . in this paper , the domain_knowledge is presented as a knowledge_graph , derived from the semantic data_modeling . further , to achieve better multi_class classification , multinomial_naive_bayes algorithm is applied to categorize items in their respective classes . for the experiments , data about various news groups were used for testing the accuracy of the model . experimental results have proved that the proposed classifier performs better compared to existing systems .
title : a novel cnn-based method for question_classification in intelligent_question_answering ; abstract : sentence_classification , which is the foundation of the subsequent text-based processing , plays an important role in the intelligent_question_answering ( iqa ) . convolutional_neural_networks ( cnn ) as a kind of common architecture of deep_learning , has been widely used to the sentence_classification and achieved excellent_performance in open field . however , the class_imbalance problems and fuzzy sentence feature problem are common in iqa . with the aim to get better performance in iqa , this paper proposes a simple and effective method by increasing generalization and the diversity of sentence features based on simple cnn . in proposed method , the professional entities could be replaced by placeholders to improve the performance of generalization . and cnn reads sentence_vectors from both forward and reverse directions to increase the diversity of sentence features . the testing results show that our methods can achieve better performance than many other complex cnn models . in addition , we apply our method in practice of iqa , and the results show the method is effective .
title : relation construction for aspect-level_sentiment_classification ; abstract : aspect-level_sentiment_classification aims to obtain fine-grained sentiment_polarities of different aspects in one sentence . most existing_approaches handle the classification by acquiring the importance of context words towards each given aspect individually , and ignore the benefits brought by aspect relations . since the sentiment of one aspect can be deduced through their relationship according to other aspects , in this paper , we propose a novel relation construction multi-task_learning network ( rmn ) , which is the first attempt to extract aspect relations as an auxiliary classification_task . rmn generates aspect representations through graph convolution networks with a semantic dependency_graph and utilizes the bi-attention_mechanism to capture the relevance between the aspect and the context . unlike_conventional multi-task_learning methods that need extra datasets , we construct an auxiliary relation-level classification_task that extracts aspect relations from the original dataset with shared parameters . extensive_experiments on five public datasets from semeval 14 , 15 , 16 and mams show that our rmn improves about 0.09 % to 0.8 % on accuracy and about 0.04 % to 1.19 % on f1 score , compared to several comparative baselines .
title : leveraging multilingual transformers for hate_speech detection ; abstract : detecting and classifying instances of hate in social_media text has been a problem of interest in natural_language_processing in the recent_years . our work leverages state of the art transformer language models to identify hate_speech in a multilingual setting . capturing the intent of a post or a comment on social_media involves careful evaluation of the language style , semantic content and additional pointers such as hashtags and emojis . in this paper , we look at the problem of identifying whether a twitter post is hateful and offensive or not . we further discriminate the detected toxic_content into one of the following three classes : ( a ) hate_speech ( hate ) , ( b ) offensive ( offn ) and ( c ) profane ( prfn ) . with a pre-trained multilingual transformer-based text encoder at the base , we are able to successfully identify and classify hate_speech from multiple_languages . on the provided testing corpora , we achieve macro_f1 scores of 90.29 , 81.87 and 75.40 for english , german and hindi respectively while performing hate_speech detection and of 60.70 , 53.28 and 49.74 during fine-grained classification . in our experiments , we show the efficacy of perspective api features for hate_speech classification and the effects of exploiting a multilingual training scheme . a feature_selection study is provided to illustrate impacts of specific features upon the architecture 's classification head .
title : classification of hate tweets using hybrid deep_belief_network algorithm ; abstract : social_media platforms have presented a way to express the users ’ opinions on various topics and connect to friends and share messages , photos , and videos . but there has been an increase in abusive , racial , and hateful messages . as a result , hate tweets have become a significant issue in social_media . detecting hate tweets from twitter posts with little contextual detail poses several practical problems . furthermore , the variety of user-generated information and the existence of different hate_speech make determining the degree and purpose of the post extremely difficult . a deep_belief_network with softmax regression is implemented in this work utilizing various embedding techniques for detecting hate speeches in social_media . a deep_belief_network is chosen for resolving the sparse high-dimensional matrix estimation hitch of the text data . softmax regression is executed to classify the text data in the provided learned feature_space , succeeding the feature_extraction procedure using hybrid dbn . experiments are performed on the publicly_accessible dataset and evaluate the effectiveness of the deep_learning model by considering various metrics .
title : comparison of accuracy between convolutional_neural_networks and naïve_bayes classifiers in sentiment_analysis on twitter ; abstract : the needs and demands of the community for the ease of accessing information encourage the increasing use of social_media tools such as twitter to share , deliver and search for information needed . the number of large tweets shared by twitter users every second , making the collection of tweets can be processed into useful information using sentiment_analysis . the need for a large number of tweets to produce information encourages the need for a classifier model that can perform the analysis process quickly and provide accurate results . one algorithm that is currently popular and is widely used today to build classifier models is deep_learning . sentiment_analysis in this research was conducted on english-language tweets on the topic `` turkey crisis 2018 '' by using one of the deep_learning algorithms , convolutional_neural_network ( cnn ) . the resulting of cnn classifier model will then be compared with the naïve_bayes classifier ( nbc ) classifier model to find out which classifier model can provide better accuracy in sentiment_analysis . the research methods that will be carried out in this research are data retrieval , pre-processing , model design and training , model testing and visualization . the results obtained from this research indicate that the cnn classifier model produces an accuracy of 0.88 or 88 % while the nbc classifier model produces an accuracy of 0.78 or 78 % in the testing phase of the data test . based on these results it can be concluded that the classifier model with deep_learning algorithm produces better accuracy in sentiment_analysis compared to the naïve_bayes classifier model .
title : intent_classification using pre-trained language agnostic embeddings for low_resource_languages ; abstract : building spoken_language_understanding ( slu ) systems that do not rely on language specific automatic_speech_recognition ( asr ) is an important yet less explored problem in language processing . in this paper , we present a comparative study aimed at employing a pre-trained acoustic model to perform slu in low_resource scenarios . specifically , we use three different embeddings extracted using allosaurus , a pre-trained universal phone decoder : ( 1 ) phone ( 2 ) panphone , and ( 3 ) allo embeddings . these embeddings are then used in identifying the spoken intent . we perform experiments across three different languages : english , sinhala , and tamil each with different data sizes to simulate high , medium , and low_resource scenarios . our system improves on the state-of-the-art ( sota ) intent_classification accuracy by approximately 2.11 % for sinhala and 7.00 % for tamil and achieves_competitive_results on english . furthermore , we present a quantitative_analysis of how the performance scales with the number of training_examples used per intent .
title : named_entity_recognition : a genetic approach for vote based classifier_ensemble selection ; abstract : in this paper , the search capability of genetic_algorithm ( ga ) is utilized to construct a vote based classifier_ensemble for named_entity_recognition ( ner ) . our underlying assumption is that the reliability of prediction of each classifier differs among the various named_entity ( ne ) classes . thus , it is necessary to find out the subset of classes for which any particular classifier is most suitable . we use ga to determine which classifier is suitable to vote for which ne classes . we use maximum_entropy ( me ) framework to build a number of classifiers depending upon the various feature_representations . one most important characteristic of these features is that these are language independent in nature and can be easily derived for almost all the languages with minimum effort . weighted_voting is used to combine the outputs of classifiers . the proposed technique is evaluated for a resource-constrained language like bengali . evaluation results for a resource-constrained language like bengali yield the recall , precision and f-measure values of 72.53 % , 83.54 % and 77.64 % , respectively . we also observe that the vote based classifier_ensemble identified by our proposed ga based method_outperforms all the individual_classifiers and performs reasonably better than the two different conventional baseline ensemble_techniques .
title : a pattern_based two-stage text classifier ; abstract : in a classification problem typically we face two challenging_issues , the diverse characteristic of negative documents and sometimes a lot of negative documents that are closed to positive documents . therefore , it is hard for a single classifier to clearly classify incoming documents into classes . this paper proposes a novel gradual problem_solving to create a two-stage classifier . the first stage identifies reliable negatives ( negative documents with weak positive characteristics ) . it concentrates on minimizing the number of false_negative documents ( recall-oriented ) . we use rocchio , an existing recall based classifier , for this stage . the second stage is a precision-oriented `` fine_tuning '' , concentrates on minimizing the number of false_positive documents by applying pattern ( a statistical phrase ) mining techniques . in this stage a pattern-based scoring is followed by threshold setting ( thresholding ) . experiment_shows that our statistical phrase_based two-stage classifier is promising . ©_2013_springer-verlag .
title : investigating the use of extractive summarisation in sentiment_classification ; abstract : in online_reviews , authors often use a short passage to describe the overall feeling about a product or a service . a review as a whole can mention many details not in line with the overall feeling , so capturing this key passage is important to understand the overall sentiment of the review . this paper investigates the use of extractive summarisation in the context of sentiment_classification . the aim is to find the summary sentence , or the short passage , which gives the overall sentiment of the review , filtering out potential noisy information . experiments are carried out on a movie review data-set . the main finding is that subjectivity detection plays a central role in building summaries for sentiment_classification . subjective extracts carry the same polarity of the full text reviews , while statistical and positional approaches are not able to capture this aspect .
title : accuracy_improvement of automatic text_classification based on feature_transformation ; abstract : in this paper , we describe a comparative study on techniques of feature_transformation and classification to improve the accuracy of automatic text_classification . the normalization to the relative word_frequency , the principal_component_analysis ( k-l transformation ) and the power transformation were applied to the feature_vectors , which were classified by the euclidean distance , the linear_discriminant function , the projection distance , the modified projection distance and the svm .
title : archival classification system in control and vigilancy agencies of colombia ; abstract : this article arises from disciplinary research funded by the university of la salle , program information systems , library and archives , which aimed to evaluate the file_systems of classification of the supervisory and control in colombia , which in its conception and in accordance with the guidelines issued by the archivo general de la_nación , is an organic-functional system . based on new trends in file_systems classification and the need to revitalize and bring new classification structures for the public sector in colombia , suggested a classification_scheme crossfunctional , which , under state agencies , adapts the dynamics of structural changes and management documentaries own government .
title : node slicing broad learning system for text_classification ; abstract : text_classification is playing an increasingly important role in natural_language_processing ( nlp ) . most research adopts deep structure neural_networks to achieve text_classification tasks . however , deep structure networks often suffer from time-consuming trainning process and hardware dependence . in this paper , a flat network called broad learning system ( bls ) is employed to derive a novel learning method - node slicing broad learning system ( nsbls ) . firstly , one-to-one correspondence between the words and the feature node groups is established to obtain a feature layer with rich words , on the basic of which the enhancement layer is generated representing the global_information . then we activate some nodes in the feature node groups , compact them with the enhancement layer and use ridge_regression to obtain multiple outputs . finally , an intergration bls layer is used to correct and combine the multiple outputs to get the final output . the experiment_shows that nsbls has good performance on several datasets .
title : mining customers opinion on services and applications of mobile payment companies in indonesia using sentiment_analysis approach ; abstract : the development of technology and digital has also increased the ease of accessing the internet . one aspect of daily life that are affected by the adoption of technology and the internet is the field of payment transactions . payment transactions are inseparable from everyday_life . at this time with the development of technology , payment transactions can be done with the more practical , easy , safe and convenient . the technology is called financial technology . mobile payment is a service that is part of financial technology . the aspects contained in the mobile payment are top up , transfers , cash withdrawals , online payment , and offline payments . classifications of reviews from twitter are classified using support_vector_machine . the results of this study are go-pay and ovo must pay attention to every aspect and improve every aspect , of course , to increase customer_satisfaction . the accuracy level of the classification model produced for bigram is 92 % ( go-pay ) and 93 % ( ovo ) . it also shows that sentiment_analysis using bigram can improve accuracy level .
title : conceptual sentiment_analysis model ; abstract : bag-of-words approach is popularly used for sentiment_analysis . it maps the terms in the reviews to term-document_vectors and thus disrupts the syntactic structure of sentences in the reviews . association among the terms or the semantic structure of sentences is also not preserved . this research work focuses on classifying the sentiments by considering the syntactic and semantic structure of the sentences in the review . to improve accuracy , sentiment classifiers based on relative_frequency , average frequency and term_frequency_inverse_document_frequency were proposed . to handle terms with apostrophe , preprocessing_techniques were extended . to focus on opinionated contents , subjectivity extraction was performed at phrase_level . experiments were performed on pang & lees , kaggle 's and uci 's dataset . classifiers were also evaluated on the uci 's product and restaurant dataset . sentiment_classification accuracy improved from 67.9 % for a comparable term weighing technique , deltatfidf , up to 77.2 % for proposed classifiers . inception of the proposed concept_based approach , subjectivity extraction and extensions to preprocessing_techniques , improved the accuracy to 93.9 % .
title : an effective luggage searching system based on image_classification and retrieval ; abstract : in this paper , we propose an efficient luggage searching system based on image_classification and retrieval . in this system , we can register and retrieve the luggage automatically . we bring in classification to register images so as to increase the retrieval speed . the block hsv histogram and the scale_invariant_feature_transform ( sift ) are used for image_retrieval . experiments show that the proposed system has a fast speed and high precision . ©2010 ieee .
title : a semantic approach for document_classification using deep_neural_networks and multimedia knowledge_graph ; abstract : the amount of available multimedia data in different formats and from different sources increases everyday . from an information_retrieval point of view , this high volume and heterogeneity of data involves several issues to be addressed related to information_overload and lacks of well structured information . even if modern information_retrieval systems offer to the user manifold search options , it is still hard to find systems with optimal performances in the document seeking process starting from a given topic . in recent_years , several frameworks have been proposed and developed to support this task based on different models and techniques . in this paper we propose a semantic approach to document_classification using both textual and visual topic_detection techniques based on deep_neural_networks and multimedia knowledge_graph . a semantic multimedia knowledge_base has been exploited and several experimental results show the effectiveness of our proposed approach .
title : stacked debert : all attention in incomplete data for text_classification ; abstract : in this paper , we propose stacked debert , short for stacked denoising bidirectional_encoder_representations_from_transformers . this novel model improves robustness in incomplete data , when compared to existing systems , by designing a novel encoding scheme in bert , a powerful language representation model solely based on attention_mechanisms . incomplete data in natural_language_processing refer to text with missing or incorrect words , and its presence can hinder the performance of current models that were not implemented to withstand such noises , but must still perform well even under duress . this is due to the fact that current_approaches are built for and trained with clean and complete data , and thus are not able to extract features that can adequately represent incomplete data . our proposed approach consists of obtaining intermediate input representations by applying an embedding_layer to the input tokens followed by vanilla transformers . these intermediate features are given as input to novel denoising transformers which are responsible for obtaining richer input representations . the proposed approach takes advantage of stacks of multilayer_perceptrons for the reconstruction of missing words ’ embeddings by extracting more abstract and meaningful hidden feature_vectors , and bidirectional_transformers for improved embedding representation . we consider two datasets for training and evaluation : the chatbot natural_language_understanding evaluation corpus and kaggle 's twitter sentiment corpus . our model shows improved f1-scores and better robustness in informal/incorrect texts present in tweets and in texts with speech-to-text error in the sentiment and intent_classification tasks.1
title : adop fert-automatic detection of occupations and profession in medical texts using flair and bert ; abstract : technological developments in healthcare industry are gener- ating lots of electronic_health_records as well as text data which is usually referred as medical text data . processing medical text data in unstruc- tured form is not only challenging but also has lot of applications . named_entity_recognition , the task of extracting named_entities and classifying them into predefined_categories is an important preprocessing_step in the nlp pipeline . extracting named_entities from medical text is very useful for many applications and at the same time very challenging because of the characteristics of medical text data . considering the gravity of medi-_cal text processing , in this paper , we ( team mucic ) describe the models submitted to `` medical doocu-ments professions recognition '' ( med- doprof ) , a first shared_task consisting of three tracks , namely : track 1 : meddoprof-ner , track 2 : meddoprof-class , and track 3 : meddoprof-norm , in spanish_language . we participated in track 1 and 2 and proposed two models based on fine-tuning bert embed- dings using i ) bertfortokenclassification from transformers and ii ) flair framework , for the automatic detection of occupations and professions in medical text . the model using bertfortokenclassification reported micro_f1 scores of 0.629 and 0.598 for track 1 and 2 respectively , while the flair framework model reported micro_f1 scores of 0.8 and 0.764 for track 1 and 2 respectively . further , the flair framework model for meddoprof-ner track became one among the best models .
title : register identification from the unrestricted open web using the corpus of online registers of english ; abstract : this article examines the automatic identification of web registers , that is , text varieties such as news articles and reviews . most studies have focused on corpora restricted to include only preselected classes with well-defined characteristics . these corpora feature only a subset of documents found on the unrestricted open web , for which register identification has been particularly difficult because the range of linguistic variation on the web is known to be substantial . as part of this study , we present the first open release of the corpus of online registers of english ( core ) , which is drawn from the unrestricted open web and , currently , is the largest collection of manually_annotated web registers . furthermore , we demonstrate that the core registers can be automatically identified with competitive_results , with the best performance being an f1-score of 68 % with the deep_learning model bert . the best performance was achieved using two modeling strategies . the first one involved modeling the registers using propagated register labels , that is , repeating the main register label along with its corresponding subregister label in a multilabel model . in the second one , we explored how the length of the document affects model performance , discovering that the beginning provided superior classification_accuracy . overall , the current study presents a systematic approach for the automatic identification of a large number of web registers from the unrestricted web , hence providing new pathways for future_studies .
title : latent-variable generative_models for data-efficient text_classification ; abstract : generative classifiers offer potential advantages over their discriminative counterparts , namely in the areas of data efficiency , robustness to data shift and adversarial_examples , and zero-shot learning ( ng and jordan,2002 ; yogatama et al. , 2017 ; lewis and fan,2019 ) . in this paper , we improve generative text classifiers by introducing discrete latent_variables into the generative story , and explore several graphical_model configurations . we parameterize the distributions using standard neural_architectures used in conditional language modeling and perform learning by directly maximizing the log marginal likelihood via gradient-based optimization , which avoids the need to do expectation-maximization . we empirically characterize the performance of our models on six text_classification datasets . the choice of where to include the latent_variable has a significant_impact on performance , with the strongest results obtained when using the latent_variable as an auxiliary conditioning variable in the generation of the textual input . this model consistently_outperforms both the generative and discriminative classifiers in small-data settings . we analyze our model by using it for controlled generation , finding that the latent_variable captures interpretable properties of the data , even with very small training_sets .
title : speaker identification system using cnn approach ; abstract : in this paper , a text independent speaker identification ( si ) system is proposed using convolutional_neural_network ( cnn ) . also , the proposed methodology is tested in a noisy environment . the text independent si system is used in this work due to its ability to learn features for classification_task . spectrogram images are used at front_end for feature_extraction . for classification , convolutional_neural_network is utilized indicating promising_results . dataset used in this work comprise of 5 speakers , with each speaker uttering 4 voice samples . the overall accuracy achieved for the proposed approach is 96.54 % .
title : categorization of bangla medical text documents based on hybrid internal feature ; abstract : this paper aims to develop an automatic_text_categorization system that classifies bangla medical and non-medical text documents based on two primary features , that is , word length and the presence of english equivalent words in the text documents . to start with , it has been shown that based on the word length and the number of english equivalent words present in a particular text , bangla medical text documents can be identified among other text documents of any domain . sgd ( stochastic_gradient_descent ) classification algorithm is used and an accuracy of 97.75 % has been achieved . comparisons have also been done with other commonly used classifiers to test the system from which it has been observed that sgd performs better than those classifiers .
title : an incremental approach to classify healthcare urls using a novel ‘ web document_classification model ’ ; abstract : in the present research work , we proposed a novel web document_classification model ( wdcm ) to classify healthcare-related urls . this is required as the web is flooded with millions of urls and they may be duplicated or irrelevant most of the time . it is a challenging task to extract and organize web_documents specific to a certain domain . another challenge is to provide the updated and latest information to the users . in this research , the authors have proposed wdcm which tackles the above-mentioned challenges with the incremental_learning approach . authors have also used sentiment_analysis and document similarity_measures to classify urls in the healthcare domain . the analysis of the experimental results shows that with machine_learning classifiers , cosine_similarity measure with logistic_regression showed the highest_accuracy of 93.75 % . euclidean distance ( ed ) measure with logistic_regression showed minimum accuracy of 86.6 % . when implemented as an algorithm ed measure showed the highest_accuracy of 96.60 % for training and 95.83 % for the testing dataset in five iterations . jaccard distance_measure showed lower accuracy of 65.71 % for training and 85.33 for the testing dataset in five iterations . it is also observed that the maximum urls fetched are with the.com domain .
title : cases as conceptual categories : evidence from german ; abstract : cognitive linguists generally agree that the study of language can offer rich insights into the general nature of human conceptualization and categorization . the question of how conceptual_categorization is reflected in language ( via linguistic categorization ) is thus a fundamental issue in linguistic theory . an especially interesting aspect of this issue is the question as to whether conceptual categories can be marked not only by vocabulary items in languages but also by the grammars of the languages themselves . lakoff ( 1987 : 91 ) notes that the grammaticization of conceptual categories is common and states furthermore that “ conceptual categories marked by the grammars of languages are important in understanding the nature of cognitive categories in general� ? . the present paper is a contribution to this general line of inquiry in its challenge to the usual assumption that morphological cases are mere grammatical markers without inherent semantic content . on the contrary , i will show how the dative ( dat ) and accusative ( acc ) cases in german can be analyzed as meaningful in encoding fundamental cognitive categories .
title : applying machine_learning in accounting research ; abstract : quite often , in order to derive meaningful_insights , accounting researchers have to analyze large bodies of text . usually , this is done manually by several human coders , which makes the process time consuming , expensive , and often neither replicable nor accurate . in an attempt to mitigate these problems , we perform a feasibility study investigating the applicability of computer-aided content_analysis techniques onto the domain of accounting research . krippendorff ( 1980 ) defines an algorithm 's reliability as its stability , reproducibility and accuracy . since in computer-aided text_classification , which is inherently objective and repeatable , the first two requirements , stability and reproducibility , are not an issue , this paper focuses exclusively on the third requirement , the algorithm 's accuracy . it is important to note that , although inaccurate classification results are completely worthless , it is surprising to see how few research papers actually mention the accuracy of the used classification methodology . after a survey of the available techniques , we perform an in depth analysis of the most promising one , lpu ( learning from positive and unlabelled ) , which turns out to have an f-value and accuracy of about 90 % , which means that , given a random text , it has a 90 % probability of classifying it correctly . © 2010 elsevier ltd. all rights_reserved .
title : neural discourse_structure for text_categorization ; abstract : we show that discourse_structure , as defined by rhetorical structure theory and provided by an existing discourse_parser , benefits text_categorization . our approach uses a recursive_neural_network and a newly_proposed attention_mechanism to compute a representation of the text that focuses on salient content , from the perspective of both rst and the task . experiments consider variants of the approach and illustrate its strengths_and_weaknesses .
title : handwritten_document age classification based on handwriting styles ; abstract : handwriting styles are constantly changing over time . we approach the novel problem of estimating the approximate age of historical handwritten_documents using handwriting styles . this system will have many applications in handwritten_document processing engines where specialized processing techniques can be applied based on the estimated age of the document . we propose to learn a distribution over styles across centuries using topic_models and to apply a classifier over weights learned in order to estimate the approximate age of the documents . we present a comparison of different distance_metrics such as euclidean distance and hellinger distance within this application . © 2011 copyright society of photo-optical instrumentation engineers ( spie ) .
title : classifying emotions in twitter messages using a deep_neural_network ; abstract : many people use social_media nowadays to express their emotions or opinions about something . this paper proposes the use of a deep_learning network_architecture for emotion classification in twitter messages , using the six emotions model of ekman : happiness , sadness , anger , fear , disgust and surprise . we collected the tweets from a labeled_dataset that contains about 2.5 million_tweets and used the word2vec predictive model to learn the relations of each word and transform them into numbers that the deep network receives as input . our approach achieved a 63 % accuracy with all the classes and 77 % accuracy on a binary_classification scheme .
title : an ensemble approach to handle out of vocabulary in multilabel document_classification ; abstract : in document_classification , out of vocabulary occured when there are words that are not covered by the classifier . this condition can affect prediction performance . the classifier needs to be re-trained using new documents containing those words in order to handle out of vocabulary condition . in this paper , we proposed an ensemble_method that able to re-train a classifier in incremental fashion . that is , it does not use any previous training documents , only the new ones . our approach also handles multilabel_classification where a document can be classified into more than one label , which is a the case to indonesian_news articles we use for this paper . result from our experiment_shows that the ensemble_method can cover out of vocabulary words and correctly classify a document where other classifiers which do not cover the words can not . it some conditions , it can even outperform batch-trained classifier performance . and also , because our ensemble_method is an incremental method , its training time and memory consumption depends to the training_data it currently uses .
title : distributed text feature_selection based on bat algorithm optimization ; abstract : the feature_selection effect directly_affects the classification_accuracy of the text . this paper introduces a new text feature_selection method based on bat optimization . this method uses the traditional feature_selection method to pre-select the original features , and then uses the bat group algorithm to optimize the pre-selected features in binary code form , and uses the classification_accuracy as the individual fitness . however , when the amount of text information is large , the execution time of the single machine is long . according to this shortcoming , combining the bat algorithm and the spark parallel_computing framework , the text feature_selection algorithm sbatfs is proposed . the algorithm combines the good search performance of the bat algorithm with the distributed and efficient calculation speed to realize the efficient solution of the text feature_selection optimization model . the results show that compared with the traditional feature_selection method , after sbatfs is used for feature optimization , the classification_accuracy is effectively improved .
title : skit-s2i : an indian accented speech to intent dataset ; abstract : conventional conversation assistants extract text transcripts from the speech_signal using automatic_speech_recognition ( asr ) and then predict intent from the transcriptions . using end-to-end spoken_language_understanding ( slu ) , the intents of the speaker are predicted directly from the speech_signal without requiring intermediate text transcripts . as a result , the model can optimize directly for intent_classification and avoid cascading errors from asr . the end-to-end slu system also helps in reducing the latency of the intent prediction model . although many datasets are available publicly for text-to-intent tasks , the availability of labeled speech-to-intent datasets is limited , and there are no datasets available in the indian accent . in this paper , we release the skit-s2i dataset , the first publicly available indian-accented slu dataset in the banking domain in a conversational tonality . we experiment with multiple baselines , compare different pretrained speech encoder 's representations , and find that ssl pretrained representations perform slightly better than asr pretrained representations lacking prosodic_features for speech-to-intent_classification . the dataset and baseline code is available at \url_{_https : //github.com/skit-ai/speech-to-intent-dataset }
title : classification of extreme facial events in sign_language videos ; abstract : we propose a new approach for extreme states classification ( esc ) on feature_spaces of facial cues in sign_language ( sl ) videos . the method is built upon active appearance model ( aam ) face tracking and feature_extraction of global and local aams . esc is applied on various facial cues-as , for instance , pose rotations , head movements and eye blinking-leading to the detection of extreme states such as left/right , up/down and open/closed . given the importance of such facial events in sl analysis , we apply esc to detect visual events on sl videos , including both american ( asl ) and greek ( gsl ) corpora , yielding promising qualitative and quantitative results . further , we show the potential of esc for assistive annotation tools and demonstrate a link of the detections with indicative higher-level linguistic events . given the lack of facial annotated_data and the fact that manual annotations are highly time-consuming , esc results indicate that the framework can have significant_impact on sl processing and analysis . © 2014 antonakos et al .
title : feature_extraction techniques for gender classification based on handwritten_text : a critical review ; abstract : features of the handwritten_text play a vital role in the area of handwriting identification . it became more challenging when one has to identify gender , age , and handedness of the person through handwriting . in last two decade , the use of various feature_extraction techniques immerged having advantages one on the other . ample research is done on writer identification systems by implementing various feature_extraction techniques . in this paper , we have shifted the concern toward various features and features extraction techniques implemented on gender identification through handwriting . the objective of this survey is to present the critical review of work done in area feature_extraction in gender identification taking only handwriting into consideration . we have categorized all the feature_extraction techniques used by the researchers for gender classification into four broad categories : statistical- , transform- , gradient- , and model-based techniques . from the survey , we have identified few techniques that deserve future attention of the researchers for optimal results .
title : biobygans : biomedical_named_entity_recognition by fusing contextual and syntactic features through graph_attention_network in node classification framework ; abstract : background : automatic and accurate recognition of various biomedical named_entities from literature is an important task of biomedical_text_mining , which is the foundation of extracting biomedical knowledge from unstructured_texts into structured formats . using the sequence labeling framework and deep_neural_networks to implement biomedical_named_entity_recognition ( bioner ) is a common method at present . however , the above method often underutilizes syntactic features such as dependencies and topology of sentences . therefore , it is an urgent problem to be solved to integrate semantic and syntactic features into the bioner model . results : in this paper , we propose a novel biomedical_named_entity_recognition model , named biobygans ( biobert/spacy-graph_attention_network-softmax ) , which uses a graph to model the dependencies and topology of a sentence and formulate the bioner task as a node classification problem . this formulation can introduce more topological features of language and no longer be only concerned about the distance between words in the sequence . first , we use periods to segment sentences and spaces and symbols to segment words . second , contextual features are encoded by biobert , and syntactic features such as part of speeches , dependencies and topology are preprocessed by spacy respectively . a graph_attention_network is then used to generate a fusing representation considering both the contextual features and syntactic features . last , a softmax_function is used to calculate the probabilities and get the results . we conduct_experiments on 8 benchmark_datasets , and our proposed model outperforms existing bioner state-of-the-art methods on the bc2gm , jnlpba , bc4chemd , bc5cdr-chem , bc5cdr-disease , ncbi-disease , species-800 , and linnaeus datasets , and achieves f1-scores of 85.15 % , 78.16 % , 92.97 % , 94.74 % , 87.74 % , 91.57 % , 75.01 % , 90.99 % , respectively . conclusion : the experimental results on 8 biomedical benchmark_datasets demonstrate the effectiveness of our model , and indicate that formulating the bioner task into a node classification problem and combining syntactic features into the graph attention networks can significantly_improve model performance .
title : fulfillment and responsiveness on online travel agencies using multiclass_classification ; abstract : in recent_years , online travel agencies ( ota ) is widely used by people due to its simplicity and efficiency . tight competition between industries makes companies must pay attention to the quality of their services since it is capable to enhance customer_satisfaction . to evaluate its service , the company needs to comprehend their position as ota providers . users ' opinion in social_media is essential for recognizing the company performances . in this case , sentiment_analysis and multiclass_classification methods help the company to understand their service_quality specifically . as a case_study , we use the most popular online travel agencies ( ota ) providers in indonesia : traveloka , tiket.com , and pegipegi . based on our criterion , we examine the fulfillment and responsiveness dimensions of these three ota providers . we apply naïve_bayes classifier ( nb ) model to classify users ' opinions . this model has accuracy around 75-85 % for the three ota providers . the result reveals that pegipegi obtains better service_quality with 57 % positive and 43 % negative_sentiment than traveloka and tiket.com with 56 % positive and 44 % negative . the overall result_shows general topics of fulfillment and responsiveness dimensions are related to the ticket availability and customer_service performances .
title : bi-lstm and ensemble based bilingual sentiment_analysis for a code-mixed hindi-english social_media text ; abstract : india is a multilingual and multi-script country and a large part of its population speaks more than one language . it has been noted that such multilingual speakers switch between languages while communicating informally . the code-mixed_language is very common in informal communication and social_media , and extracting sentiments from these code-mixed sentences is a challenging task . in this work , we have worked on sentiment_classification for one of the most common code-mixed_language pairs in india i.e . hindi-english . the conventional sentiment_analysis techniques designed for a single language do n't provide satisfactory_results for such texts . we have proposed two approaches for better sentiment_classification . we have proposed an ensembling based approach which is based on hybridization of naive_bayes , svm , linear_regression , and sgd classifiers . we have also developed a bidirectional_lstm based novel approach . the approaches provide quite satisfactory_results for the code-mixed hindi-english text .
title : comparison of the accuracy and the execution time of classification_algorithms for bulgarian literary works ; abstract : the purpose of this paper is to compare the accuracy and the execution time of machine_learning algorithms for classification of texts , written by bulgarian authors . the algorithms examined are : multinomial_naive_bayes classifier , support_vector_machines , random_forest and adaboost . the results show that the multinomial_naive_bayes classifier is the most accurate and fastest algorithm for classifying texts by two authors with an equal number of poems in bulgarian_language . the ensemble algorithm adaboost is the most accurate for unbalanced_data classification . the support_vector classification has the highest_accuracy . in a classification with an unbalanced set of data , the fastest algorithm is bernoulli naive_bayes_classifier .
title : fast_text categorization based on collaborative work in the semantic and class spaces ; abstract : the blooming of the internet information has made fast_text categorization very essential . generally , in order to accelerate the classification process , the classifier needs to be simplified as much as possible ; however , the accuracy might descend drastically in that case , this paper proposes a novel approach to achieve a suitable tradeoff between the speed and accuracy . with category information fusion and basis orthogonality non-negative_matrix_factorization , the documents can be mapped from the term space to a semantic or class s-pace , and a simple and fast classification method in the class space is proposed . furthermore a criterion for re-classifying in the semantic space is discussed . finally , the collaborative work framework in the semantic and class spaces is implemented . experiments in two benchmarks are presented , and the results are encouraging . © 2011 ieee .
title : emotion recognition from helpdesk messages ; abstract : this paper describes system for emotion recognition which can be used to determine the priority of messages on the first level of helpdesk services . an algorithm used in this paper uses artificial_intelligence ( svm classifier ) and can recognize 5 different emotions . the used emotional classes were based on acoustic model which was inspired by acoustic emotion recognition research works . the proposed system has evaluated 5 classifiers and identifies a dominant emotion class . this work also describes a small database which was created on the basis of the selected helpdesk messages . the database was used in training and testing of the mentioned classifier . success of classifier achieved in this work is 76.63 % and impact of the proposed optimization methods on the final model accuracy has been proven .
title : study on feature_selection and weighting based on synonym merge in text_categorization ; abstract : feature_selection and weighting is one of the key problem in text_categorization . the chief obstacles to feature_selection are noise and sparseness . this paper presents an approach of chinese text feature_selection and weighting based on semantic statistics . first , we use synonymous concepts to extract feature_values in text based on thesaurus which names tongyici cilin . then , we introduce a new weight function based on term_frequency and entropy , which adjusts the effect of the feature term in the classifier according to the feature term 's strength . experiments show that our method is much better than kinds of traditional feature_selection methods and it improve the performance of text_categorization systems © 2010 ieee .
title : performance and evaluation of different kernels in support_vector_machine for text_mining ; abstract : text_mining is the subfield of data_mining . text analysis or mining is enormously growing field for research simultaneously to artificial_intelligence and data_mining . unstructured_data or multimedia data is being constantly generated via social_media web_sites , call center logs , blogs , and so on . therefore , the explosion of textual_data in a social_media network is overwhelming . text analysis or mining is widely being used to determine meaningful and noteworthy information for the huge amount of unstructured multimedia or heterogeneous data . in this paper , we compare the performance of text_classification through support_vector_machine using multimedia data . this text analysis uses the technique of different fields like predict , machine_learning , information , visualization , and natural_language_processing . the use of support_vector_machine for learning text nonlinear classifier is determined here . the reason for using svm is that its performance is more accurate as compared to other soft_computing tools and algorithms . in this paper , different kernel tricks have applied with nonlinear classifier for classification of text data_mining . the results of proposed experiments predict that support_vector_machine with radial_basis_function achieves the highest overall accuracy .
title : classification by weighting , similarity and knn ; abstract : in this paper , the grouping method of the similar words , is proposed for the classification of documents . it is shown that the grouping of words has equivalent ability to the lsa in the classification_accuracy . further , a new combining method is proposed for the documents classification , which consists of grouping , latent_semantic_analysis ( lsa ) followed by the k-nearest_neighbor classification ( k-nn ) . the combining method proposed here , shows the higher_accuracy in the classification than the conventional methods of the knn , and the lsa followed by the knn . thus , the grouping method is effective as a preprocessing before the conventional method . ©_springer-verlag_berlin_heidelberg 2006 .
title : class-biased sarcasm detection using bilstm variational_autoencoder-based synthetic oversampling ; abstract : recent research works have established the importance of sarcasm detection in the domain of sentiment_analysis . automatic sarcasm detection using social_media data is a challenging task in the presence of imbalanced_classes . real-life social_media data often suffer from this problem of class_imbalance resulting in dramatical degradation of the performance of classification models attempting to detect sarcasm . motivated by this , in the current article , a bi-lstm variational_autoencoder model has been proposed to alleviate the problem of imbalanced_classes in social_media datasets targeted to train sarcasm detection models . the proposed bva model is trained with a large corpus of sarcastic and non-sarcastic tweets to obtain the most suitable latent_space representation of the same . these inherently class-biased latent vectors are then oversampled using synthetic_minority_oversampling techniques . the quality of the proposed method is established by training and testing a set of well-known classifiers in terms of precision , recall , f1-score , auc , and g-mean . extensive_experiments reveal that the proposed bva model combined with oversampling techniques can improve classifier performance for sarcasm detection to a greater extent .
title : an intelligent framework for e-recruitment system based on text_categorization and semantic analysis ; abstract : in the field of online job recruiting , accurate job and resume categorization is critical for both the seeker and the recruiter . using natural_language_processing ( nlp ) technology we have developed an autonomous text_classification system that pos_tag , tokenizes , lemmatize the data . we have utilized phrase matcher to calculate the score of resumes based on recruiter 's information , suggest lacking skills to users , and provide the top resumes to the recruiter . finally , the proposed system is presented together with its findings and analysis . we divided candidates into groups based on the information in their resumes . we used domain_adaptation due to the sensitive nature of the resumes content . a word_order similarity between sentences is used to categorize the resume data on large dataset of job_description . the system is evaluated and resulted in improved precision and recall .
title : on using partial supervision for text_categorization ; abstract : in this paper , we discuss the merits of building text_categorization systems by using supervised clustering techniques . traditional_approaches for document_classification on a predefined set of classes are often unable to provide sufficient accuracy because of the difficulty of fitting a manually categorized collection of documents in a given classification model . this is especially the case for heterogeneous collections of web_documents which have varying styles , vocabulary , and authorship . hence , this paper investigates the use of clustering in order to create the set of categories and its use for classification of documents . completely unsupervised_clustering has the disadvantage that it has difficulty in isolating sufficiently fine-grained classes of documents relating to a coherent subject_matter . in this paper , we use the information from a preexisting taxonomy in order to supervise the creation of a set of related clusters , though with some freedom in defining and creating the classes . we show that the advantage of using partially_supervised clustering is that it is possible to have some control over the range of subjects that one would like the categorization system to address , but with a precise mathematical definition of how each category is defined . an extremely effective way then to categorize documents is to use this a priori_knowledge of the definition of each category . we also discuss a new technique to help the classifier distinguish better among closely_related clusters .
title : comparison of machine_learning approaches on arabic twitter sentiment_analysis ; abstract : with the dramatic expansion of information over the internet , users around the world express their opinion daily on the social_network such as facebook and twitter . large corporations nowadays invest on analyzing these opinions in order to assess their products or services by knowing the people feedback toward such business . the process of knowing users ' opinions toward particular product or services whether positive or negative is called sentiment_analysis . arabic is one of the common languages that have been addressed regarding sentiment_analysis . in the literature , several approaches have been proposed for arabic sentiment_analysis and most of these approaches are using machine_learning techniques . machine_learning techniques are various and have different performances . therefore , in this study , we try to identify a simple , but workable approach for arabic sentiment_analysis on twitter . hence , this study aims to investigate the machine_learning technique in terms of arabic sentiment_analysis on twitter . three techniques have been used including naïve_bayes , decision_tree ( dt ) and support_vector_machine ( svm ) . in addition , two simple sub-tasks pre-processing have been also used ; term_frequency-inverse_document_frequency ( tf-idf ) and arabic stemming to get the heaviest weight term as the feature for tweet classification . tf-idf aims to identify the most frequent words , whereas stemming aims to retrieve the stem of the word by removing the inflectional derivations . the dataset that has been used is modern arabic corpus which consists of arabic tweets . the performance of classification has been evaluated based on the information_retrieval metrics precision , recall , and f-measure . the experimental results have shown that dt has outperformed the other techniques by obtaining 78 % of f-measure .
title : graph based based knn for text_categorization ; abstract : this research proposes the string_vector based version of the knn as the approach to the text_categorization . traditionally , texts should be encoded into numerical_vectors for using the traditional version of knn , and encoding so leads to the three main problems : huge dimensionality , sparse distribution , and poor transparency . in order to solve the problems , in this research , texts are encoded into string_vectors , instead of numerical_vectors , the similarity_measure between string_vectors is defined , and the knn is modified into the version where string_vector is given its input . as the benefits from this research , we may expect the better performance , more compact representation of each text , and better transparency . the goal of this research is to improve the text_categorization performance by solving them .
title : composite semantic relation_classification ; abstract : different semantic interpretation tasks such as text entailment and question_answering require the classification of semantic relations between terms or entities within text . however , in most cases it is not possible to assign a direct semantic relation between entities/terms . this paper proposes an approach for composite semantic relation_classification , extending the traditional semantic relation_classification task . different from existing_approaches , which use machine_learning models built over lexical and distributional word_vector features , the proposed model uses the combination of a large commonsense_knowledge base of binary relations , a distributional navigational algorithm and sequence classification to provide a solution for the composite semantic relation_classification problem .
title : an enhanced data_mining model for text_classification ; abstract : classification plays a vital role in many information_management and retrieval tasks . this paper studies classification of text document . text_classification is a supervised technique that uses labeled_training_data to learn the classification system and then automatically_classifies the remaining text using the learned system . in this paper , we propose a mining model consists of sentence-based concept analysis , document-based concept analysis , and corpus-based concept-analysis . then we analyze the term that contributes to the sentence semantics on the sentence , document , and corpus levels rather than the traditional analysis of the document only . after extracting feature_vector for each new document , feature_selection is performed . it is then followed by k-nearest_neighbour classification . the approach enhances the text_classification accuracy . © 2012 ieee .
title : on classification of abstracts obtained from medical journals ; abstract : classification of medical documents was mostly carried out on english data sets and these studies were performed on hospital_records rather than academic texts . the main reasons behind this situation are the lack of publicly available data sets and the tasks being costly and time-consuming . as the first contribution of this study , two data sets including turkish and english counterparts of the same abstracts published in turkish medical journals were constructed . turkish is one of the widely used agglutinative languages worldwide and english is a good example of non-agglutinative languages . while english abstracts were obtained automatically from medline database with a computer program , turkish counterparts of these documents were collected manually from the internet . as the second contribution of this study , an extensive comparison on classification of abstracts obtained from turkish medical journals was made by using these two equivalent data sets . features were extracted from text documents with three different approaches : unigram , bigram and hybrid . hybrid approach includes a combination of unigram_and_bigram features . in the experiments , three different feature_selection methods and seven different classifiers were utilised . according to the results on both data sets , classification performance of the english abstracts outperformed the turkish counterparts . maximum accuracies were obtained from the combination of unigram_features , distinguishing_feature_selector ( dfs ) and multinomial_naïve_bayes ( mnb ) classifier for both data sets . unigram_features were generally more efficient than bigram and hybrid features . however , analysis of top-10 features indicated that nearly half of the features were translations of each other for turkish and english data sets .
title : lexical use in emotional autobiographical narratives of persons with schizophrenia and healthy_controls ; abstract : language dysfunction has long been described in schizophrenia and most studies have focused on characteristics of structure and form . this project focuses on the content of language based on autobiographical narratives of five basic emotions . in persons with schizophrenia and healthy_controls , we employed a comprehensive automated analysis of lexical use and we identified specific words and semantically or functionally related words derived from dictionaries that occurred significantly more often in narratives of either group . patients employed a similar number of words but differed in lower expressivity and complexity , more self-reference and more repetitions . we developed a classification method for predicting subject status and tested its accuracy in a leave-one-subject-out evaluation procedure . we identified a set of 18 features that achieved 65.7 % accuracy in predicting clinical status based on single emotion narratives , and 74.4 % accuracy based on all five narratives . subject clinical status could be determined automatically more accurately based on narratives related to anger or happiness experiences and there were a larger number of lexical differences between the two groups for these emotions compared to other emotions .
title : a use case of patent classification using deep_learning with transfer_learning ; abstract : purpose : patent classification is one of the areas in intellectual_property analytics ( ipa ) , and a growing use case since the number of patent applications has been increasing worldwide . we propose using machine_learning algorithms to classify portuguese patents and evaluate the performance of transfer_learning methodologies to solve this task . design/methodology/approach : we applied three different approaches in this paper . first , we used a dataset available by inpi to explore traditional_machine_learning_algorithms and ensemble_methods . after preprocessing data by applying tf-idf , fasttext and doc2vec , the models were evaluated by cross-validation in 5 folds . in a second approach , we used two different neural_networks architectures , a convolutional_neural_network ( cnn ) and a bi-directional_long_short-term_memory ( bilstm ) . finally , we used pre-trained bert , distilbert , and ulmfit models in the third approach . findings : berttimbau , a bert architecture model pre-trained on a large portuguese corpus , presented the best results for the task , even though with a performance of only 4 % superior to a linearsvc model using tf-idf feature_engineering . research limitations : the dataset was highly_imbalanced , as usual in patent applications , so the classes with the lowest samples were expected to present the worst performance . that result happened in some cases , especially in classes with less than 60 training_samples . practical_implications : patent classification is challenging because of the hierarchical classification system , the context overlap , and the underrepresentation of the classes . however , the final model presented an acceptable_performance given the size of the dataset and the task complexity . this model can support the decision and improve the time by proposing a category in the second level of icp , which is one of the critical phases of the grant patent process . originality/value : to our knowledge , the proposed models were never implemented for portuguese patent classification .
title : alga : adaptive lexicon learning using genetic_algorithm for sentiment_analysis of microblogs ; abstract : sentiment_analysis is about classifying opinions_expressed in text . the aim of this study is to improve polarity_classification of sentiments in microblogs by building adaptive sentiment_lexicons . in the proposed method , corpora-based and lexicon-based approaches are combined and lexicons are generated from text . the sentiment_classification is formulated as an optimization_problem , in which the goal is to find optimum sentiment_lexicons . a novel genetic_algorithm is then proposed to solve this optimization_problem and find lexicons to classify text . the algorithm generates adaptive sentiment_lexicons , and then a meta-level feature is extracted based on it , which is then used alongside bing liu 's lexicon and n-gram features . the experiments are conducted on six datasets . in terms of accuracy , the results outperform the state-of-the-art methods proposed in the literature in two of the datasets . also , in four of the datasets , the proposed approach_outperforms in terms of f-measure . applying the proposed method on six datasets , the accuracy is higher than 80 % in all six datasets and the f-measure is higher than 80 % in four of these datasets . using the sentiment_lexicons created by the proposed algorithm , one can get a better understanding of the specific language and culture of twitter users and sentiment_orientation of words in different contexts . it is also shown that it is useful not to omit the conventional stop-words , as each word can have its sentimental implications .
title : feature_selection for text_classification using or+svm-rfe ; abstract : feature_selection is the key_issue in text_classification because there are a large number of attributes . in this paper , we propose a new algorithm or+svm-rfe that integrates odds radio ( or ) with recursive feature_elimination based on svm ( svm-rfe ) . odds radio is first used to roughly and rapidly select a feature_subset . then svm-rfe is used to delicately select a smaller feature_subset . experiment results show the feature_subset selected by or+svm-rfe obtains a good classification performance with less features . ©2010 ieee .
title : sentiment_analysis using blstm-resnet on textual images ; abstract : sentiment_analysis is a popular classification problem where the degree of emotion is analysed based on text . convolution neural_network ( cnn ) are part of various sentiment_classification models , but their use has been limited to one-dimensional ( 1d ) convolution or shallow two-dimensional ( 2d ) convolution . the primary_focus of this research is to use deep 2d-cnn architectures , inspired by the popular computer vision model resnet , to replace a shallow 2d-cnn in an existing blstm ( bidirectional_long_short_term_memory ) -2dcnn model . this research investigates a new method for sentiment_analysis which is an amalgam of the practices popular in natural_language_processing ( nlp ) as well as computer vision , striving to extract opinions from a text by transforming text into images . the text images are formed by transforming the word_embeddings matrix into a greyscale image with the help of blstm cell . intensive experiments have been conducted on the sentiment140 dataset with our novel blstm-resnet model , which contains residual blocks with skip connections . several blstm-resnet variants were tested to investigate the impact of network depth and dataset size on sentiment detection . moreover , two sets of residual blocks are designed to form our shallow and deep blstm-resnet models . our best shallow blstm-resnet models have achieved 4.06 % and 3.43 % increases in accuracy for dataset sizes 80,000 and 200,000 respectively , compared with the baseline blstm-2dcnn model . in addition , an overall improvement is observed on accuracy with every additional residual block in our shallow blstm-resnet model until accuracy saturates , and the same trend has been seen on the impact of dataset size on the performance . our deep blstm-resnet models show the same positive impact of network depth impact and dataset size on sentiment_analysis . further investigation on the shallow and deep blstm-resnet models shows that deep blstm-resnet outperforms shallow blstm-resnet , in general .
title : ivturs : a linguistic fuzzy_rule-based classification system based on a new interval-valued_fuzzy reasoning method with tuning and rule selection ; abstract : interval-valued_fuzzy_sets have been shown to be a useful tool to deal with the ignorance related to the definition of the linguistic labels . specifically , they have been successfully applied to solve classification problems , performing simple modifications on the fuzzy_reasoning method to work with this representation and making the classification based on a single number . in this paper , we present ivturs , which is a new linguistic fuzzy_rule-based classification method based on a new completely interval-valued_fuzzy reasoning method . this inference process uses interval-valued restricted equivalence functions to increase the relevance of the rules in which the equivalence of the interval membership degrees of the patterns and the ideal membership degrees is greater , which is a desirable behavior . furthermore , their parametrized construction allows the computation of the optimal function for each variable to be performed , which could involve a potential improvement in the system 's behavior . additionally , we combine this tuning of the equivalence with rule selection in order to decrease the complexity of the system . in this paper , we name our method ivturs-farc , since we use the farc-hd method to accomplish the fuzzy_rule learning process . the experimental study is developed in three steps in order to ascertain the quality of our new proposal . first , we determine both the essential role that interval-valued_fuzzy_sets play in the method and the need for the rule selection_process . next , we show the improvements achieved by ivturs-farc with respect to the tuning of the degree of ignorance when it is applied in both an isolated way and when combined with the tuning of the equivalence . finally , the significance of ivturs-farc is further depicted by means of a comparison by which it is proved to outperform the results of farc-hd and furia , which are two high performing fuzzy classification_algorithms . © 2013 ieee .
title : advanced linguistic explanations of classifier decisions for users ' annotation support ; abstract : we propose several new concepts for providing enhanced explanations of classifier decisions in linguistic ( human readable ) form . these are intended to help operators to better understand the decision process and support them during sample annotation to improve their certainty and consistency in successive labeling cycles . this is expected to lead to better , more consistent data sets ( streams ) for use in training and updating classifiers . the enhanced explanations are composed of 1 ) grounded reasons for classification decisions , represented as linguistically readable fuzzy_rules , 2 ) a classifier 's level of uncertainty in relation to its decisions and possible alternative suggestions , 3 ) the degree of novelty of current samples and 4 ) the levels of impact of the input features on the current classification response . the last of these are also used to reduce the lengths of the rules to a maximum of 3 to 4 antecedent parts to ensure readability for operators and users . the proposed techniques were embedded within an annotation gui and applied to a real-world application scenario from the field of visual inspection . the usefulness of the proposed linguistic explanations was evaluated based on experiments conducted with six operators . the results indicate that there is approximately an 80 % chance that operator/ user labeling behavior improves significantly when enhanced linguistic explanations are provided , whereas this chance drops to 10 % when only the classifier responses are shown .
title : relevance-based word_embedding ; abstract : learning a high-dimensional dense representation for vocabulary terms , also known as a word_embedding , has recently_attracted much attention in natural_language_processing and information_retrieval tasks . the embedding_vectors are typically learned based on term proximity in a large corpus . this means that the objective in well-known word_embedding algorithms , e.g. , word2vec , is to accurately predict adjacent word ( s ) for a given word or context . however , this objective is not necessarily equivalent to the goal of many information_retrieval ( ir ) tasks . the primary objective in various ir_tasks is to capture relevance instead of term proximity , syntactic , or even semantic similarity . this is the motivation for developing unsupervised relevance-based word_embedding models that learn word_representations based on query-document relevance information . in this paper , we propose two learning models with different objective_functions ; one learns a relevance distribution over the vocabulary set for each query , and the other classifies each term as belonging to the relevant or non-relevant class for each query . to train our models , we used over six million unique queries and the top ranked documents retrieved in response to each query , which are assumed to be relevant to the query . we extrinsically evaluate our learned word_representation models using two ir_tasks : query_expansion and query classification . both query_expansion experiments on four trec collections and query classification experiments on the kdd cup 2005 dataset suggest that the relevance-based word_embedding models significantly_outperform state-of-the-art proximity-based embedding models , such as word2vec and glove .
title : a two-stage machine_learning approach for temporally-robust text_classification ; abstract : one of the most relevant research topics in information_retrieval is automatic document_classification ( adc ) . several adc algorithms have been proposed in the literature . however , the majority of these algorithms assume that the underlying data distribution does not change over time . previous work has demonstrated evidence of the negative_impact of three main temporal effects in representative datasets textual datasets , reflected by variations observed over time in the class_distribution , in the pairwise class similarities and in the relationships between terms and classes [ 1 ] . in order to minimize the impact of temporal effects in adc algorithms , we have previously introduced the notion of a temporal weighting function ( twf ) , which reflects the varying nature of textual datasets . we have also proposed a procedure to derive the twf 's_expression and parameters . however , the derivation of the twf requires the running of explicit and complex statistical_tests , which are very cumbersome or can not even be run in several cases . in this article , we propose a machine_learning methodology to automatically learn the twf without the need to perform any statistical_tests . we also propose new strategies to incorporate the twf into adc algorithms , which we call temporally-aware classifiers . experiments showed that the fully-automated temporally-aware classifiers achieved significant_gains ( up to 17 % ) when compared to their non-temporal counterparts , even outperforming some state-of-the-art algorithms ( e.g. , svm ) in most cases , with large reductions in execution time .
title : an improved text_classification method based on gini_index ; abstract : in text_classification , the purity of the gini_index can be used . when purity value is greater , the characteristic of the information contained in the attribute is higher , and the feature distinguishing capability is stronger . but using the gini purity formula on feature_weight , the classification result is not very good , one of the main reasons is those rare_words only appearing in one category and not appearing in other categories can not be strictly differentiated . in order to solve this problem , on the basis of gini_index , an improved feature_weight method based on gini_index has proposed . by introducing the approximation quality of features term in the categories , according to the category distinguishing ability adjust term_weight , using the purity formula feature_weight comparison , the above problem is well solved , which can effectively_improve the performance of text_classification . the experiments have verified the feasibility of the proposed method . © 2005 - 2012 jatit_&_lls . all rights_reserved .
title : leveraging machine_learning to detect data curation activities ; abstract : this paper describes a machine_learning approach for annotating and analyzing data curation work logs at icpsr , a large social_sciences data archive . the systems we studied track curation work and coordinate team decision-making at icpsr . archive staff use these systems to organize , prioritize , and document curation work done on datasets , making them promising resources for studying curation work and its impact on data reuse , especially in combination with data usage analytics . a key challenge , however , is classifying similar activities so that they can be measured and associated with impact metrics . this paper contributes : 1 ) a set of data curation activities ; 2 ) a computational model for identifying curation actions in work log descriptions ; and 3 ) an analysis of frequent data curation activities at icpsr over time . we first propose a set of data curation actions to help us analyze the impact of curation work . we then use this set to annotate a set of data curation logs , which contain records of data transformations and project_management decisions completed by archive staff . finally , we train a text classifier to detect the frequency of curation actions in a large set of work logs . our approach supports the analysis of curation work documented in work log systems as an important step toward studying the relationship between research data curation and data reuse .
title : inspection text_classification of power_equipment based on textcnn ; abstract : a large number of text and reports about the power_equipment are generated in power system , which consist of implicit information of operation condition and insulation status . with the development of convolutional_neural_network ( cnn ) , the inspection text can be analyzed intelligently to improve the reliability of power system . in order to extract valuable_information from inspection text for state evaluation of power_equipment in local area , an information_extraction model for inspection text based on textcnn is proposed , improved and verified . first , the feature embedding of inspection text were performed by word2vec method . secondly , the corpus were augmented with back translation method . then , the textcnn was adopted to classify the risk level of the power_equipment or area involved in the inspection text . finally , the classification results from the model were evaluated by classification_accuracy , f1 score , confusion_matrix and compared with the model based on bilstm and rcnn . the results demonstrated that the performance of textcnn was the best among the three models on augmented dataset by back translation method with acc and f1 scores of 0.9087 and 0.9099 , respectively , which is the most suitable model among these three for recognition and classification of inspection text of power_equipment .
title : hate_speech detection in thai social_media with ordinal-imbalanced text_classification ; abstract : cyberbullying has become a serious problem in thai social_media . for example , some thai people posted hate speeches on myanmar workers in thailand during the covid-19_pandemic , which might elevate hate_crime . it is imperative and urgent to detect cyberbullying on thai social_media . the task is a text_classification problem . moreover , hate speeches contain the order of severity levels , but many pieces of work did not consider this point in the model . therefore , we developed a thai hate-speech classification method with various loss_functions to detect such hate speeches accurately . we evaluated them on a corpus of ordinal-imbalanced thai text . the evaluated outcomes indicated that the best-in terms of $ f $ 1 -score-model was the model with a loss_function of a hybrid between an ordinal regression loss_function and pearson_correlation coefficients ( common in similarity_function ) . it yielded an average f1-score of 78.38 % -0.88 % significantly higher than the score achieved by a conventional loss_function-and an average mean squared error of 0.2478-5.49 % relative_improvement . thus , the proposed hybrid loss_function improved the efficiency of the model .
title : comparison of feature_selection metrics for classifying pornographic_web pages ; abstract : this paper evaluates four features election metrics ( ptfidf , cc , or , gss ) in th eir application to inverse chi-square based classification of pornographic_web pages . for each metric , an effective feature_selection method is applied to select positive and negative features based on a threshold pair . for each metric , a sub-optimal threshold pair with top classification performance is obtained through systematic iteration of pair combinations . the experimental results indicate that the gss and cc feature_selection metrics produce best classification results . for gss and cc , we propose a heuristic to obtain a threshold pair with close to top performance more efficiently . the classification results for a predefined threshold pair , the iteratively computed sub-optimal threshold pair , and the heuristics generated threshold pair are then compared . experimental results illustrate that the classification performance of the heuristics generated threshold pair is comparable to that of iteratively computed sub-optimal threshold pair .
title : an optimization based feature_extraction and machine_learning techniques for named_entity identification ; abstract : the processing of unstructured and structured documents involves the recognition of specific entity classes in the named_entity_recognition ( ner ) and the categorization of these entities into certain predefined_classes . biomedical instances such as rnas , dnas , disorders , viruses , proteins , genes and chemical components are identified using biomedical_named_entity_recognition ( bner ) . the techniques used to retrieve those other ebontities have a major role to play in this bner . supervised_machine_learning ( sml ) approaches are used in various bner techniques.the primary benefit of supervised_learning is the ability to gather data or generate data output from prior experiences . if your training_set lacks the examples you wish to include in a class , the decision_boundary of your model may be overstretched.the boundary_condition is employed when a particle goes past the region where a boundary constraint is no longer valid.in these approaches , in order to enhance the recognition process 's effectiveness , these features are used . a set of distinguishing and discriminating characteristics are used for identifying features , which is having ability for indicating entity occurrence.bio curators annotates only limited number of articles also consumes more processing time . in this work , propose an enhanced system for curatable-biomedical named_entities recognition ( ecbner ) and feature_extraction approaches for bio-medical named_entity_recognition using aimproved particle_swarm_optimization ( ipso ) . classification of curatable named-entities is useful in facilitating biocuration with a straightforward technique for accelerating workflow of proposed biocuration . curatable and non-curatable are classified using a support_vector_machine ( svm ) in this work.the process of gathering and organizing knowledge , facts , and information in the realm of life sciences is known as “ biocuration. ” in ml , combination of classifiers provides productive exploration guidance and it is a successful strategy of it . an independent classifier 's exhibition in characterization can be improved utilizing this . consequence of different classifiers mix is accumulated to defeat singular classifiers conceivable nearby soft spot for delivering exceptionally strong acknowledgment . quality/disease ner is handled under conditional_random_field ( crf ) and all activity terms are gathered and prepared in a simultaneous way to extricate precise biomedical named substances . at long last , this overall structure to learn portrayal by joining general and area explicit highlights is proposed and assessed , demonstrating exact outcomes contrasted with existing systems .
title : protaugment : unsupervised diverse short-texts paraphrasing for intent_detection meta-learning ; abstract : recent research considers few-shot intent_detection as a meta-learning problem : the model is learning to learn from a consecutive set of small tasks named episodes . in this work , we propose protaugment , a meta-learning algorithm for short texts classification ( the intent_detection task ) . protaugment is a novel extension of prototypical_networks , that limits overfitting on the bias introduced by the few-shots classification objective at each episode . it relies on diverse paraphrasing : a conditional language model is first fine-tuned for paraphrasing , and diversity is later introduced at the decoding stage at each meta-learning episode . the diverse paraphrasing is unsupervised as it is applied to unlabelled_data , and then fueled to the prototypical_network training objective as a consistency loss . protaugment is the state-of-the-art method for intent_detection meta-learning , at no extra labeling_efforts and without the need to fine-tune a conditional language model on a given application domain .
title : prediction of user_ratings of dianping based on k-bert model ; abstract : in the internet era , people usually consider the ratings and reviews of stores on review platforms when choosing travel locations . today , the mainstream rating scheme for total store scores is weighted by review scores , but this scoring system can be negatively affected by malicious scoring and uneven scoring . problems such as incompleteness and other issues will affect the authenticity of the store 's rating . to this end , this paper designs a k-bert dianping user rating prediction based on k-bert model to reflect real review ratings . compared with the traditional bert pre-training model , the k-bert model can solve knowledge-driven problems faster through knowledge_graph injection . in this paper , a dianping knowledge map is established . through the steps of text_preprocessing , text pre-training , and dianping dataset fine-tuning , it is found that the accuracy_rate of the k-bert model in the dianping rating classification is about 95 % . by comparing the model with bert , logistic_regression , it is found that the predicted effect of the k-bert model is significantly better than the above two models .
title : mice : a module for named_entities recognition and classification ; abstract : in the field of corpus_linguistics , named_entity treatment includes the recognition and classification of different types of discursive elements like proper_names , date , time , etc . these discursive elements play an important role in different natural_language_processing applications and techniques such as information_retrieval , information_extraction , translations memories , document routers , etc . © john benjamins publishing company .
title : benchmarking privacy in text_classification ; abstract : in most machine_learning models , the data used for training or testing is public , available to anyone who wishes to see it . new research has improved these models , by adding privacy and distributing the processing load on multiple workers in the cloud . the aim of the paper is to perform an analysis between a classical approach ( in which we have access to all the data ) and one in which the privacy is preserved ( federated_learning ) to explore the cases when a private model can be suitable in real-life scenarios .
title : automated_essay_scoring using multi-classifier fusion ; abstract : the method of multi-classifier fusion was applied to essay scoring . in this paper , each essay was represented by vector_space_model ( vsm ) . after removing the stopwords , we extracted the features of contents and linguistics from the essays , and each vector was expressed by corresponding weight . three classical approaches including document_frequency ( df ) , information gain ( ig ) and chi-square statistic ( chi ) were used to select features by some predetermined thresholds . according to the experimental results , we classified the test essay to appropriate category using different classifiers , such as naive_bayes ( nb ) , k_nearest_neighbors ( knn ) and support_vector_machine ( svm ) . finally the ensemble_classifier was combined by those component classifiers . after training for multi-classifier fusion technique , the experiments on cet4 essays about same topic in chinese learner english corpus ( clec ) show that precision over 73 % was achieved . ©_2011_springer-verlag .
title : constructing a taxonomy for sentiment visualization analysis using visual metaphors ; abstract : background due to the recent development of data_mining and natural_language_processing ( nlp ) technologies , sentiment_analysis targets are more diverse tendencies than just information with affirmative or negative side . accordingly , there are increasing cases of analyzing sentiment information using high-dimensional visualization technology , and these cases are often difficult to understand from a public point of view . in order to solve these problems , there is an increasing movement to visualize sentiment information using visual metaphors . therefore , for the purpose of easier to understand related cases , there is a need for a measure to systematically organize information on their research methods , purposes , and visual metaphors . methods in this study , a taxonomy is proposed that can examine in detail the research process of sentiment_analysis visualization cases based on visual metaphors . first , sentiment visualization cases based on visual metaphors are collected and used as data for constructing a taxonomy . second , selecting the criteria that constitute the taxonomy , and based on the step-by-step analysis work that appears in the metaphor process , the attributes of the criteria are largely divided into five elements ( target , task-oriented intermediation , representation , visual variables and visualization technique ) and detailed sub-elements are selected . third , classification work is performed using an actual study based on the created taxonomy . finally , in order to find the utility and improvements of the taxonomy , an qualitative evaluation is conducted for subjects . results the designed taxonomy in this study provided it easy to understand what kind of sentiment information the visual metaphor from sentiment visualization is based on , what motives or backgrounds the metaphor has progressed , what representation have replaced sentiment information , and how visual variables are performed to add interpretive meaning to the representation . during the verification process , it was estimated that the taxonomy of this study helps to understand inclusively sentiment visualizations using visual metaphors . on the other hand , we also confirmed the need for adding more subcategory level 1 elements to the representation and visualization techniques . furthermore , specifying as well as subdividing the definitions of both criteria 's element is just as necessary . conclusions we expect that the taxonomy proposed in this study can be a guideline to inform researchers of the visual techniques and ideas that is required to become a comprehendible sentiment_analysis visualization result to users . in future work , this study will improve the taxonomy to easily explain the visual metaphor process that appears in different cases . in addition , we will conduct a quantitative evaluation of several related researchers to verify the effectiveness of the classification system and the understanding of the visual metaphor process . finally , more diverse visual metaphor cases will be classified and converted into a database , and we will create a system that can help users to freely explore related cases .
title : milanlp @ wassa : does bert feel sad when you cry ? ; abstract : the paper describes the milanlp team ’ s submission ( bocconi_university , milan ) in the wassa 2021 shared_task on empathy detection and emotion classification . we focus on track 2 - emotion classification - which consists of predicting the emotion of reactions to english news stories at the essay-level . we test different models based on multi-task and multi-input frameworks . the goal was to better exploit all the correlated information given in the data set . we find , though , that empathy as an auxiliary task in multi-task_learning and demographic_attributes as additional input provide worse performance with respect to single-task learning . while the result is competitive in terms of the competition , our results suggest that emotion and empathy are not related_tasks - at least for the purpose of prediction .
title : chi_square feature_extraction based svms arabic_text_categorization system ; abstract : this paper aims to implement a support_vector_machines ( svms ) based text_classification system for arabic_language articles . this classifier uses chi_square method as a feature_selection method in the preprocessing_step of the text_classification system design procedure . comparing to other classification methods , our classification system shows a high classification effectiveness for arabic articles term of macroaveraged fl = 88.11 and microaveraged fl = 90.57 .
title : opinion_mining : is feature_engineering still relevant ? ; abstract : this paper manifests the experimentation with sentiment_polarity detection over stanford 's imdb movie review dataset using a support_vector_machine classifier ( svm ) . our prime motivation was to find out the best possible combinations of classic features and preprocessing_techniques for the classification of positive and negative opinions . we also explored two variants of kernels with numerous parameter_settings for the classifier in the hope of getting the best svm model . our best model achieved an accuracy score of 85.45 % . the results indicate that a model with a non-linear radial_basis_function ( rbf ) kernel leads to the highest_accuracy . the features that contributed the most are stemmed word n-grams .
title : acoustic event classification using convolutional_neural_networks ; abstract : the classification of human-made acoustic events is important for the monitoring and recognition of human activities or critical behavior . in our experiments on acoustic event classification for the utilization in the sector of health_care , we defined different acoustic events which represent critical events for elderly or people with disabilities in ambient assisted_living environments or patients in hospitals . this contribution presents our work for acoustic event classification using deep_learning techniques . we implemented and trained various convolutional_neural_networks for the extraction of deep feature_vectors making use of current best practices in neural_network design to establish a baseline for acoustic event classification . we convert chunks of audio_signals into magnitude spectrograms and treat acoustic events as images . our data set contains 20 different acoustic events which were collected in two different recording sessions combining human and environmental sounds . our results demonstrate how efficient convolutional_neural_networks perform in the domain of acoustic event classification .
title : acquisition of a classification model for a risk search system from unbalanced textual examples ; abstract : this paper proposes a method that acquires a more appropriate classification model for a risk search system analysing corporate reputation information included in bulletin_board sites . the method inductively acquires the model from textual examples composed of many negative_examples and a few positive_examples . it selects two kinds of important negative_examples by referring to expressions related to a specific label . here , the label represents the contents of the papers . finally , the method uses the selected negative_examples and all the positive_examples to acquire the model . the paper verifies the effectiveness of the method through comparative_experiments . copyright © 2009 , inderscience publishers .
title : exploiting term relationship to boost text_classification ; abstract : document_classification provides an effective way to handle the explosive online textual_data . however , in practical classification settings , we face the so-called feature_sparsity problem caused by a lack of training documents or the shortness of text to be classified . in this paper , we solve the sparsity problem by exploiting term relationships along with naive_bayes classifiers . the first method is to estimate term relationships based on the co-occurrence information of two terms in a certain context . the second method estimates the term relationships based on the distribution of terms over different hierarchical categories in a publicly available document taxonomy . thereafter , term relationship is used to augment naive_bayes classifiers . we test our methods on two open-domain data sets to demonstrate its advantages . the experimental results show that our method can significantly_improve the classification performance , especially when we do not have enough training_data or the texts are web_search queries . copyright 2009 acm .
title : dmix : distance constrained interpolative mixup ; abstract : interpolation-based regularisation methods have proven to be effective for various tasks and modalities . mixup is a data augmentation method that generates virtual training_samples from convex combinations of individual inputs and labels . we extend mixup and propose dmix , distance-constrained interpolative mixup for sentence_classification leveraging the hyperbolic_space . dmix achieves state-of-the-art results on sentence_classification over existing data_augmentation methods across datasets in four languages .
title : improved particle_swarm_optimization algorithm and its application in text feature_selection ; abstract : text feature_selection is an importance step in text_classification and directly_affects the classification performance . classic_feature_selection methods mainly include document_frequency ( df ) , information gain ( ig ) , mutual_information ( mi ) , chi-square_test ( chi ) . theoretically , these methods are difficult to get improvement due to the deficiency of their mathematical_models . in order to further improve effect of feature_selection , many researches try to add intelligent optimization_algorithms into feature_selection method , such as improved ant_colony algorithm and genetic_algorithms , etc . compared to the ant_colony algorithm and genetic_algorithms , particle_swarm_optimization algorithm ( pso ) is simpler to implement and can find the optimal point quickly . thus , this paper attempt to improve the effect of text feature_selection through pso . by analyzing current achievements of improved pso and characteristic of classic_feature_selection methods , we have done many explorations in this paper . above all , we selected the common pso model , the two improved pso models based respectively on functional inertia weight and constant constriction factor to optimize feature_selection methods . afterwards , according to constant constriction factor , we constructed a new functional constriction factor and added it into traditional pso model . finally , we proposed two improved pso models based on both functional constriction factor and functional inertia weight , they are respectively the synchronously improved pso model and the asynchronously improved pso model . in our experiments , chi was selected as the basic feature_selection method . we improved chi through using the six pso models mentioned above . the experiment results and significance tests show that the asynchronously improved pso model is the best one among all models both in the effect of text_classification and in the stability of different dimensions .
title : an approach to identify indic languages using text_classification and natural_language_processing ; abstract : india is one of the most culturally and linguistically diverse nations in the world . india stands second in the world for the most languages spoken by its diverse population , who speak their own regional_languages for communication . english is offered as a second additional official_language in india . however , there is a communication gap in india because of how little english is used there . it 's nearly impossible for humans to bridge this breach by translating from one language into another . however , it is possible to translate languages by taking the help of a machine . as per the literature survey , it was observed that neural_machine_translation ( nmt ) is a cutting-edge strategy that significantly outperformed more conventional machine_translation methods for translating one language into another.the main objective of this proposed work is to achieve accurate identification of indic language texts and scripts and provide relevant names of the language after the detection process . the entire work is carried out in stages which includes , collection of the dataset from different sources , preprocessing with the help of data mining techniques , identifying the language of input and in future , approaches like rule_based , statistical and neural_networks will be used followed by post-processing and efficient tasks like machine_translations , named_entity_recognition , etc . will be carried out .
title : literature-based concept profiles for gene annotation : the issue of weighting ; abstract : background : text-mining has been used to link biomedical concepts , such as genes or biological_processes , to each other for annotation purposes or the generation of new hypotheses . to relate two concepts to each other several authors have used the vector_space_model , as vectors can be compared efficiently and transparently . using this model , a concept is characterized by a list of associated concepts , together with weights that indicate the strength of the association . the associated concepts in the vectors and their weights are derived from a set of documents linked to the concept of interest . an important issue with this approach is the determination of the weights of the associated concepts . various schemes have been proposed to determine these weights , but no comparative studies of the different approaches are available . here we compare several weighting approaches in a large_scale classification experiment . methods : three different techniques were evaluated : ( 1 ) weighting based on averaging , an empirical approach ; ( 2 ) the log_likelihood ratio , a test-based measure ; ( 3 ) the uncertainty coefficient , an information-theory based measure . the weighting_schemes were applied in a system that annotates genes with gene_ontology codes . as the gold standard for our study we used the annotations provided by the gene_ontology annotation project . classification performance was evaluated by means of the receiver operating characteristics ( roc ) curve using the area under the curve ( auc ) as the measure of performance . results and discussion : all methods performed well with median auc scores greater than 0.84 , and scored considerably higher than a binary approach without any weighting . especially for the more specific gene_ontology codes excellent_performance was observed . the differences between the methods were small when considering the whole experiment . however , the number of documents that were linked to a concept proved to be an important variable . when larger amounts of texts were available for the generation of the concepts ' vectors , the performance of the methods diverged considerably , with the uncertainty coefficient then outperforming the two other methods . © 2007 elsevier ireland ltd. all rights_reserved .
title : a multi-classification sentiment_analysis model of chinese short text based on gated linear units and attention_mechanism ; abstract : sentiment_analysis of social_media texts has become a research hotspot in information_processing . sentiment_analysis methods based on the combination of machine_learning and sentiment_lexicon need to select features . selected emotional features are often subjective , which can easily lead to overfitted models and poor generalization_ability . sentiment_analysis models based on deep_learning can automatically_extract effective text emotional features , which will greatly_improve the accuracy of text sentiment_analysis . however , due to the lack of a multi-classification emotional corpus , it can not accurately express the emotional_polarity . therefore , we propose a multi-classification sentiment_analysis model , glu-rcnn , based on gated linear units and attention_mechanism . our model uses the gated linear units based attention_mechanism to integrate the local_features extracted by cnn with the semantic features extracted by the lstm . the local_features of short text are extracted and concatenated by using multi-size convolution kernels . at the classification layer , the emotional features extracted by cnn and lstm are respectively concatenated to express the emotional features of the text . the detailed evaluation on two benchmark_datasets shows that the proposed model outperforms state-of-the-art approaches .
title : compassion detection from text : a comparative_analysis using bert , ulmfit and deepmoji ; abstract : compassion is a key quality of the human condition . for computers to become artificial_general_intelligence ( agi ) , they must be taught how to detect and exhibit compassion . detection of compassion requires both a definition and proper approaches . a definition of compassion , which has both cognitive and affective components , can be found in clinical_psychology , encompassing the recognition of suffering and a desire to help , among other aspects . we use this definition to annotate a new original text dataset for compassion detection created by web_crawling reddit communities . this dataset represents the first contribution of this paper . using this new annotated_dataset , we examine three approaches which are currently prevalent in the area of emotion detection ( ed ) , based on bert ( bidirectional_encoder_representations_from_transformers ) , ulmfit ( universal_language model fine-tuning ) and deepmoji ( which uses attention-based bidirectional_lstm without transformers ) . we show that : ( a ) an approach based on feeding the emoji-based embeddings from deepmoji to an ensemble of classifiers performs slightly better than bert and much better than ulmfit on our compassion detection dataset ; ( b ) it is easier to detect the compassionate class than the not compassionate class . we also analyze two other related datasets , one on politeness and one on insults , and find that the approach of feeding the emoji-based embeddings from deepmoji to a downstream classifier is only slightly inferior in performance to a bert classifier , thus warranting consideration in tasks involving affective aspects . this comparative_analysis is the second contribution of this paper toward the ed from text literature .
title : applying machine_learning classifiers in argumentation context ; abstract : group_decision_making is an area that has been studied over the years . group decision_support_systems emerged with the aim of supporting decision_makers in group_decision-making processes . in order to properly support decision-makers these days , it is essential that gdss provide mechanisms to properly support decision-makers . the application of machine_learning techniques in the context of argumentation has grown over the past few years . arguing includes negotiating arguments for and against a certain point of view . from political debates to social_media posts , ideas are discussed in the form of an exchange of arguments . during the last years , the automatic detection of this arguments has been studied and it ’ s called argument_mining . recent advances in this field of research have shown that it is possible to extract arguments from unstructured_texts and classifying the relations between them . in this work , we used machine_learning classifiers to automatically_classify the direction ( relation ) between two arguments .
title : sentiment_classification of customer_reviews about automobiles in roman urdu ; abstract : text_mining is a broad field having sentiment mining as its important constituent in which we try to deduce the behavior of people towards a specific item , merchandise , politics , sports , social_media comments , review_sites etc . out of many issues in sentiment mining , analysis and classification , one major issue is that the reviews and comments can be in different languages like english , arabic , urdu etc . handling each language according to its rules is a difficult task . a lot of research work has been done in english_language for sentiment_analysis and classification but limited sentiment_analysis work is being carried out on other regional_languages like arabic , urdu and hindi . in this paper , waikato environment for knowledge analysis ( weka ) is used as a platform to execute different classification models for text_classification of roman urdu text . reviews dataset has been scrapped from different automobiles sites . these extracted roman urdu reviews , containing 1000 positive and 1000 negative reviews , are then saved in weka attribute-relation file_format ( arff ) as labeled_examples . training is done on 80 % of this data and rest of it is used for testing purpose which is done using different models and results are analyzed in each case . the results show that multinomial_naive_bayes outperformed bagging , deep_neural_network , decision_tree , random_forest , adaboost , k-nn and svm classifiers in terms of more accuracy , precision , recall and f-measure .
title : joint models for answer verification in question_answering systems ; abstract : this paper studies joint models for selecting correct_answer sentences among the top $ k $ provided by answer sentence selection ( as2 ) modules , which are core components of retrieval-based question_answering ( qa ) systems . our work shows that a critical step to effectively exploit an answer set regards modeling the interrelated information between pair of answers . for this purpose , we build a three-way multi-classifier , which decides if an answer supports , refutes , or is neutral with respect to another one . more specifically , our neural_architecture integrates a state-of-the-art as2 model with the multi-classifier , and a joint layer connecting all components . we tested our models on wikiqa , trec-qa , and a real-world dataset . the results show that our models obtain the new state of the art in as2 .
title : opinion_mining using decision_tree based feature_selection through manhattan hierarchical cluster measure ; abstract : opinion_mining plays a major role in text_mining applications in consumer attitude detection , brand and product positioning , customer_relationship_management , and market_research . these applications led to a new generation of companies and products meant for online market perception , reputation_management and online content monitoring . subjectivity and sentiment_analysis focus on private states automatic identification like beliefs , opinions , sentiments , evaluations , emotions and natural_language speculations . subjectivity classification labels data as either subjective or objective , whereas sentiment_classification adds additional granularity through further classification of subjective data as positive/negative or neutral . features are extracted from the data for classifying the sentiment . feature_selection has gained importance due to its contribution to save classification cost with regard to time and computation load . in this paper , the main focus is on feature_selection for opinion_mining using decision_tree based feature_selection . the proposed method is evaluated using imdb data set , and is compared with principal_component_analysis ( pca ) . the experimental results show that the proposed feature_selection method is promising . © 2005 - 2013 jatit_&_lls . all rights_reserved .
title : a classification model of power operation inspection defect_texts based on graph_convolutional_network ; abstract : aiming at the problems of diversification , complexity and islanding of power operation and inspection data and the high dependence of operation and inspection operations on expert experience and normative information , the key technology research of intelligent judgment of defect types of power operation inspection equipment is carried out . for the field of power operation and inspection , the defect_text classification algorithm based on graph convolutional_neural_network is proposed . and the practical tests in a large defect_text network diagram built by main transformer defect_reports are performed . and the proposed model achieves better classification results than 7 benchmark models in the defect_text classification_task . specifically , the accuracy , weighed-precision , and weighed-f1 indicators reach 73.39 , 72.42 , and 72.21 respectively , which improves the model ’ s ability to identify defect types to a greater extent and plays an important role in improving the intelligence and digitalization of power operation and inspection work .
title : global semantic information_extraction model for chinese long text_classification based on fine-tune bert ; abstract : since bidirectional_encoder_representation_from_transformers ( bert ) was proposed , bert has obtained new state-of-the-art results in 11 natural_language_processing ( nlp ) tasks and is the most advanced embedding model available . however , the pre-trained bert model can process the maximum text sequence length is 512 . usually , people use text truncation method to make the sequence length match the preset value . but this processing can result in the loss of global_information and lead to errors . in order to solve the above problem , we use the long short-term_memory ( lstm ) model on top of the bert model for secondary extraction of features , while using the attention_mechanism to optimize global features . this is our proposed bertlstmatt model . the experiment results on thucnews_dataset show that our model has better classification performance than other models .
title : self-supervised regularization for text_classification ; abstract : text_classification is a widely_studied problem and has broad applications . in many real-world problems , the number of texts for training classification models is limited , which renders these models prone to overfitting . to address this problem , we propose ssl-reg , a data-dependent regularization approach based on self-supervised_learning ( ssl ) . ssl is an unsupervised_learning approach which defines auxiliary tasks on input data without using any human-provided labels and learns data representations by solving these auxiliary tasks . in ssl-reg , a supervised classification_task and an unsupervised ssl task are performed simultaneously . the ssl task is unsupervised , which is defined purely on input texts without using any human-provided labels . training a model using an ssl task can prevent the model from being overfitted to a limited number of class_labels in the classification_task . experiments on 17 text_classification datasets demonstrate the effectiveness of our proposed method .
title : learning word_embeddings with chi-square weights for healthcare tweet classification ; abstract : twitter is a popular source for the monitoring of healthcare information and public disease . however , there exists much noise in the tweets . even though appropriate keywords appear in the tweets , they do not guarantee the identification of a truly health-related tweet . thus , the traditional keyword-based classification_task is largely ineffective . algorithms for word_embeddings have proved to be useful in many natural_language_processing ( nlp ) tasks . we introduce two algorithms based on an existing word_embedding learning algorithm : the continuous bag-of-words model ( cbow ) . we apply the proposed algorithms to the task of recognizing healthcare-related_tweets . in the cbow model , the vector representation of words is learned from their contexts . to simplify the computation , the context is represented by an average of all words inside the context window . however , not all words in the context window contribute equally to the prediction of the target word . greedily incorporating all the words in the context window will largely limit the contribution of the useful semantic words and bring noisy or irrelevant_words into the learning process , while existing word_embedding algorithms also try to learn a weighted cbow model . their weights are based on existing pre-defined syntactic rules while ignoring the task of the learned embedding . we propose learning weights based on the words ' relative importance in the classification_task . our intuition is that such learned weights place more emphasis on words that have comparatively more to contribute to the later task . we evaluate the embeddings learned from our algorithms on two healthcare-related datasets . the experimental results demonstrate that embeddings learned from the proposed algorithms outperform existing techniques by a relative accuracy_improvement of over 9 % .
title : statistical bayesian learning for automatic arabic_text_categorization ; abstract : approach : problem_statement : the rapid increasing of online arabic documents necessitated applying text_categorization techniques which are commonly used for english_language to categorize them automatically . the complex_morphology of arabic_language and its large_vocabulary size make using these techniques difficult and costly in time and effort . approach : we have investigated bayesian learning models in order to enhance arabic atc . three classifiers based on bayesian theorem had been implemented which are simple naïve_bayes ( nb ) , multi-variant bernoulli naïve_bayes ( mbnb ) and multinomial_naïve_bayes ( mnb ) models . trec-2002 light stemmer was applied for arabic stemming . for text_representation , bow and character-level 3 , 4 and 5 g had been used . in order to reduce the dimensionality of feature_space , we have used several feature_selection methods ; mutual_information ( mi ) , chi-square statistic ( chi ) , odds_ratio ( or ) and gss-coefficient ( gss ) . conclusion : mbnb classifier outperforms both of nb and mnb classifiers . bow_representation type leads to the best classification performance , nevertheless using character_level n-gram leads to satisfied results by bayesian learning for arabic atc . © 2011 science publications .
title : an exploration of semi-supervised text_classification ; abstract : good performance in supervised text_classification is usually obtained with the use of large amounts of labeled_training_data . however , obtaining labeled_data is often expensive and time-consuming . to overcome these limitations , researchers have developed semi-supervised_learning ( ssl ) algorithms exploiting the use of unlabeled_data , which are generally easy and free to access . with ssl , unlabeled and labeled_data are combined to outperform supervised-learning algorithms . however , setting up ssl neural_networks for text_classification is cumbersome and frequently based on a trial and error process . we show that the hyperparameter configuration significantly impacts ssl performance , and the learning_rate is the most influential parameter . additionally , increasing model size also improves ssl performance , particularly when less pre-processing data are available . interestingly , as opposed to feed-forward models , recurrent models generally reach a performance threshold as pre-processing data size increases . this article expands the knowledge on hyperparameters and model size in relation to ssl application in text_classification . this work supports the use of ssl work in future nlp projects by optimizing model design and potentially lowering training time , particularly if time-restricted .
title : ben-cnn-bilstm : a model of consequential document set identification of bengali text ; abstract : document set identification assigns a text to its predefined text set . therefore , the objective of any classification work is to create a model that can categorise various texts and objects into distinct classes . in this paper , three consequential deep_learning based models , viz . bilstm , bilstm with attention_layer , and cnn-bilstm have been used , which have the auto-learning capability in bengali corpora . the cnn-bilstm model has been designated the classification model for its best performance in categorising bengali text documents . this consequential model , named ben-cnn-bilstm , was learned with bengali text documents to determine the category of an unknown bengali document . at first , more than four lac news articles from renowned bengali newspapers are processed . after that , the training_data is processed and entered into the proposed model . finally , the model performance is assessed using the test dataset to calculate recall , precision , f-score , and accuracy . compared to other standard classification_algorithms in bengali text_classification , our proposed ben-cnn-bilstm model achieved 93.94 % accuracy . thus , it can be said that the proposed ben-cnn-bilstm model can be a new document set identification technique for bengali datasets .
title : contrastive_learning with heterogeneous_graph attention networks on short_text_classification ; abstract : graph neural_networks ( gnns ) have attracted extensive interest in text_classification tasks due to their expected superior performance in representation_learning . however , most existing_studies adopted the same semi-supervised_learning setting as the vanilla graph convolution network ( gcn ) , which requires a large amount of labelled_data during training and thus is less robust when dealing with large-scale graph data with fewer labels . additionally , graph structure information is normally captured by direct information aggregation via network schema and is highly dependent on correct adjacency information . therefore , any missing adjacency knowledge may hinder the performance . addressing these problems , this paper thus proposes a novel method to learn a graph structure , nc-hgat , by expanding a state-of-the-art self-supervised heterogeneous_graph neural_network model ( hgat ) with simple neighbour contrastive_learning . the new nc-hgat considers the graph structure information from heterogeneous graphs with multilayer_perceptrons ( mlps ) and delivers consistent results , despite the corrupted neighbouring connections . extensive_experiments have been implemented on four benchmark short-text datasets . the results demonstrate that our proposed model nc-hgat significantly_outperforms state-of-the-art methods on three datasets and achieves_competitive_performance on the remaining dataset .
title : incremental active opinion learning over a stream of opinionated documents ; abstract : applications that learn from opinionated documents , like tweets or product_reviews , face two challenges . first , the opinionated documents constitute an evolving stream , where both the author 's attitude and the vocabulary itself may change . second , labels of documents are scarce and labels of words are unreliable , because the sentiment of a word depends on the ( unknown ) context in the author 's mind . most of the research on mining over opinionated streams focuses on the first aspect of the problem , whereas for the second a continuous supply of labels from the stream is assumed . such an assumption though is utopian as the stream is infinite and the labeling_cost is prohibitive . to this end , we investigate the potential of active stream learning algorithms that ask for labels on demand . our proposed acostream 1 approach works with limited labels : it uses an initial seed of labeled_documents , occasionally requests additional labels for documents from the human expert and incrementally adapts to the underlying stream while exploiting the available labeled_documents . in its core , acostream consists of a mnb classifier coupled with `` sampling '' strategies for requesting class_labels for new unlabeled_documents . in the experiments , we evaluate the classifier performance over time by varying : ( a ) the class_distribution of the opinionated stream , while assuming that the set of the words in the vocabulary is fixed but their polarities may change with the class_distribution ; and ( b ) the number of unknown_words arriving at each moment , while the class polarity may also change . our results show that active_learning on a stream of opinionated documents , delivers good performance while requiring a small selection of labels
title : impact of polarity in deception_detection ; abstract : usually , most works use and combine different methods for generating features in order to improve deception_detection ; nevertheless , they do not take into account the fact that features may change depending on the nature of text . in this research , a study on the effect of the polarity over the set of features generated for deception_detection task was carried out . we implemented a polarity classifier to generate subsets of positive and negative opinions . next , a semantic and lexical method were used over the subsets to generate features and construct vectors . it was proven that adding polarity information did not positively impacted on deception_detection . however , partitioning datasets improved classification results . to classify subsets , attribute_selection was implemented and a bayesian classifier was fed with the resulting vectors . research findings show that cues to deception are affected by the opinion_polarity . in addition , this approach registered up to 86 % f-measure .
title : discovery of twitter user interestingness based on retweets , reply mentions and pure mentions relationships ; abstract : with the rising popularity of social_media platforms such twitter , sentiment_classification for social_media has become a hot research topic . there were many research studies conducted on twitter as it is one of the most widely used social_media . previous_studies have approached the problem as a tweet-level classification_task where each tweet is classified as being positive , negative or neutral . however , getting an overall sentiment might not be useful to a business organizations which are using twitter for monitoring consumer opinion of their products or services . it is more useful to determine specifically which tweets where users are happy or unhappy about . this paper proposes the discovery of twitter user level interestingness based on relationships such as retweets , reply-mentions and pure-mentions using google 's pagerank algorithm . we conducted experiments for telecommunications companies related_tweets and compared the results with hard-marked results by seven annotators .
title : iiith at bioasq challenge 2015 task 3a : extreme classification of pubmed articles using mesh labels ; abstract : automating the process of indexing journal abstracts has been a topic of research for several years . biomedical semantic indexing aims to assign correct mesh_terms to the pubmed documents . in this paper we report our participation in the task 3a of bioasq challenge 2015 . the participating teams were provided with pubmed articles and asked to return relevant mesh_terms . we tried three different approaches : nearest_neighbours , idf-ratio based indexing and multi-label_classification . the official challenge results demonstrate that we consistently performed better than the baseline approaches for task 3a .
title : a comparative study on chinese open_domain suggestion_mining ; abstract : as a new research task , suggestion_mining increasingly gained attention in recent_years . however , it is still open and challenging due to complex semantics , large diversity of domains , and the absence of large labeled and balanced_datasets . more importantly , most of the research is focused on english in-domain suggestion_mining . but as compared to english , chinese suggestion has more abundant expression forms , showing many different characteristics , so it is the necessity to carry out suggestions mining research in chinese environment . in this work , the performances of several classification models for chinese suggestion_mining were compared . firstly , a chinese suggestion_mining corpus was constructed for open_domain , and then trained several models for the suggestion_mining , which included both traditional_machine_learning models ( feature_engineering-based models ) and deep_learning models . our results demonstrated that these models can successfully classify chinese sentences into two classes : suggestion and non-suggestion . the results of this study can guide future_research in chinese open_domain suggestion_mining .
title : scene_classification , data cleaning , and comment summarization for large-scale location databases ; abstract : this paper presents a framework that can automatically analyze the images and comments in user-uploaded location databases . the proposed framework integrates image_processing and natural_language_processing techniques to perform scene_classification , data cleaning , and comment summarization so that the cluttered information in user-uploaded databases can be presented in an organized way to users . for scene_classification , rgb image features , segmentation features , and the features of discriminative objects are fused with an attention_module to improve classification_accuracy . for data cleaning , incorrect images are detected using a multilevel feature_extractor and a multiresolution distance calculation scheme . finally , a comment summarization scheme is proposed to overcome the problems of unstructured sentences and the improper usage of punctuation marks , which are commonly found in customer_reviews . to validate the proposed framework , a system that can classify and organize scenes and comments for hotels is implemented and evalu-ated . comparisons with existing related studies are also performed . the experimental results validate the effectiveness and superiority of the proposed framework .
title : analysis of eeg frequency bands for envisioned speech_recognition ; abstract : the use of automatic_speech_recognition ( asr ) interfaces have become increasingly_popular in daily life for use in interaction and control of electronic devices . the interfaces currently being used are not feasible for a variety of users such as those suffering from a speech_disorder , locked-in_syndrome , paralysis or people with utmost privacy requirements . in such cases , an interface that can identify envisioned speech using electroencephalogram ( eeg ) signals can be of great benefit . various works targeting this problem have been done in the past . however , there has been limited work in identifying the frequency bands ( $ \delta , \theta , \alpha , \beta , \gamma $ ) of the eeg signal that contribute towards envisioned speech_recognition . therefore , in this work , we aim to analyze the significance of different eeg frequency bands and signals obtained from different lobes of the brain and their contribution towards recognizing envisioned speech . signals obtained from different lobes and bandpass filtered for different frequency bands are fed to a spatio-temporal deep_learning architecture with convolutional_neural_network ( cnn ) and long short-term_memory ( lstm ) . the performance is evaluated on a publicly available dataset_comprising of three classification_tasks - digit , character and images . we obtain a classification_accuracy of $ 85.93\ % $ , $ 87.27\ % $ and $ 87.51\ % $ for the three tasks respectively . the code for the implementation has been made available at https : //github.com/ayushayt/imaginedspeechrecognition .
title : irony and stereotype spreading author profiling on twitter using machine_learning : a bert-tfidf based approach ; abstract : in this paper we introduce our system for the task of determining whether an author spreads irony and stereotype in english tweets or not , a part of pan 2022 ( irostereo ) task . for the irony spreading author classification_task , 600 authors each containing 200 tweets have been used . the uniqueness of the task is that it is not a classification between ironic and non ironic tweets , instead it is a classification of irony and non irony spreading authors . the task contains a subtask also that addresses stereotype stance_detection . for the previous years , several representation methods like character/word n-grams etc . have been used for tweet representations , but there was not a clear clue whether a combination of other representations would be helpful . to do this end , we introduce bert combined with tfidf representation to address this specific problem . later we used logistic_regression classifier for the classification_task . it was seen that the bert representation combined with tfidf showed very promising_results .
title : an improved ml-knn approach for multi-label text_categorization ; abstract : conventional knn algorithms ignore label correlations when being applied to multi-label text_categorization . to cover this shortage , an improved multi-label knn approach for text_categorization is proposed . a specific distance_metric based on kl divergence is derived to measure the similarity between individual documents . based on statistical information gained from the label sets of neighboring documents , a fuzzy maximum a posteriori principle is utilized to conjecture the label sets of the unlabeled_documents . different from ml-knn , the proposed approach can exploit label correlations to improve classification performance effectively . experiments on three benchmark_datasets using 5 popular multi-label evaluation_metrics suggest that the proposed approach achieves superior performance to some well-established multi-label learning algorithms , such as ml-knn , rank-svm and boostexter .
title : enhancing performance of naïve_bayes in text_classification by introducing an extra weight using less number of training_examples ; abstract : this paper presents an effective and efficient method for classifying text documents in order to deliver feasible information_retrieval using naïve_bayes algorithm . today lots of algorithms have earned good score in the field of information_retrieval , naïve_bayes is one of them . in this paper , a weight_matrix is introduced during training text documents which is combination of term_frequency ( tf ) and inverse_class_frequency ( icf ) and later this weighted term is powered by a significant number and added with the posteriori value during the prediction time of naïve_bayes ( nb ) algorithm to establish a better and efficient performance of the classification_task . here the precedence base element tf results an additional weight for each term ( word ) of the text . on the other hand , icf gives each common word a low score . finally the combinational term 'weight_matrix ' gives an extra weight and balances weight where necessary . as a result , improve the performance accuracy of the nb classifier . experimental results show that nb with weight_matrix rarely demotes accuracy compared to standard naïve_bayes , instead of enhancing accuracy dramatically .
title : classifying homographs in japanese social_media texts using a user interest model ; abstract : the analysis of text data from social_media is hampered by irrelevant noisy_data , such as homographs . noisy_data is not usable and makes analysis , such as counting estimates , of the target data difficult , which adversely affects the quality of the analysis results . we focus on this issue and propose a method to classify homographs that are contained in social_media texts ( i.e . twitter ) using topic_models . we also report the results of an evaluation experiment . in the evaluation experiment , the proposed method showed an accuracy_improvement of 8.5 % and a reduction of 16.5 % in the misidentification rate compared with conventional methods .
title : combining heterogeneous classifiers for word_sense_disambiguation ; abstract :
title : insta-vax : a multimodal benchmark for anti-vaccine and misinformation posts detection on social_media ; abstract : sharing of anti-vaccine posts on social_media , including misinformation posts , has been shown to create confusion and reduce the publics confidence in vaccines , leading to vaccine hesitancy and resistance . recent_years have witnessed the fast rise of such anti-vaccine posts in a variety of linguistic and visual forms in online networks , posing a great challenge for effective content_moderation and tracking . extending previous work on leveraging textual_information to understand vaccine information , this paper presents insta-vax , a new multi-modal dataset consisting of a sample of 64,957 instagram posts related to human vaccines . we applied a crowdsourced annotation procedure verified by two trained expert judges to this dataset . we then bench-marked several state-of-the-art nlp and computer vision classifiers to detect whether the posts show anti-vaccine attitude and whether they contain misinformation . extensive_experiments and analyses demonstrate the multimodal models can classify the posts more accurately than the uni-modal models , but still need improvement especially on visual context understanding and external_knowledge cooperation . the dataset and classifiers contribute to monitoring and tracking of vaccine discussions for social scientific and public_health efforts in combating the problem of vaccine misinformation .
title : multi-attribute classification of text documents as a tool for ranking and categorization of educational innovation projects ; abstract : we suggest a semi-automatic text processing method for ranking and categorization of educational innovation projects ( eip ) . the eip is a nation-wide program for strategic development of an university or a group of academic institutions . our approach to the eip evaluation is based on the multi-dimensional system ranking that uses quantitative indicators for three main missions of higher_education institutions , namely , education , research , and knowledge_transfer . the main part of this paper is devoted to the design of a semi-automatic method for ranking the eips exploiting multi-attribute document_classification . the ranking methodology is based on the generalized borda voting_method . © 2014 springer-verlag_berlin_heidelberg .
title : combining statistical and semantic features for trajectory point classification ; abstract : trajectory point classification can be described as a supervised sequence labeling problem , in which a model is trained by labeling data to predict the category of unknown points and identify key events in the trajectory . due to the difficulty of labeling trajectory point , a large amount of trajectory data is either unlabeled or labeled in an imbalanced way . to make matters worse , traditional trajectory point classification methods are generally constrained to utilize the statistical features of the labeled_data and the semantic features as well as the large amount of unlabeled_data have not been well studied yet . for this reason , the performance of traditional trajectory point classification methods is far from satisfactory . to solve this problem , we transfer existing language model knowledge to construct the semantic features and construct a trajectory point classification model by combining both the motion features and semantic features . the simulation results show that , compared with the traditional_methods , our method has improved the accuracy of trajectory point classification by three and seven percentage_points in the classification of circular and turning movements respectively .
title : indonesian medical question_classification with pattern_matching ; abstract : indonesian medical question_answering system requires the extraction of named_entity_recognition process . this research aims to propose and evaluate a systematic approach to classify problem , intervention , comparison and outcome ( pico ) from the indonesian medical sentences . we here declare that the extraction using the pico frames for indonesian medical sentences is the first . the advantage of pico frame is to accelerate the classification process based on problem intervention , comparison , and outcome criteria . our strategy here was to build a combining question term with multiple classifiers and repetition . the training and test data were generated automatically from indonesia medical literature with 200 sentences by the exact pattern match of head words of p-i-c-o categories . this approach achieved f-measure values of 0.90 for problem and intervention ; 0.89 for problem , intervention , and comparison ; 0.91 for problem , comparison and outcome . it then can be concluded that by the pattern in matching criteria of the training_set and the classification of pico elements is reproducible with minimal expert intervention .
title : leveraging pre-trained contextualized_word_embeddings to enhance sentiment_classification of drug reviews ; abstract : traditionally , pharmacovigilance data are collected during clinical_trials on a small_sample of patients and are therefore insufficient to adequately assess drugs . nowadays , consumers use online drug forums to share their opinions and experiences about medication . these feedbacks , which are widely available on the web , are automatically analyzed to extract relevant_information for decision-making . currently , sentiment_analysis methods are being put forward to leverage consumers ' opinions and produce useful drug monitoring indicators . however , these methods ' effectiveness depends on the quality of word_representation , which presents a real challenge because the information contained in user_reviews is noisy and very subjective . over time , several sentiment_classification problems use machine_learning methods based on the traditional bag of words model , sometimes enhanced with lexical_resources . in recent_years , word_embedding models have significantly_improved classification performance due to their ability to capture words ' syntactic and semantic properties . unfortunately , these latter models are weak in sentiment_classification tasks because they are unable to encode sentiment information in the word_representation . indeed , two words with opposite polarities can have close word_embeddings as they appear together in the same context . to overcome this drawback , some studies have proposed refining pre-trained_word_embeddings with lexical_resources or learning word_embeddings using training_data . however , these models depend on external_resources and are complex to implement . this work proposes a deep contextual_word_embeddings model called elmo that inherently captures the sentiment information by providing separate vectors for words with opposite polarities . different variants of our proposed model are compared with a benchmark of pre-trained_word_embeddings models using svm classifier trained on drug review dataset . experimental results show that elmo embeddings improve classification performance in sentiment_analysis tasks on the pharmaceutical domain .
title : adaptive conversation system based on script : first work : construct script with vector and classify it with sift ; abstract : our work is about conversation system based on script , supposing to design an adaptive agent . purpose of this paper is to verify our idea of the transforming from text to image , and to implement an approach to find a new way of feature characterization on conversation text for our system . with sent2vec and sift , we designed a way of expression of text features and conducted experiments to classify conversation situations as image_processing . good results in close tests and bad results in open tests indicate the necessity of further work on our idea and the insignificance of the new feature characterization .
title : chinese text_classification for small_sample set ; abstract : text_classification is one of the most important topics in the fields of internet information_management and natural_language_processing . machine_learning based text_classification methods are currently most popular ones with better performance than rule_based ones . but they always need lots of training_samples , which not only brings heavy work for previous manual classification , but also puts_forward a higher request for storage and computing resources during the computer post-processing . naïve_bayes algorithm is one of the most effective methods for text_classification with the same problem . only in the large training_sample set can it get a more accurate result . this paper mainly studies naïve_bayes classification algorithm for chinese text based on poisson_distribution model and feature_selection . the experimental results have shown that this method keeps high classification_accuracy even in a small_sample set . © 2011 the journal of china universities of posts and telecommunications .
title : generative_adversarial learning with negative data augmentation for semi-supervised text_classification ; abstract : in recent_years , semi-supervised generative_adversarial_networks ( ss-gans ) models such as gan-bert have achieved promising_results on the text_classification task . one of the techniques used in these models to mitigate the generator from mode collapse is feature matching ( fm ) . although fm addresses some of the critical issues of ss-gans , these models still suffer from mode collapse with missing coverage outside the data manifold . moreover , fm loosely tries to match the distribution between the real data and the fake generated samples . by doing this , the generator can generate fake samples inside high-density regions in the data manifold , where the discriminator learns to misclassify them as out-of-data-manifold regions . in this work , we employ the negative data augmentation ( nda ) technique , for the first time in text_classification , to alleviate the mentioned problems . nda is a unique way of producing out-of-distribution fake examples by applying mixup transformation on the fake samples and augmented real data . in our new model ( nda-gan ) , we produce nda samples by combining the genera-tor ’ s output with the contextual representation of the real data . as a result of the mixing , nda samples are less likely to place in the high-density regions , and due to blending with real data representations , these samples reasonably preserve a close distance to the data manifold . consequently , the nda samples increase the dis-criminator ’ s power to find the optimal decision_boundary . our experimental results demonstrate that the negative augmented_samples improve the overall accuracy of our proposed model and make it more confident when detecting out-of-distribution samples .
title : active_learning with complementary sampling for instructing class-biased multi-label text emotion classification ; abstract : high-quality corpora have been very scarce for the text emotion research . existing corpora with multi-label emotion annotations have been either too small or too class-biased to properly support a supervised emotion learning . in this paper , we propose a novel active_learning method for efficiently instructing the human annotations for a less-biased and high-quality multi-label emotion corpus . specifically , to compensate annotation for the minority-class examples , we propose a complementary sampling_strategy based on unlabeled resources by measuring a probabilistic distance between the expected emotion label distribution in a temporary corpus and an uniform distribution . qualitative evaluations are also given to the unlabeled_examples , in which we evaluate the model uncertainties for multi-label emotion predictions , their syntactic representativeness for the other unlabeled_examples , and their diverseness to the labeled_examples , for a high-quality sampling . through active_learning , a supervised emotion classifier gets progressively improved by learning from these new examples . experiment results suggest that by following these sampling_strategies we can develop a corpus of high-quality examples with significantly relieved bias for emotion classes . compared to the learning procedures based on traditional active_learning algorithms , our learning procedure indicates the most efficient learning_curve and estimates the best multi-label emotion predictions .
title : sentiment based multi-index integrated scoring method to improve the accuracy of recommender system ; abstract : to the best of our knowledge , few studies have focused on the inconsistency between user_ratings and reviews as well as natural noise management in recommender_systems ( rss ) . to address these issues , this study introduces a sentiment based multi-index integrated scoring method to provide a reliable information input that reflects comprehensive user_preferences for recommendation algorithms and facilitate improved performance . initially , bing liu 's lexicon is expanded using a semi-supervised_learning technique to obtain additional sentiment words and calculate the sentiment_scores of reviews ; then a normalized sentiment_score method based on sigmoid_function that considers the emotional_tendencies of different users in reviews is designed to convert the scores into values corresponding to the rating scale of rs . subsequently , a degree classification criteria approach is adopted to assign users and items to more fine-grained classes further , a natural noise detection method is exploited to identify and correct noise ratings according to classification conditions . to effectively integrate normalized review and denoised rating information , two factors , user consistency and review feedback , are considered to obtain the importance of reviews and ratings ; then , a weighted_average method is used to generate a set of comprehensive ratings . the experimental results on two benchmark_datasets indicate that the superiority of memory-based or model-based collaborative_filtering methods ( cfs ) using comprehensive ratings over their respective methods using original ratings is determined by various accuracy metrics , which demonstrates that our scheme can enhance the reliability and accuracy of user information . thus , the proposed scheme provides new insights for improving the accuracy of rss from the perspective of multiple information sources . additionally , this method exhibits good generalizability and practicality .
title : multi-task deep_learning for legal document translation , summarization and multi-label_classification ; abstract : the digitalization of the legal domain has been ongoing for a couple of years . in that process , the application of different machine_learning ( ml ) techniques is crucial . tasks such as the classification of legal_documents or contract clauses as well as the translation of those are highly_relevant . on the other side , digitized documents are barely accessible in this field , particularly in germany . today , deep_learning ( dl ) is one of the hot_topics with many publications and various applications . sometimes it provides results outperforming the human level . hence this technique may be feasible for the legal domain as well . however , dl requires thousands of samples to provide decent results . a potential solution to this problem is multi-task dl to enable transfer_learning . this approach may be able to overcome the data scarcity problem in the legal domain , specifically for the german_language . we applied the state of the art multi-task model on three tasks : translation , summarization , and multi-label_classification . the experiments were conducted on legal document corpora utilizing several task combinations as well as various model parameters . the goal was to find the optimal configuration for the tasks at hand within the legal domain . the multi-task dl approach outperformed the state of the art results in all three tasks . this opens a new direction to integrate dl technology more efficiently in the legal domain .
title : extracting parallel phrases from comparable_corpora ; abstract : the state-of-the-art statistical_machine_translation models are trained with the parallel_corpora . however , the traditional smt loses its power when it comes to language pairs with few bilingual resources . this paper proposes a novel method that treats the phrase_extraction as a classification_task . we first automatically generate the training and testing phrase_pairs for the classifier . then , we train a svm classifier which can determine the phrase_pairs are either parallel or non-parallel . the proposed approach is evaluated on the translation task of chinese-english . experimental results show that the precision of the classifier on test sets is above 70 % and the accuracy is above 98 % the quality of the extracted data is also evaluated by measuring the impact on the performance of a state-of-the-art smt system , which is built with a small parallel_corpus . it shows better results over the baseline system .
title : exploring contextual word_representation for arabic question_classification ; abstract : identifying the category of a question is an essential task in question_answering systems . contextual continuous word_representation proved to be effective in various natural_language_processing tasks . however , these models have not been considered in the field of arabic question_classification . in this paper , we investigate the use of a contextual word_representation named embeddings from language models ( elmo ) on arabic question_classification . we study the behaviour of this representation by building numerous neural_network architectures trained to classify questions . the dataset contains 3173 questions annotated with two taxonomies including arabic taxonomy and an updated li roth taxonomy . by comparing to enriched word2vec with subword information technique , which is a context-free representation , we show that elmo representation achieves better performance at the cost of reduced sized word_vector . the best classifier achieves up to 94 % in terms of accuracy , macro f 1 score , and weighted f 1 score .
title : artificial bee colony optimization for feature_selection in opinion_mining ; abstract : opinion_mining , a sub-discipline of information_retrieval and computational_linguistics concerns not with what a document is about , but with its expressed opinion . feature_selection is an important step in opinion_mining , as customers express product opinions separately according to individual features . earlier research on feature-based opinion_mining had many drawbacks like selecting a feature considering only grammatical_information or treating features with same meanings as different . however this led to a large corpus which subsequently affected the classification_accuracy . statistical techniques like correlation based feature ( cfs ) have been extensively used for feature_selection to reduce the corpus size . the selected features are sub optimal due to the non polynomial ( np ) hard nature of the technique used . in this work , we propose artificial bee colony ( abc ) algorithm for optimization of feature_subset . naïve_bayes , fuzzy unordered rule_induction algorithm ( furia ) and ripple down rule learner ( ridor ) classifiers are used for classification . the proposed method is compared with features extracted based on inverse_document_frequency ( idf ) . hence , this method is useful for reducing feature_subset size and computational_complexity thereby increasing the classification_accuracy . © 2005 - 2014 jatit_&_lls . all rights_reserved .
title : review on sentiment_analysis on music ; abstract : in this study , we present a review on how lyrics can be prove its usefulness in mood classification utilizing the features like linguistic lyric feature_set and text stylistic feature_set . from this study , one may be able to understand the concept and process of sentiment_analysis on the basis of lyrics . some psychological tools like big five inventory and cac scale need to be studied to understand which personality_type may generate what perception to a particular song . big five inventory gives the measure of personality_type and cac scale gives the measure of perception . performance of lyric feature_sets was measured individually as well as by combining them . fusion methods were used to measure performance of combined feature_sets . basically , two fusion methods used were feature concatenation and late_fusion . additionally , examination of learning curves was done which proves that the less number of training_samples were required for lyrics+audio system . the discoveries have shown the best class in lyrics sentiment_analysis and musical mood classification .
title : categorizing temporal events : a case_study of domestic_terrorism ; abstract : in many emergency incidents , multiple reports and information sources are often used to help intelligence and security personnel to understand the situation during a short time period . proper categorization and analysis of this information could enhance the efficiency of handling this large amount of potentially conflicting information , thus contributing to saving lives . the study of categorization of temporal events in cyber_security application is , however , not widely found . in this research , we developed an automated approach to categorizing temporal events described in textual_documents . the approach consists of automatic indexing , term_extraction , and automatic categorization . we conducted a case_study of domestic_terrorism where we analyzed 96 online_news articles about a shooting tragedy that resulted in 6 deaths and 1 seriously injured . analyses of different numbers of extracted textual_features ( from 20 to 100 ) used in the temporal categorization revealed a gradual improvement of classification_accuracies across different algorithms used . naïve_bayes and svm classification provided stable improvement ( from 47 % to 68 % ) , whereas neural_network had the highest_accuracy when 70 features were used . the results provide new insights for researchers and intelligence personnel to understand the relationship between textual_features and emergency_event evolution . © 2012 ieee .
title : improving quality of email categorization with tournament methods ; abstract : the tournament text_classification methods are proposed in this article to perform the task of email categorization , in which the essence is to break down the multi-class categorization process into a set of binary_classification tasks . we implement the methods of elimination and round_robin_tournament to classify emails within 15 folders . substantial experiments are conducted to compare the effectiveness and robustness of the tournament methods against the n-way classification method and the voting_method . the experimental results show that the tournament methods outperform the n-way method by 11.7 % and the voting_method by 4.1 % regarding precision , and the round_robin performs slightly better than the elimination tournament on average .
title : a new 3d convolutional_neural_network ( 3d-cnn ) framework for multimedia event_detection ; abstract : multimedia event_detection has received a great deal of interest due to developments in video technology and an increase in multimedia data . however , complexities of video content such as noisy , overlapping , repeated interaction between individuals , and various scenes are becoming difficult for characterizing the subjects and concepts . in particular , internet users find it difficult to search for a specified event . to solve the above problem , a method is proposed that best suits for event_detection , demonstrating the 3d convolutional_neural_network ( 3d-cnn ) structure to accomplish promising performance in multimedia event classification . to take an advantage of motion content of the event in the video , temporal axis is considered . both the feature_extraction and classification are incorporated in this model . experiments are carried out on the columbia consumer video benchmark_dataset , and results are compared with other existing_works .
title : independent_component_analysis based on natural gradient algorithm for text_mining ; abstract : the volume of texts produced daily is growing significantly . computers have difficulty processing and interpreting this considerable amount of largely unstructured_data without using feasible techniques . efficient and effective techniques and algorithms are important in exploring useful patterns . text_mining ( tm ) , the process of extracting useful knowledge from text , is gaining considerable research attention . in this article , we explain the natural gradient independent_component_analysis ( ngica ) of text_classification ( tc ) , which is an application of tm involving the preprocessing of text data followed by classification . an unsupervised framework of machine-learning classification is proposed based on ngica for tc . a manually_labelled dataset is used to assess the proposed model , and its validity is proven through experiments . the proposed model achieves an overall f-measure of 83 % for tc . results show the potential of the proposed model for compared with the infomax method that provided 81 % of an overall f-measure for tc .
title : multi-class emotions classification by sentic levels as features in sentiment_analysis ; abstract : sentiment_analysis has become a critical research area in recent days and pervasive in real_life . considering the identification of emotions from textual_content , we propose the hourglass of emotions as the feature that comes from the intensity of affective dimensions and combination thereof . thus , based on a news dataset labeled with six primary emotions , we intend to solve the multi-class classification problem comparing decomposition methods - one against all and one against one - and several aggregation methods . as base_classifiers algorithms , we adopted support_vector_machine , naive_bayes , decision_tree and random_forests . anchored on the results , we found that it is feasible to use this new set of features . the combination of support_vector_machine and weng pairwise coupling method was the best one , producing an accuracy of 55.91 % .
title : a reranking approach for recognition and classification of speech input in conversational dialogue_systems ; abstract : we address the challenge of interpreting spoken input in a conversational dialogue system with an approach that aims to exploit the close relationship between the tasks of speech_recognition and language understanding through joint_modeling of these two tasks . instead of using a standard pipeline approach where the output of a speech_recognizer is the input of a language understanding module , we merge multiple speech_recognition and utterance classification hypotheses into one list to be processed by a joint reranking model . we obtain substantially improved performance in language understanding in experiments with thousands of user_utterances collected from a deployed spoken_dialogue system . © 2012 ieee .
title : soft labeling constraint for generalizing from sentiments in single domain ; abstract : in this work , we deal with domain_generalization in sentiment_analysis . in traditional domain_generalization systems , multiple source_domains are used to generalize to a single target domain . however , we tackle the scenario where examples of sentiments from only one domain are available . recent_works have proposed to generate target domain examples from a single source_domain by means of an adversarial_training , ensuring that generated examples performs well on classifier trained on source_domain . however , the inherent assumption is that domain_shift is only due to covariate shift . in our work , we argue that , in realistic scenarios such as sentiment_analysis , there is significant change in label distribution across domains as well . subsequently , we propose a soft labeling formulation that provides better generalization and more robust classifiers across unseen sentiment domains . experimental results on the amazon-reviews benchmark_dataset show the effectiveness of the proposed formulation .
title : ocr-based image features for biomedical image and article classification : identifying documents relevant to cis-regulatory elements ; abstract : images form a significant and useful source of information in published biomedical_articles , which is still under-utilized in biomedical document_classification and retrieval . much current work on biomedical image_retrieval and classification employs simple , standard image features such as gray scale histograms and edge direction to represent and classify images . we have used such features as well to classify images in our early work [ 5 ] , where we used image-class-tags to represent and classify articles . in the work presented here we focus on a different literature classification_task , motivated by the need to identify articles discussing cis-regulatory elements and modules in the context of understanding complex gene-networks . the curators who try to identify such articles in the vast literature use as a major cue a certain type of image in which the conserved cis-regulatory region on the dna is shown . our experiments show that automatically identifying such images using common image features ( like those mentioned above ) can be highly error_prone . however , using optical_character_recognition ( ocr ) to extract alphabet characters from images , calculating character distribution and using the distribution parameters as image features , allows us to form a novel representation of images , and identify dna-content in images with high precision and recall ( over 0.9 ) . utilizing the occurrence of such dna-rich images within articles , we train a classifier that identifies articles pertaining to cis-regulatory elements with a similarly high precision and recall . the use of ocr-based image features has much potential beyond the current task , to identify other types of biomedical sequence-based images showing dna , rna and proteins . moreover , the ability to automatically identify such images has much potential to be widely applicable in other important biomedical document_classification tasks . copyright © 2012 acm .
title : emotion classification in german plays with transformer-based language models pretrained on historical and contemporary language ; abstract : we present results of a project on emotion classification on historical german plays of enlightenment , storm and stress , and german classicism . we have developed a hierarchical annotation_scheme consisting of 13 sub-emotions like suffering , love and joy that sum up to 6 main and 2 polarity classes ( positive/negative ) . we have conducted textual annotations on 11 german plays and have acquired over 13,000 emotion annotations by two annotators per play . we have evaluated multiple traditional_machine_learning_approaches as well as transformer-based models pretrained on historical and contemporary language for a single-label text sequence emotion classification for the different emotion categories . the evaluation is carried out on three different instances of the corpus : ( 1 ) taking all annotations , ( 2 ) filtering overlapping annotations by annotators , ( 3 ) applying a heuristic for speech-based analysis . best results are achieved on the filtered corpus with the best models being large transformer-based models pretrained on contemporary german_language . for the polarity_classification accuracies of up to 90 % are achieved . the accuracies become lower for settings with a higher number of classes , achieving 66 % for 13 sub-emotions . further pretraining of a historical model with a corpus of dramatic texts led to no improvements .
title : augmentation-agnostic regularization for unsupervised contrastive_learning with its application to speaker verification ; abstract : this paper presents a regularization method for unsupervised contrastive_learning and its application to speaker verification . the proposed method , called augmentation-agnostic regularization , enhances the training of speaker embeddings in an adversarial manner . our main idea is to use an augmentation seed classifier , which learns to classify the randomization seeds used in data_augmentation methods , and to train an embedding network with a regularization term to fool the classifier . this method prevents the characteristics of the augmentation procedure from remaining in the embed-dings , facilitating the extraction of speaker characteristics . in experiments , we demonstrate the effectiveness of the proposed regularization in two challenging data-deficient conditions , namely a small-sample training condition and a short-utterance testing condition , and show performance_improvements over the conventional augmented adversarial_training method . the unsupervised model trained with our method achieved compa-rable performance with the supervised x-vector baseline model .
title : broad_application of artificial_intelligence for document_classification , information_extraction and predictive_analytics in real_estate ; abstract : real_estate represents a major share of economic_activities and wealth in all economies . due to the lack of widely acknowledged standards , however , the structuring , providing and managing of a life cycle-comprehensive building documentation yet remain challenging . based on the empirical_analysis of 8965 digital documents from 14 properties of 8 different owners , the article presents a model that will unify existing_approaches and lead to the development of a document_classification standard . this provides the basis for software systems to process relevant data and create timely information over the entire life cycle of a building . further , it is shown that automated information_extraction through artificial_intelligence will become instrumental for enhanced and innovative business_models and products in real_estate such as automated data validation and data evaluation , documentation review , benchmarking and other analytical applications .
title : sentiment_analysis for bangla sentences using convolutional_neural_network ; abstract : sentiment_analysis , also known as opinion_mining or emotion analysis , is a process to determine emotional reaction of people towards an interaction or an event . an opinion may be positive , negative or neutral depends on individuals judgment or evaluation towards a topic or an event . usually sentiments can be varied in cultures and languages . on sentiment_analysis , an extensive amount of research works can be seen for well-resourced_languages like english , japanese etc . however , such works are relatively less observed for low-resourced language like bangla . in this paper , we propose a framework that analyzes sentiments from texts written in bangla . in our proposal , we use bangla comments and generate a classification model . the model is generated by a neural_network variance called convolutional_neural_network . the classifier model obtains a classification_accuracy of 99.87 % , which is 6.87 % better than the available state-of-the art bangla sentiment classifier .
title : a data-driven approach for video_game playability analysis based on players ’ reviews ; abstract : playability is a key concept in game_studies defining the overall quality of video_games . although its definition and frameworks are widely_studied , methods to analyze and evaluate the playability of video_games are still limited . using heuristics for playability evaluation has long been the mainstream with its usefulness in detecting playability issues during game_development well acknowledged . however , such a method falls short in evaluating the overall playability of video_games as published software products and understanding the genuine needs of players . thus , this paper proposes an approach to analyze the playability of video_games by mining a large number of players ’ opinions from their reviews . guided by the game-as-system definition of playability , the approach is a data_mining pipeline where sentiment_analysis , binary classification , multi-label_text_classification , and topic_modeling are sequentially performed . we also conducted a case_study on a particular video_game product with its 99,993 player reviews on the steam platform . the results show that such a review-data-driven method can effectively evaluate the perceived quality of video_games and enumerate their merits and defects in terms of playability .
title : identifying student difficulty in a digital learning environment ; abstract : this paper discusses the development of tutoralert , a natural_language_processing system similar to those used in sentiment_analysis , but applied to the data generated by students in a digital online learning environment in order to detect confused or frustrated students . a number of machine_learning algorithms were tested in the development process , including support_vector_machines ( svm ) , naive_bayes , and random_forest classifiers . as well , an array of natural_language preparation techniques were employed to determine the optimum preprocessing configuration to produce relevant results . we found that detecting potential student frustration or confusion was most successful using a sequential_minimal_optimization algorithm ( smo ) , along with the stanford part-of-speech tagger ( pos_tagger ) , the iterated version of the lovins stemmer , and a custom dictionary to help determine relevance probability . this model produced a promising initial f1 score of 0.79 and an accuracy of 0.83 . further , agreement values of 88 % were achieved during inter-rater_reliability testing between the classifier and human judges .
title : a scalable meta-classifier combining search and classification techniques for multi-level text_categorization ; abstract : nowadays , documents are increasingly associated with multi-level category hierarchies rather than a flat category scheme . as the volume and diversity of documents grow , so do the size and complexity of the corresponding category hierarchies . to be able to access such hierarchically classified documents in real-time , we need fast automatic methods to navigate these hierarchies . today 's data domains are also very different from each other , such as medicine and politics . these distinct domains can be handled by different classifiers . a document_representation system which incorporates the inherent category structure of the data should also add useful semantic content to the data vectors and thus lead to better separability of classes . in this paper , we present a scalable meta-classifier to tackle today 's problem of multi-level data classification in the presence of large_datasets . to speed up the classification process , we use a search-based method to detect the level-1 category of a test document . for this purpose , we use a category-hierarchy-based vector representation . we evaluate the meta-classifier by scaling to both longer_documents as well as to a larger category set and show it to be robust in both cases . we test the architecture of our meta-classifier using six different base_classifiers ( random_forest , c4.5 , multilayer_perceptron , naïve_bayes , bayesnet ( bn ) and part ) . we observe that even though there is a very small variation in the performance of different architectures , all of them perform much better than the corresponding single baseline_classifiers . we conclude that there is substantial potential in this meta-classifier architecture , rather than the classifiers themselves , which successfully improves classification performance .
title : constructing a natural_language inference dataset using generative neural_networks ; abstract : natural_language inference is an important task for natural_language_understanding . it is concerned with classifying the logical relation between two sentences . in this paper , we propose several text generative neural_networks for generating text hypothesis , which allows construction of new natural_language inference datasets . to evaluate the models , we propose a new metric -- the accuracy of the classifier trained on the generated dataset . the accuracy obtained by our best generative_model is only 2.7 % lower than the accuracy of the classifier trained on the original , human crafted dataset . furthermore , the best generated dataset combined with the original dataset achieves the highest_accuracy . the best model learns a mapping embedding for each training example . by comparing various metrics we show that datasets that obtain higher rouge or meteor scores do not necessarily yield higher classification_accuracies . we also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one .
title : a method of text_classification based on statistical technology and set_theory ; abstract : points out the limitations of general text feature_extraction method based on tfidf in problems of text_classification , and presents the standpoint that combines the term_distribution characteristic , term_frequency and document_frequency to extract the text feature , thus giving a new method to compute term 's weight , and a new way of text_classification . experiment showed that the method can keep the text 's feature to a maximum , and avoid the problem of dimensional disaster in vsm effectively , so it can be applied in problems of large_scale text_classification .
title : cross_language text_classification via subspace co-regularized multi-view learning ; abstract : in many multilingual text_classification problems , the documents in different languages often share the same set of categories . to reduce the labeling_cost of training a classification model for each individual language , it is important to transfer the label knowledge gained from one language to another language by conducting cross_language classification . in this paper we develop a novel subspace co-regularized multi-view learning method for cross_language text_classification . this method is built on parallel_corpora produced by machine_translation . it jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents . our empirical_study on a large set of cross_language text_classification tasks shows the proposed method consistently_outperforms a number of inductive methods , domain_adaptation methods , and multi-view learning methods .
title : multimodal feature_extraction for memes sentiment_classification ; abstract : in this study , we propose feature_extraction for multimodal meme classification using deep_learning approaches . a meme is usually a photo or video with text shared by the young generation on social_media platforms that expresses a culturally relevant idea . since they are an efficient way to express emotions and feelings , a good classifier that can classify the sentiment behind the meme is important . to make the learning process more efficient , reduce the likelihood of overfitting , and improve the generalizability of the model , one needs a good approach for joint feature_extraction from all modalities . in this work , we proposed to use different multimodal neural_network approaches for multimodal feature_extraction and use the extracted features to train a classifier to identify the sentiment in a meme .
title : readability indices for the assessment of textbooks : a feasibility study in the context of efl ; abstract : readability indices have been widely used in order to measure textual difficulty . they can be useful for the automatic classification of texts , especially in language teaching . among other applications , they allow for the previous determination of the difficulty_level of texts without the need of reading them through . the aim of this research is twofold : first , to examine the degree of accuracy of the six most commonly used readability indices , and second , to present a new optimized measure . the main problem is that these readability indices may offer disparity , and this is precisely what has motivated our attempt to unite their potential . a discriminant analysis of all the variables under examination has enabled the creation of a much more precise model , improving the previous best results by 15 % . furthermore , errors and disparities in the difficulty_level of the analyzed texts have been detected .
title : on-line text_categorization algorithm based on information fusion : semantic svm ; abstract : the aim of this paper is to make svms ( support_vector_machines ) more applicable to on-line text_categorization applications . as svms are of good generation ability even with small training_sets and text feature_vectors are clustery in the feature_space , an algorithm for text_categorization , namely , semantic support_vector_machine ( semantic svm ) , is proposed by substituting the original training text set with the semantic center set . this semantic center set is used as the training text and support_vector candidates . the steps to generate the semantic center set and the framework of the on-line learning algorithm of semantic svm are then presented , as well as the implementation of the on-line learning algorithm based on sequential_minimal_optimization . experimental results show that , compared with the standard svms , the proposed semantic svm and its algorithm can improve the on-line learning speed and the classifying speed by orders with a high classifying veracity .
title : lightrel semeval-2018 task 7 : lightweight and fast relation_classification ; abstract : we present lightrel , a lightweight and fast relation classifier . our goal is to develop a high baseline for different relation_extraction tasks . by defining only very few data-internal , word-level features and external_knowledge sources in the form of word_clusters and word_embeddings , we train a fast and simple_linear classifier .
title : annotation of cultural heritage documents based on xml dictionaries and data clustering ; abstract : cultural heritage forms the local and national identities . it shapes relationships between neighbors and other communities around the world . the sweet wine named `` commandaria '' is part of cypriot heritage and currently holds a protected destination of origin within european_union , usa and canada . in the framework of the commandaria project we managed to gather an enormous amount of data , related to commandaria wine , corresponding to photographs , scanned_documents and videos . the need of a method for efficient retrieval of these data based on their actual content was mandatory . the data were appropriately indexed through a multilevel labeling scheme allowing access from various modalities and for a variety of applications . despite the huge efforts for automatic characterization and classification human intervention is the only way for reliable multimedia data annotation . manual data annotation is an extremely laborious process and efficient tools developed for this purpose can make , in many cases , the true difference . in this paper we present the culhiat , a cultural heritage item annotation tool , which uses structured_knowledge , in the form of xml dictionaries , combined with a hierarchical classification_scheme , to attach semantic labels to image and video segments at various levels of granularity . finally , xml dictionary creation and editing tools are available during annotation allowing the user to always use the semantic label she/he wishes instead of the automatically created ones . ©_2010_springer-verlag_berlin_heidelberg .
title : machine_learning in building a collection of computer science course syllabi ; abstract : syllabi are rich educational_resources . however , finding computer science syllabi on a generic search_engine does not work well . towards our goal of building a syllabus collection we have trained various decision_tree , naive-bayes , support_vector_machine and feed-forward_neural_network classifiers to recognize computer science syllabi from other web_pages . we have also trained our classifiers to distinguish between artificial_intelligence and software_engineering syllabi . our best classifiers are 95 % accurate at both the tasks . we present an analysis of the various feature_selection methods and classifiers we used hoping to help others developing their own collections . ©_2012_springer-verlag .
title : small-footprint keyword_spotting using deep_neural_network and connectionist temporal classifier ; abstract : mainly for the sake of solving the lack of keyword-specific data , we propose one keyword_spotting ( kws ) system using deep_neural_network ( dnn ) and connectionist temporal classifier ( ctc ) on power-constrained small-footprint mobile devices , taking full advantage of general corpus from continuous_speech_recognition which is of great amount . dnn is to directly predict the posterior of phoneme units of any personally customized key-phrase , and ctc to produce a confidence_score of the given phoneme sequence as responsive decision-making mechanism . the ctc-kws has competitive_performance in comparison with purely dnn based keyword specific kws , but not increasing any computational_complexity .
title : silhouettes and quasi residual plots for neural_nets and tree-based classifiers ; abstract : classification by neural_nets and by tree-based methods are powerful tools of machine_learning . there exist interesting visualizations of the inner workings of these and other classifiers . here we pursue a different goal , which is to visualize the cases being classified , either in training_data or in test data . an important aspect is whether a case has been classified to its given class ( label ) or whether the classifier wants to assign it to a different class . this is reflected in the ( conditional and posterior ) probability of the alternative class ( pac ) . a high pac indicates label bias , that is , the possibility that the case was mislabeled . the pac is used to construct a silhouette plot which is similar in spirit to the silhouette plot for cluster_analysis . the average silhouette width can be used to compare different classifications of the same dataset . we will also draw quasi residual plots of the pac versus a data feature , which may lead to more insight in the data . one of these data features is how far each case lies from its given class . the graphical displays are illustrated and interpreted on datasets containing images , mixed features , and tweets . supplementary materials for this article are available online .
title : filtering unwanted messages from osn user wall ; abstract : one key_issue in today on-line social_networks ( osns ) is to give clients the capacity to control the messages_posted all alone private space to stay away from that undesirable substance is shown . up to now osns give little backing to this necessity . to defeat this issue , we propose a framework permitting osn clients to have an immediate control on the messages_posted on their dividers . this is accomplished through an adaptable guideline based framework , that permits clients to redo the separating criteria to be matter-of-truth to their dividers , and a machine_learning based delicate classifier consequently naming messages in substance based sifting .
title : evaluating the quality of word_representation models for unstructured clinical_text based icu mortality prediction ; abstract : in modern hospitals , the role of clinical_decision_support systems ( cdss ) in assisting care providers is well-established . most conventional cdss systems are built on the availability of patient data in the form of structured electronic_health_records . however , a significant percentage of patient data is still stored in the form of unstructured clinical_text notes , especially in developing_countries . these contain abundant patient-specific information , which has so far remained largely under-utilized in powering cdss applications . in this paper , we attempt to build one such cdss system for patient mortality prediction , using unstructured clinical_records . effectiveness of such prediction models largely depends on optimally capturing latent concept features , thus , word_representation quality is of utmost_importance . we experiment with three popular word_embedding models - word2vec , fasttext and glove for generating word_embeddings of unstructured nursing notes of patients from a standard , open dataset , mimic-iii . these word_representations are used as features to train machine_learning classifiers to build icu mortality prediction models , a critical cdss in icus of hospitals . experimental validation showed that a model built on word2vec skipgram based random_forest classifier was the most optimal word_embedding based mortality prediction model , that outperformed traditional severity scores like saps-ii , sofa , aps-iii and oasis , by a significant margin of 43-52 % .
title : neural_network model for video-based analysis of student ’ s emotions in e-learning ; abstract : abstract : in this paper , we consider a problem of an automatic analysis of the emotional_state of students during online classes based on video surveillance data . this problem is actual in the field of e-learning . we propose a novel neural_network model for recognition of students ’ emotions based on video images of their faces and use it to construct an algorithm for classifying the individual and group emotions of students by video clips . at the first step , it performs detection of the faces and extracts their features followed by grouping the face of each student . to increase the accuracy , we propose to match students ’ names selected with the aid of the algorithms of the text recognition . at the second step , specially learned efficient neural_networks perform the extraction of emotional features of each selected person , their aggregation with the aid of statistical functions , and the subsequent classification . at the final_step , it is possible to visualize fragments of the video lesson with the most pronounced emotions of the student . our experiments with some datasets from emotiw ( emotion recognition in the wild ) show that the accuracy of the developed algorithms is comparable with their known analogous . however , when classifying emotions , the computational performance of these algorithms is higher .
title : a comparative_analysis of text_classification algorithms for ambiguity detection in requirement_engineering document using weka ; abstract : the volume of digital documents is increasing day by day and thus the task of automatic categorization of document is very important for information and knowledge discovery . classification is the most common method for finding the mine rule from the large databases . ambiguity is the major problem in requirement_engineering ( re ) documents . our proposed work uses weka text_classification technique to identify and classify ambiguity in the re document . the present study uses different algorithms on the ambiguity detection dataset and on the basis of different statistical measures like accuracy , time , and error_rate we find suitable algorithms for this purpose . the main aim of this paper is to do a comparative study of various classification techniques and methodologies and a detailed analysis of different statistical parameters that are used in classification_algorithms in order to analyze the quality of classification .
title : crats : an lda-based model for jointly mining latent communities , regions , activities , topics , and sentiments from geosocial network data ; abstract : geosocial networks like yelp and foursquare have been rapidly_growing and accumulating plenty of data such as social links between users , user check-ins to venues , venue geographical locations , venue categories , and user textual comments on venues . these data contain rich knowledge on the user 's social_interactions in communities , geographical mobility patterns between regions , categorical preferences on activities , aspect interests in topics , and opinion expressions for sentiments . such knowledge is essential for two key applications , namely , text sentiment_classification and venue recommendations , which will be developed in this paper . to extract the knowledge from the data , the key task is to discover the latent communities , regions , activities , topics , and sentiments of users . however , these latent_variables are interdependent , e.g. , users in the same community usually travel on nearby regions and share common activities and topics , which renders a big challenge for modeling these latent_variables . to tackle this challenge , in this study , we propose an lda-based model called crats that jointly mines the latent communities , regions , activities , topics , and sentiments based on the important dependencies among these latent_variables . to the best of our knowledge , this is the first study to jointly model these five latent_variables . finally , we conduct a comprehensive performance evaluation for crats in different applications , including text sentiment_classification and venue recommendations , using three large-scale real-world geosocial network data sets collected from yelp and foursquare . experimental results show that crats achieves significantly superior performance against other state-of-the-art techniques .
title : some effects of acoustic attributes of speech on the processing of phonetic feature information ; abstract : five experiments with 7 undergraduates investigated how acoustic attributes of the speech_signal contribute to feature-processing interactions that occur in phonetic classification . ss performed speeded classification_tasks that explicitly required a phonetic decision for each response . stimuli were natural cv syllables differing by multiple phonetic_features , although classification responses were based on a single target feature . comparisons of classification times in experimental and control tasks demonstrate that feature information may either be processed separately as independent cues for each feature or as a single integral segment that jointly specifies several features . the observed form on processing depended on the acoustic manifestations of feature variation in the signal . stop-consonant place_of_articulation and voicing cues , conveyed independently by the pattern and excitation source of the initial formant transitions , may be processed separately . however , information for consonant place_of_articulation and vowel quality , features that interactively affect the shape of initial formant transitions , are processed as an integral segment . ( 50 ref ) ( psycinfo database record ( c ) 2006 apa , all rights_reserved ) . © 1980 american_psychological_association .
title : development of text_classification based on difficulty_level in adaptive learning system using convolutional_neural_network ; abstract : text_classification is advantageous and necessary in a variety of fields . the adaptive learning system is one of the sectors that requires text_classification . adaptive learning is a type of online learning that makes learning recommendations based on the needs of the students . the learning module and the testing module are the two aspects of adaptive learning . the purpose of the learning module is to give recommendations of learning_materials . the testing module , on the other hand , is used to assess and test the student 's progress . the learning module requires text_classification based on difficulty_level to ensure that students have access to learning resources that are appropriate for their ability level . the dataset used in this study is the ebook of science for junior high_school , and we also crawling from ruangguru ( one of the e-learning platforms in indonesia ) . the method that used in this study is cnn that combines with word_embedding techniques . the dimension of glove and word2vec are 100 dimensions . cnn-word2vec performs better than cnn-glove with 96 % accuracy and 61.20 seconds of computing time . based on this , cnn-word2vec is appropriately applied to adaptive learning that requires accuracy and fast computing time .
title : improving machine_reading_comprehension via adversarial_training ; abstract : adversarial_training ( at ) as a regularization method has proved its effectiveness in various tasks , such as image_classification and text_classification . though there are successful applications of at in many tasks of natural_language_processing ( nlp ) , the mechanism behind it is still unclear . in this paper , we aim to apply at on machine_reading_comprehension ( mrc ) and study its effects from multiple perspectives . we experiment with three different kinds of rc tasks : span-based rc , span-based rc with unanswerable questions and multi-choice rc . the experimental results show that the proposed method can improve the performance significantly and universally on squad1.1 , squad2.0 and race . with virtual_adversarial_training ( vat ) , we explore the possibility of improving the rc models with semi-supervised_learning and prove that examples from a different task are also beneficial . we also find that at helps little in defending against artificial adversarial_examples , but at helps the model to learn better on examples that contain more low-frequency words .
title : mining twitter for fine-grained political opinion_polarity classification , ideology detection and sarcasm detection ; abstract : in this paper , we propose three models for socio-political opinion_polarity classification of microblog_posts . firstly , a novel probabilistic model , joint-entity-sentiment-topic ( jest ) model , which captures opinions as a combination of the target entity , sentiment and topic , will be proposed . secondly , a model for ideology detection called jest-ideology will be proposed to identify an individual 's orientation towards topics/issues and target entities by extending the proposed opinion_polarity classification framework . finally , we propose a novel method to accurately detect sarcastic opinions by utilizing detected fine-grained opinion and ideology .
title : analysis of covid-19 5g conspiracy_theory tweets using sentencebert embedding ; abstract : twitter is a popular major social_media platform with a central role in the distribution of information , and as such a fertile land for the growth of conspiracy_theories in different subjects , with covid-19 conspiracies among them . in this research , we collected a dataset of 331,448 tweets related to the covid-19 5g conspiracy_theory . we present a workflow to collect , classify , and analyze conspiracy related_tweets as supporting or opposing the conspiracy_theory . we hand_labeled 4,291 tweets and trained a classifier using a novel approach containing two sets of features : a set of sentence_embeddings produced by covid-twitter-bert and sentence-bert , and a set of external features . we used five different classifiers and ensemble_learning to combine them . we classified the dataset and analyzed the classified dataset to conclude that opponents of the conspiracy dominate the conversation on twitter .
title : computational analysis of thematic blog data for sociological inference mining ; abstract : this paper describes our proposed approach for computational analysis of thematic blog data through a novel combine of sophisticated information_retrieval and language processing techniques . we have implemented algorithms for topic_modeling , entity_extraction and sentiment_classification with a view to draw sociologically relevant inferences from freeform unstructured social_media data . our experimental data comprised of more than 600 blog posts on the broader theme of 'discrimination , abuse and sexual crime against women ' collected during two discrete time periods . we have tried to extract some important inferences from the data such as key persons and organizations mentioned in the data , key themes encountered in the entire data collection , sentiment_orientation inherent in the texts and variation in topic trends during the two discrete time periods . the results obtained are very interesting and validate the usefulness of our approach for computational analysis of social_media data . © 2013 ieee .
title : improving sentiment_classification accuracy of financial_news using n-gram approach and feature_weighting methods ; abstract : sentiment_classification of financial_news deals with the identification of positive and negative news so that they can be applied in decision_support system to perform stock trend predictions . this paper explores several types of feature_space as different datasets for sentiment_classification of the news article . experiments are conducted based on n-gram approach ( unigram , bigram and the combination of unigram_and_bigram ) used as feature_extraction with different feature_weighting methods , while , document_frequency ( df ) is used as feature_selection method . we performed experiments to measure the classification_accuracy of support_vector_machine ( svm ) with two kernel methods of linear and radial_basis_function ( rbf ) . results showed that an efficient feature_extraction increased classification_accuracy when it is used as a combination of unigram_and_bigram . moreover , we also found that df can be applied as a dimension reduction method to reduce the feature_space without loss of accuracy .
title : memory-based semantic_role_labeling of catalan and spanish ; abstract : in this paper we present a memory-based semantic_role_labeling ( srl ) system for catalan and spanish . we approach the srl task as two distinct classification problems : the assignment of semantic roles to arguments of verbs , and the assignment of semantic classes to verbs . we hypothesize that the two tasks can be solved in a uniform way , for both languages . building on the same pool of features reported useful in earlier work , we train two classifiers for the two subtasks , selecting features systematically in a hillclimbing search . we use the ib1 classifier , a supervised memory-based learning algorithm based on the k-nn classifier . the system achieves overall f-scores of 85.69 for catalan , and 84.12 for spanish .
title : deep_learning based text_classification with web_scraping methods ; abstract : it is a huge repository of information used to meet the many needs of internet users . the collection and processing of data in some internet sites in this information repository has become very important nowadays . in this study , categorical news headlines and summaries in a turkish news_agency site were collected by using web_scraping methods and test data were classified by using 'one hot encoding ' method with vector learning methods and depth learning methods . a classification_accuracy of 90 % has been achieved .
title : research on the application of natural_language_processing in the virtual comment ’ s classification ; abstract : natural_language_processing ( npl ) is a booming technical field in artificial_intelligence , and it is closely_related to our lives . npl studies how to enable computers to understand and use human language to achieve information interaction between humans and computers , and then extract effective text information from a large amount of text . this includes not only allowing computers to understand natural_language texts , but also extending the ability to enable computers to express deep intentions based on natural_language texts . this research briefly outlines some processes of natural_language_processing in practical_applications . network virtual comment classification problem is considered with a two-layer lstm model to train the original data set , and the results show the effectiveness of the proposed design .
title : am i a resource-poor language ? data sets , embeddings , models and analysis for four different nlp tasks in telugu_language ; abstract : due to the lack of a large annotated_corpus , many resource-poor indian languages struggle to reap the benefits of recent deep feature_representations in natural_language_processing ( nlp ) . moreover , adopting existing language models trained on large english corpora for indian languages is often limited by data availability , rich morphological variation , syntax , and semantic differences . in this paper , we explore the traditional to recent efficient representations to overcome the challenges of a low_resource_language , telugu . in particular , our main objective is to mitigate the low-resource problem for telugu . overall , we present several contributions to a resource-poor language viz . telugu . ( i ) a large annotated_data ( 35,142 sentences in each task ) for multiple nlp tasks such as sentiment_analysis , emotion identification , hate-speech detection , and sarcasm detection , ( ii ) we create different lexicons for sentiment , emotion , and hate-speech for improving the efficiency of the models , ( iii ) pretrained word and sentence_embeddings , and ( iv ) different pretrained_language_models for telugu such as elmo-te , bert-te , roberta-te , albert-te , and distilbert-te on a large telugu corpus consisting of 8,015,588 sentences ( 1,637,408 sentences from telugu wikipedia and 6,378,180 sentences crawled from different telugu websites ) . further , we show that these representations significantly_improve the performance of four nlp tasks and present the benchmark results for telugu . we argue that our pretrained embeddings are competitive or better than the existing multilingual pretrained_models : mbert , xlm-r , and indicbert . lastly , the fine-tuning of pretrained_models show higher performance than linear probing results on four nlp tasks with the following f1-scores : sentiment ( 68.72 ) , emotion ( 58.04 ) , hate-speech ( 64.27 ) , and sarcasm ( 77.93 ) . we also experiment on publicly available telugu datasets ( named_entity_recognition , article genre classification , and sentiment_analysis ) and find that our telugu pretrained_language_models ( bert-te and roberta-te ) outperform the state-of-the-art system except for the sentiment task . we open-source our corpus , four different datasets , lexicons , embeddings , and code https : //github.com/cha14ran/dream-t . the pretrained_transformer models for telugu are available at https : //huggingface.co/ltrctelugu .
title : classification of the wind_turbine components based on importance degrees : a three-way decision perspective ; abstract : the bad working environment and various complex influencing factors lead to the high failure_rate of a wind_turbine . it is necessary to decide on a reasonable maintenance strategy for wind_turbines . the classification of wind_turbine components based on their importance degrees can identify the components which have an important impact on the reliability and maintenance of wind_turbines . however , the traditional multi-criteria decision-making method can only provide the importance ranking of components , rather than the importance classification of components . the emergence of the three-way decision ( twd ) method makes up for this deficiency . then we classify the importance degrees of components by the twd method . firstly , the decision-theoretic rough_sets are introduced into the uncertain linguistic setting to construct uncertain linguistic decision-theoretic rough_sets . secondly , the weights of experts are attained based on the consistency degree of loss_functions . thirdly , conditional_probability is attained by the evaluation based on distance from the average solution method . and the classification of wind_turbine components is derived based on the minimum-loss principle . finally , a case_study about the classification of the wind_turbine components based on importance degrees is employed to certify the practicability of our designed method . the proposed model extends both the theory and practice of twd and offers a classification helpful to make maintenance strategies for reducing the risk of component failure .
title : extracting medication information from unstructured public_health data : a demonstration on data from population-based and tertiary-based samples ; abstract : background : unstructured_data from clinical epidemiological studies can be valuable and easy to obtain . however , it requires further extraction and processing for data_analysis . doing this manually is labor-intensive , slow and subject to error . in this study , we propose an automation framework for extracting and processing unstructured_data . methods : the proposed automation framework consisted of two natural_language_processing ( nlp ) based tools for unstructured_text data for medications and reasons for medication use . we first checked spelling using a spell-check program trained on publicly available knowledge sources and then applied nlp techniques . we mapped medication names into generic names using vocabulary from publicly available knowledge sources . we used who ’ s anatomical therapeutic chemical ( atc ) classification system to map generic medication names to medication classes . we processed the reasons for medication with the lancaster stemmer method and then grouped and mapped to disease classes based on organ systems . finally , we demonstrated this automation framework on two data sources for mylagic encephalomyelitis/ chronic_fatigue_syndrome ( me/cfs ) : tertiary-based ( n = 378 ) and population-based ( n = 664 ) samples . results : a total of 8681 raw medication records were used for this demonstration . the 1266 distinct medication names ( omitting supplements ) were condensed to 89 atc classification system categories . the 1432 distinct raw reasons for medication use were condensed to 65 categories via nlp . compared to completion of the entire process manually , our automation process reduced the number of the terms requiring manual labor for mapping by 84.4 % for medications and 59.4 % for reasons for medication use . additionally , this process improved the precision of the mapped results . conclusions : our automation framework demonstrates the usefulness of nlp strategies even when there is no established mapping database . for a less established database ( e.g. , reasons for medication use ) , the method is easily modifiable as new knowledge sources for mapping are introduced . the capability to condense large features into interpretable ones will be valuable for subsequent analytical studies involving techniques such as machine_learning and data_mining .
title : lexical tf-idf : an n-gram feature_space for cross-domain classification of sentiment reviews ; abstract : feature_extraction and selection is a vital step in sentiment_classification using machine_learning approach . existing_methods use only tf-idf rating to represent either unigram or n-gram feature_vectors . some approaches leverage upon the use of existing sentiment_dictionaries and use the score of a unigram sentiment word as the feature_vector and ignore tf-idf rating . in this work , we construct n-gram sentiment features by extracting the sentiment words and their intensifiers or negations from a review . then the score of an n-gram constructed from lexicon of semantic unigram and its intensifier or negation is multiplied to tf-idf rating to determine the feature score . we experiment with two benchmark_data_sets for sentiment_classification using support_vector_machine and maximum_entropy method with cross_domain validation by considering training and testing data from two different sets and obtain a substantial_improvement in terms of various performance_measures compared to existing_methods . cross-domain validation ensures proposed method can be applied for sentiment_classification of data sets where example patterns are not available , which typically is the case with commercial data sets .
title : learning the semantic meaning of a concept from the web ; abstract : many researchers have used text_classification method in solving the ontology mapping problem . their mapping results heavily depend on the availability of quality exemplars used as training_data . however , manual preparation of exemplars is costly . in this work , we propose to automatically_extract text from web_pages returned by a search_engine . search_queries are formed according to the semantic information given in the ontology . we have implemented a prototype system that automates the entire process ( from search_query formation to conditional_probability calculation ) and conducted a series of experiments . we assessed the effectiveness of our approach by comparing the obtained conditional_probabilities with human expectations . our main_contribution is that we explored the possibilities of utilizing web information for text_classification based ontology mapping and made several valuable discoveries on its usefulness for future_research . ©_springer-verlag_berlin_heidelberg 2007 .
title : classification of contract-amendment relationships ; abstract : in contract life-cycle management ( clm ) , managing and tracking the master agreements and their associated amendments is essential , in order to be kept informed with different due dates and obligations . an automatic solution can facilitate the daily jobs and improve the efficiency of legal practitioners . this paper proposes an approach based on machine_learning ( ml ) and natural_language_processing ( nlp ) to detect the amendment relationship between two documents . the algorithm takes two pdf documents preprocessed by ocr ( optical_character_recognition ) and ner ( named_entity_recognition ) as input , and then it builds the features of each document pair and classifies the relationship . different configurations are experimented on a dataset consisting of 1124 pairs of contract-amendment documents in english and french . the best result obtained a f1-score of 91 % , which outperformed 23 % compared to a heuristic-based baseline .
title : an automated approach for software_bug classification ; abstract : open_source projects for example eclipse and firefox have open_source bug repositories . user reports bugs to these repositories . users of these repositories are usually non-technical and can not assign correct class to these bugs . triaging of bugs , to developer , to fix them is a tedious and time consuming task . developers are usually expert in particular areas . for example , few developers are expert in gui and others are in java functionality . assigning a particular bug to relevant developer could save time and would help to maintain the interest level of developers by assigning bugs according to their interest . however , assigning right bug to right developer is quite difficult for tri-ager without knowing the actual class , the bug belongs to . in this research , we have classified the bugs in different labels on the basis of summary of the bug . multinomial_naïve_bayes text classifier is used for classification purpose . for feature_selection , chi-square and tfidf algorithms were used . using naïve_bayes and chi-square , we get average of 83 % accuracy . © 2012 crown_copyright .
title : dl-per : deep_learning model for chinese prehospital emergency record classification ; abstract : prehospital emergency records contain a large amount of information about prehospital emergency patients . extracting important patient information from many records has become the focus of all prehospital emergency personnel . the key to solving this problem is to achieve the automatic classification of prehospital emergency records . in this study , we consider a deep_learning-based pre-hospital emergency record classification model ( dl-per ) . the model uses a weighted text convolutional_neural_network to classify pre-hospital emergency records . first , we used prehospital emergency records to train a bidirectional_encoder_representation ( bert ) model from a transformer and let bert acquire contextual semantic information . then , we used a bidirectional long and short-term_memory ( bilstm ) model to obtain text features from a global perspective and improve the local text feature_extraction capability of the model by a weighted text convolutional_neural_network ( wtextcnn ) . we used activation_functions instead of relu activation_functions to improve the learning ability of the model . we conducted experiments using prehospital emergency records provided by the handan emergency center . the results showed that the dl-per model improved the f1 scores by up to 5.7 % , 6.8 % , 5.7 % , and 4.9 % on the four data sets , respectively , compared with the bilstm model .
title : intrusion_detection in web_applications using text_mining ; abstract : information_security has evolved from just focusing on the network and server layers to also include the web_application layer . in fact , security in some types of web_applications is often considered a particularly sensitive subject . achieving a secure web_application involves several different issues like encrypting traffic and certain database information , strictly restricting the access_control , etc . in this work we focus on detecting attempts of either gaining unauthorised access or misusing a web_application . we introduce an intrusion_detection software component based on text-mining techniques . by using text_categorisation , it is capable of learning the characteristics of both normal and malicious user behaviour from the log entries generated by the web_application server . therefore , the detection of misuse in the web_application is achieved without the need of any explicit programming or code writing , hence improving the system maintainability . because telemedicine systems are usually critical in terms of the confidential_information handled and the responsibilities consequently derived , we apply and evaluate our methods on a real web-based telemedicine system called arnasa . © 2006 elsevier ltd. all rights_reserved .
title : automatic broadcast news summarization via rank classifiers and crowdsourced annotation ; abstract : extractive speech_summarization methods generally operate as a binary classifier deciding if a sentence belongs to the summary or not . however , it is well known that even human annotators do not agree on selecting most summary sentences . in this paper , we take a probabilistic view of the summarization ground-truth and assume that more frequently selected sentences by annotators are of higher importance . using a large summary data-set obtained through crowdsourcing , we empirically show that sentence selection frequency is inversely related to its summarization rank . consequently , we model the relative importance between sentences using a rank-based classifier . additionally , we utilize an extended paralinguistic feature_set that has not been previously used for speech_summarization . lexical and structural_features are also included . support_vector_machine ( svm ) is used as the baseline binary classifier and rank classifier . experimental evaluations show that the proposed approach_outperforms traditional binary classifiers with respect to various rouge summarization metrics for different summarization compression ratios ( cr ) .
title : conclusions ; abstract : entropy guided transformation learning is a machine_learning algorithm for classification_tasks . in this book , we detail how etl generalizes transformation based learning by solving the tbl bottleneck : the construction of good template sets . etl relies on the use of the information gain measure to select feature_combinations that provide effective template sets . in this work , we also present etl committee , an ensemble_method that uses etl as the base_learner . we describe the application of etl to four language independent nlp tasks : part-of-speech_tagging , phrase chunking , named_entity_recognition and semantic_role_labeling . overall , we successfully apply it to thirteen different corpora in six different languages : dutch , english , german , hindi , portuguese and spanish . our extensive experimental results demonstrate that etl is an effective way to learn accurate transformation rules . in all experiments , etl shows better results than tbl with hand-crafted templates . our experimental results also demonstrate that etl committee is an effective way to improve the etl effectiveness . we believe that by avoiding the use of handcrafted templates , etl enables the use of transformation rules to a greater range of classification_tasks .
title : contextual text/non-text stroke classification in online_handwritten notes with conditional_random_fields ; abstract : analysing online_handwritten notes is a challenging problem because of the content heterogeneity and the lack of prior_knowledge , as users are free to compose documents that mix text , drawings , tables or diagrams . the task of separating text from non-text strokes is of crucial importance towards automated interpretation and indexing of these documents , but solving this problem requires a careful modelling of contextual_information , such as the spatial and temporal relationships between strokes . in this work , we present a comprehensive study of contextual_information modelling for text/non-text stroke classification in online_handwritten documents . formulating the problem with a conditional_random_field permits to integrate and combine multiple_sources of context , such as several types of spatial and temporal interactions . experimental results on a publicly available database of freely hand-drawn documents demonstrate the superiority of our approach and the benefit of contextual_information combination for solving text/non-text_classification . © 2013 elsevier ltd. all rights_reserved .
title : classifying case relations using syntactic , semantic and contextual features ; abstract : this paper presents a classification of semantic roles using syntactic , semantic and contextual features . the aim of our work is to identify types of semantic roles involving events and their actors ; therefore , we fulfill a feature analysis in order to select the best feature_subset which improves the fulfillment of the task . in addition , we compare four classification_algorithms : support_vector_machine ( svm ) , k-nearest_neighbor ( k-nn ) , bayes_classifier and decision_tree classifier c4.5 . this comparison was made in order to analyze the performance of these algorithms with all features against relevant features for each semantic role category . in our experimentation , we obtain that feature_selection improved the performance of algorithms in our classification_task , since with relevant features we obtained the best performance of 84.6 % with decision_tree classifier c4.5 . the results for the labeling task can be used for knowledge_representation or ontology learning .
title : impact of text_mining application on financial footnotes analysis research in progress ; abstract : in recent decade and with the advent of the extensible business reporting language ( xbrl ) , financial_reports have a great mutation in terms of a unified reporting process . nevertheless , the unstructured part of financial_reports , so called footnotes , remains as barrier facing an accurate automatic and real-time financial_analysis . the purpose of this paper is to investigate whether the text_mining approach is an appropriate solution to assist analyzing textual financial footnotes or not . the implemented text_mining prototype is able to classify textual financial footnotes into related pre-defined categories automatically . this avoids manually reading of the entire text . different text_classification supervised algorithms have been compared , where the decision_tree by 90.65 % accuracy performs better rather than other deployed classifiers . this research provides preliminary insights about the impact of using a text_mining approach on automatic financial footnote analysis in terms of saving time and increasing accuracy .
title : cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400 ; 000 documents . third , it analyzes the impact of the amount of training_data ( document_images ) and other parameters to the classification abilities . we use two datasets , the tobacco-3482 and the large-scale rvl-cdip dataset . we achieve an accuracy of 91:13 % for the tobacco-3482 dataset while earlier approaches reach only 77:6 % . thus , a relative error_reduction of more than 60 % is achieved . for the large dataset rvl-cdip , an accuracy of 90:97 % is achieved , corresponding to a relative error_reduction of 11:5 % .
title : sentiment_analysis of hotel reviews based on deep leaning ; abstract : with the development of e-commerce , a large number of hotel reviews have been produced . according to the short text features of hotel reviews , sentiment_classification method based on emotion dictionary needs a lot of emotional database resources , while machine_learning method needs complex artificial design features and feature_extraction process . in this paper , long short_term_memory is proposed . the text_classification algorithm is used to analyze the sentiment_tendency . firstly , word2vec and word_segmentation technology are used to process the short comment text into the lstm network , and dropout algorithm is added to prevent over fitting to get the final classification model , using the unique_characteristics of short-term_memory of lstm network , it has achieved a good effect on sentiment_classification of hotel reviews , with an accuracy_rate of more than 95 % .
title : natural_language_processing with “ more than words – bert ” ; abstract : question-answering ( qa ) has become one of the most popular natural_language_processing ( nlp ) and information_retrieval applications . to be applied in qa_systems , this paper presents a question_classification technique based on nlp and bidirectional_encoder_representation_from_transformers ( bert ) . we performed experimental investigation on bert for question_classification with trec-6 dataset and a thai sentence dataset . we propose an improved processing technique called “ more than words – bert ” ( mtw – bert ) that is a special nlp annotation tags for combining part-of-speech_tagging and named_entities recognition to be able for learning both pattern of grammatical tag sequence and recognized entities together as input before classifying text on bert model . experimental results showed that mtw – bert outperformed existing classification methods and achieved new state-of-the-art performance on question_classification for trec-6 dataset with 99.20 % . in addition , mtw-bert also applied for question_classification for thai sentences in wh-question category . the proposed technique remarkably achieved thai wh-classification with accuracy_rate of 87.50 % .
title : an automatic method using hybrid neural_networks and attention_mechanism for software_bug triaging ; abstract : software defect repair ( also known as software_bug fixing ) is a necessary part of software_quality_assurance . in the collective-intelligence-based software_development environment on the internet , improving the efficiency and effectiveness of software_bug triaging can help raise bug_fixing rates and reduce maintenance costs . nowadays , automatic bug_triaging approaches based on machine_learning have become mainstream , but they also have some specific problems , such as hand-crafted features and an insufficient ability to represent texts . considering successful applications of deep_learning in the field of natural_language_processing , researchers have recently tried to introduce deep_learning into the field of automatic bug_triaging , to improve the performance of predicting the right bug fixer significantly . however , different types of neural_networks have their limitations . to address the problems mentioned above , in this study , we regard bug_triaging as a text_classification problem and propose an automatic bug_triaging approach based on hybrid neural_networks and an attention_mechanism , called atten-crnn . because atten-crnn combines the advantages of a convolutional_neural_network , a recurrent_neural_network , and an attention_mechanism , it can capture essential text features and sequence features of bug_reports more effectively and then provide more accurate fixer recommendation services for software_development and maintenance . an empirical_study was conducted on two popular large-scale open-source_software projects , namely eclipse and mozilla . the experimental results obtained from over 200 000 bug_reports indicate that atten-crnn achieves higher prediction_accuracy than convolutional_neural_networks and recurrent_neural_networks , regardless of the attention_mechanism .
title : enriching pre-trained_language_model with entity information for relation_classification ; abstract : relation_classification is an important nlp task to extract relations between entities . the state-of-the-art methods for relation_classification are primarily based on convolutional or recurrent_neural_networks . recently , the pre-trained bert model achieves very successful results in many nlp classification / sequence labeling tasks . relation_classification differs from those tasks in that it relies on information of both the sentence and the two target entities . in this paper , we propose a model that both leverages the pre-trained bert language model and incorporates information from the target entities to tackle the relation_classification task . we locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities . we achieve significant improvement over the state-of-the-art method on the semeval-2010 task 8 relational dataset .
title : fraunhofer sit at checkthat ! 2022 : ensemble similarity estimation for finding previously fact-checked claims ; abstract : during the corona pandemic misinformation has been increasingly spread on social_media . since the automatic verification of social_media postings has shown to be challenging , there exists the need of systems , that can identify whether a claim in a post has already been previously analyzed by independent fact-checkers . in this paper , a system based on ensemble classification is proposed . it takes advantage of state-of-the-art sentence transformers for estimating the semantic similarity between a given tweet and individual parts of a fact-check . furthermore , it incorporates several preprocessing_steps as well as back-translation as a data augmentation technique . the proposed model ranked sixth best in the competition .
title : classification and association_rules in brazilian supreme_court judgments on pre-trial detention ; abstract : brazil has a large prison population , which places it as the third country in the world with the most incarceration rate . in addition , the criminal caseload is increasing in brazilian judiciary , which is encouraging ai usage to advance in e-justice . within this context , the paper presents a case_study with a dataset composed of 2,200 judgments from the supreme_federal_court ( stf ) about pre-trial detention . these are cases in which a provisional prisoner requests for freedom through habeas_corpus . we applied machine_learning ( ml ) and natural_language_processing ( nlp ) techniques to predict whether stf will release or not the provisional prisoner ( text_classification ) , and also to find a reliable association between the judgment outcome and the prisoners ’ crime and/or the judge responsible for the case ( association_rules ) . we obtained satisfactory_results in both tasks . classification results show that , among the models used , convolutional_neural_network ( cnn ) is the best , with 95 % accuracy and 0.91 f1-score . association results indicate that , among the rules generated , there is a high probability of drug law crimes leading to a dismissed habeas_corpus ( which means the maintenance of pre-trial detention ) . we concluded that stf has not interfered in first degree decisions about pre-trial detention and that is necessary to discuss the drug criminalization in brazil . the main_contribution of the paper is to provide models that can support judges and pre-trial detainees .
title : an integrated method for classification of indus and english document_images ; abstract : unlike classification of documents with plain background and high resolution , classification of historical_document , namely indus_script written on stone , wall , and palm leaves is challenging because of sources on which script is written and various handwriting , which causes noise , distortions , background variations , multisized text , and multifont . in this paper , we propose an integrated method that has two-stage algorithms to classify indus and english from the south_indian documents . the first stage uses morphological operations and thinning on canny of the input image to study the straightness and cursiveness of thinned components to classify the indus document from the south_indian and english . the second stage proposes region growing and thinning to study the straightness and cursiveness of the thinned edges to classify the english from the south_indian documents . we select 100 documents for each script in total 600 documents to evaluate the performance of the method . the comparative study with existing method shows that the proposed method_outperforms the existing method in terms of classification rate . © 2014 springer india .
title : a method to transform automatically extracted product features into inputs for kano-like models ; abstract : background : in the context of a larger research project , we plan to automatically_extract user needs ( i.e. , functional_requirements ) from online open sources and classify them using the principles of the kano model . in this paper , we present a two-step method for automatically transforming feature related text extracted from online open sources into inputs for kano-like models . goal : the problem we are facing is how to transform requirements and related sentiments extracted from raw texts collected from an online open_source into the input format required by our kano-like models . to solve this problem , we need a method that transforms requirements and related sentiments into a format that corresponds to answers that would be given to either the functional or dysfunctional question of the kano method on a specific requirement . method : we propose a method consisting of two steps . in the first step , we apply machine_learning methods to decide whether a text_line extracted from an online open_source corresponds to an answer of the functional or dysfunctional question asked in the kano method . in the second step , we use a dictionary-based method to classify the sentiment of each statement such that we can assign an answer value to each text_line previously classified as functional or dysfunctional . we implemented our method in the r language . we evaluate the accuracy of the proposed method using simulation . result : based on the simulation results , we found the overall accuracy of our method is 65 % . we also found that data sources such as app_store reviews are better suited to our analysis than question/answer sources such as stack_overflow . conclusion : the method we proposed can be used to automatically transform feature-related text into inputs for kano-like models but performance_improvements are needed .
title : a question-and-answer classification technique for constructing and managing spoken_dialog system ; abstract : to recognize user speech accurately and respond to it appropriately , a spoken_dialog system usually uses a question-and-answer database ( qadb ) which contains many question-and-answer pairs . the systems first select a question example which is the most similar to the recognition result for the input voice from the database . an answer sentence which is then paired with the selected question example is output to the user . many systems have a large database to enable a more appropriate answer to be output . however , when such a database is used , the waiting time increases because the system needs to find the most appropriate question example from a vast number of question examples . we propose a method of classifying the queries in the qadb . by classifying question examples into some clusters using plsa , an appropriate question example can be found more quickly than when using the conventional method . we evaluated the validity of our proposed method by changing various parameters . © 2011 ieee .
title : bae : bert-based adversarial_examples for text_classification ; abstract : modern text_classification models are susceptible to adversarial_examples , perturbed versions of the original text indiscernible by humans which get misclassified by the model . recent_works in nlp use rule-based synonym replacement strategies to generate_adversarial_examples . these strategies can lead to out-of-context and unnaturally complex token replacements , which are easily identifiable by humans . we present bae , a black box attack for generating_adversarial_examples using contextual perturbations from a bert masked_language_model . bae replaces and inserts tokens in the original text by masking a portion of the text and leveraging the bert-mlm to generate alternatives for the masked tokens . through automatic and human evaluations , we show that bae performs a stronger attack , in addition to generating_adversarial_examples with improved grammaticality and semantic coherence as compared to prior work .
title : cross-domain text_classification through iterative refining of target categories representations ; abstract : cross-domain text_classification deals with predicting topic labels for documents in a target domain by leveraging knowledge from pre-labeled_documents in a source_domain , with different terms or different distributions thereof . methods exist to address this problem by re-weighting documents from the source_domain to transfer them to the target one or by finding a common feature_space for documents of both domains ; they often require the combination of complex techniques , leading to a number of parameters which must be tuned for each dataset to yield optimal performances . we present a simpler method based on creating explicit representations of topic categories , which can be compared for similarity to the ones of documents . categories representations are initially built from relevant source documents , then are iteratively refined by considering the most similar target documents , with relatedness being measured by a simple regression model based on cosine_similarity , built once at the begin . this expectedly leads to obtain accurate representations for categories in the target domain , used to classify documents therein . experiments on common benchmark text_collections show that this approach obtains results better or comparable to other methods , obtained with fixed empirical values for its few parameters .
title : chinese text semantic representation for text_classification ; abstract : text_representation based on word_frequency statistics is often unsatisfactory because it ignores the semantic relationships between words , and considers them as independent features . in this paper , a new chinese text semantic representation model is proposed by considering contextual semantic and background information on the words in the text . the method captures the semantic relationships between words using wikipedia as a knowledge_base . words with strong semantic relationships are combined into a word-package as indicated by a graph node , which is weighted with the sum of the number and frequency of the words it contains . the contextual relationship between words in different word-packages is stated by a directed edge , which is weighted with the maximum weight of its adjacent nodes . the model retains the contextual_information on each word with a large extent . meanwhile , the semantic meaning between words is strengthened . experimental results of chinese text_classification show that the proposed model can express the content of a text accurately and improve the performance of text_classification . compared to support_vector_machines , text semantic graph-based classification can improve the efficiency by 7.8 % , reduce the error_rate by 1/3 , and show more stability .
title : performance evaluation tools for zone segmentation and classification ( pets ) ; abstract : this paper describes a set of performance evaluation tools ( pets ) for document_image zone segmentation and classification . the tools allow researchers and developers to evaluate , optimize and compare their algorithms by providing a variety of quantitative performance_metrics . the evaluation of segmentation quality is based on the pixel-based overlaps between two sets of zones proposed by randriamasy and vincent [ 1 ] . pets extends the approach by providing a set of metrics for overlap analysis , rle and polygonal representation of zones and introduces type-matching to evaluate zone classification . the software is available for research use . © 2010 ieee .
title : domain-independent classification of automatic_speech_recognition texts ; abstract : call centers receive large amounts of incoming calls . the calls are being regularly processed by the analytical system , which helps people automatically inspect all the data . such system demands a classification module that can determine the topic of conversation for each call . due to high costs of manual_annotation , the input for this module is the automatically transcribed calls . hence , the texts ( =automatic transcription ) used for classification contain ill-transcribed words which can probably influence the classification process . another important point is that this module also has special requirements : it should be domain-independent and easy to setup . document_classification task always requires an annotated_data set for classifier training , but it seems to be too costly to make an annotated training_set for each domain manually . in this paper , we propose an approach to automatic_speech_recognition texts classification that allows the user avoiding full manual_annotation and at the same time to control its quality .
title : hybrid modelling of an off line arabic handwriting_recognition system : results and evaluation ; abstract : in this paper , we presented a state of the art in the field of arabic handwriting_recognition as well as the techniques used . then , we detailed the general architecture of an arabic handwriting_recognition system ( ahrs ) and the contributions that we have proposed at each phase : first , an analytical segmentation approach based on a morphological/structural_analysis of the entire word and sub word ; then , a combination of different features to extract the relevant_information from text image ; thereafter , a hybrid classification approach using long short_term_memory recurrent_neural_network ( lstm-rnn ) and different other statistical classifiers to learn many characters shapes simultaneously and to improve the performance of our proposed system ; and finally , a post-processing approach to reconstruct the characters , words and lines of the text using morpho-syntactic analyser 'alkhalil morph sys ' . our proposed system has shown promising_results to solve the difficulties of arabic handwriting_recognition .
title : enabling classifiers to make judgements explicitly aligned with human values ; abstract : many nlp classification_tasks , such as sexism/racism detection or toxicity detection , are based on human values . yet , human values can vary under diverse cultural conditions . therefore , we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command . along with the task , we propose a practical approach that distills value-aligned knowledge from large-scale language models ( llms ) to construct value-aligned classifiers in two steps . first , we generate value-aligned training_data from llms by prompt-based few-shot learning . next , we fine-tune smaller classification models with the generated data for the task . empirical results show that our va-models surpass multiple baselines by at least 15.56 % on the f1-score , including few-shot learning with opt-175b and existing text_augmentation methods . we suggest that using classifiers with explicit human value input improves both inclusivity & explainability in ai .
title : an automatically_constructed thesaurus for neural_network based document_categorization ; abstract : this paper presents a method for computing a thesaurus from a text_corpus , and combined with a revised back-propagation neural_network ( bpnn ) learning algorithm for document_categorization . automatically_constructed thesaurus is a data_structure that accomplished by extracting the relatedness between words . neural_network is one of the efficient approaches for document_categorization . however the conventional bpnn has the problems of slow learning and easy to involve into the local minimum . we use a revised algorithm to improve the conventional bpnn that can overcome these problems . a well constructed thesaurus has been recognized as valuable tool in the effective operation of document_categorization , it overcome some problem for the document_categorization based on bag of words which ignored the relationship between words . to investigate the effectiveness of our method , we conducted the experiments on the standard reuter-21578 . the experimental results show that the proposed model was able to achieve higher categorization effectiveness as measured by the precision , recall and f-measure . crown_copyright © 2009 .
title : is attention always needed ? a case_study on language identification from speech ; abstract : language identification ( lid ) , a recommended initial_step to automatic_speech_recognition ( asr ) , is used to detect a spoken_language from audio specimens . in state-of-the-art systems capable of multilingual speech_processing , however , users have to explicitly set one or more languages before using them . lid , therefore , plays a very important role in situations where asr based systems can not parse the uttered language in multilingual contexts causing failure in speech_recognition . we propose an attention_based convolutional_recurrent_neural_network ( crnn with attention ) that works on mel-frequency_cepstral coefficient ( mfcc ) features of audio specimens . additionally , we reproduce some state-of-the-art approaches , namely convolutional_neural_network ( cnn ) and convolutional_recurrent_neural_network ( crnn ) , and compare them to our proposed method . we performed extensive_evaluation on thirteen different indian languages and our model achieves classification_accuracy over 98 % . our lid model is robust to noise and provides 91.2 % accuracy in a noisy scenario . the proposed model is easily extensible to new languages .
title : ontology-based modelling of related work sections in research articles : using crfs for developing semantic data based information_retrieval systems ; abstract : research articles are an important form of scientific communication . however , currently there are hardly any systems which exploit the content of research articles for information_retrieval . the paper describes our work carried out in developing ontology-based information_retrieval system using information extracted about sentences in research articles . we present results of a supervised_learning mechanism using conditional_random_fields for context identification and sentence_classification of sentences in the related work section of research articles . the labelling of sentences is carried out based on a classification framework , which we propose for classifying sentences in these sections . we proceed to develop a sentence context ontology for modelling the classified data obtained through crfs . we also show how the ontology is further used for creating rdf data . finally , we describe the user_interface developed using sewese tags and sparql for querying the developed rdf data . © 2010 acm .
title : legal_documents categorization by compression ; abstract : in this paper we investigate how to categorize text excerpts from italian normative texts . although text_categorization is a problem of broader interest , we single out a specific issue . namely , we are concerned with categorizing the set of subjects in which italian regions are allowed to produce norms : this is the so-called residual legislative power problem . it basically consists in making explicit a set of subjects that was originally defined only in a residual and negative fashion . the categorization of legal text_fragments is acknowledged to be a difficult problem , featured by abstract concepts along with a variety of locutions used to denote them , by convoluted sentence structure , and by several other facets . in addition , in the present case subjects are often partially overlapped , and a training_set of sufficient size ( for the problem under consideration ) does not exist : all these aspects make our task challenging . in this setting , classical feature-based approaches provide poor_quality results , so we explored algorithms based on compression techniques . we tested three such techniques : we illustrate their main features and report the results of an experimentation where our implementation of such algorithms is compared with the output of standard machine_learning algorithms . far from having found a silver bullet , we show that compression-based techniques provide the best results for the problem at hand , and argue that these approaches can be effectively coupled with more informative and semantically grounded ones . copyright 2013 acm .
title : graph_convolutional_network for swahili news classification ; abstract : this work empirically demonstrates the ability of text graph_convolutional_network ( text_gcn ) to outperform traditional natural_language_processing benchmarks for the task of semi-supervised swahili news classification . in particular , we focus our experimentation on the sparsely-labelled semi-supervised context which is representative of the practical constraints facing low-resourced african_languages . we follow up on this result by introducing a variant of the text_gcn model which utilises a bag of words embedding rather than a naive one-hot encoding to reduce the memory_footprint of text_gcn whilst demonstrating similar predictive performance .
title : improved text language identification for the south_african languages ; abstract : virtual assistants and text chatbots have recently been gaining_popularity . given the short message nature of text-based chat interactions , the language identification systems of these bots might only have 15 or 20 characters to make a prediction . however , accurate text language identification is important , especially in the early_stages of many multilingual natural_language_processing pipelines . this paper investigates the use of a naive_bayes_classifier , to accurately predict the language_family that a piece of text belongs to , combined with a lexicon based classifier to distinguish the specific south african language that the text is written in . this approach leads to a 31 % reduction in the language detection error . in the spirit of reproducible research the training and testing datasets as well as the code are published on github . hopefully it will be useful to create a text language identification shared_task for south_african languages .
title : automated classification of adverse_events in pharmacovigilance ; abstract : adverse_events ( aes ) are a significant concern in healthcare , since it is among the leading causes of morbidity and mortality [ 12 ] . according to the food and drug administration ( fda ) , between 2006 and 2014 , there was a 232 % increase in ae cases reported to have caused mortality [ 13 ] . in fact , the volume of all ae cases reported to the fda has increased by almost five fold since 1997 [ 13 ] . pharmaceutical companies are struggling to handle the increased case volume due to manual logging of individual cases . this is not a sustainable solution as we see the volume of ae case logs increase exponentially [ 12,13 ] . in this paper , we discuss our work and findings for implementing a pharmacovigilance automation solution . this solution explores machine_learning techniques in being able to identify serious vs non-serious adverse_event narrative logs . while developing our methodology , we explored both traditional_machine_learning and deep_learning techniques . our final model achieved a mean f1-score of 95 % and an mcc score of 0.80 on the ae case narratives .
title : heli-ots , off-the-shelf language identifier for text ; abstract : this paper introduces heli-ots , an off-the-shelf text language identification tool using the heli language identification method . the heli-ots language identifier is equipped with language models for 200 languages and licensed for academic as well as commercial use . we present the heli method and its use in our previous_research . then we compare the performance of the heli-ots language identifier with that of fasttext on two different data sets , showing that fasttext favors the recall of common languages , whereas heli-ots reaches both high recall and high precision for all languages . while introducing existing off-the-shelf language identification tools , we also give a picture of digital_humanities-related research that uses such tools . the validity of the results of such research depends on the results given by the language identifier used , and especially for research focusing on the less common languages , the tendency to favor widely used languages might be very detrimental , which heli-ots is now able to remedy .
title : adversarial_attack on sentiment_classification ; abstract : in this paper , we propose a white-box attack algorithm called “ global search ” method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called “ greedy search ” . the attack methods are evaluated on the convolutional_neural_network ( cnn ) sentiment classifier trained on the imdb movie review dataset . the attack_success_rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial_examples . the experiment results show that the proposed “ global search ” method generates more powerful adversarial_examples with less distortion or less modification to the source text .
title : an efficient method for document_categorization based on word2vec and latent_semantic_analysis ; abstract : document_categorization is the process of classifying documents from many mixed documents automatically , and the main problem is how to express document content in vector_space completely . this paper proposes a new model named latent_semantic_analysis ( lsa ) + word2vec to categorize documents . this is the first attempt of combining word2vec with lsa at document_categorization and it can map document to vector space under the premise of keeping document contents fully . at first , we create a term by document matrix and the element of which is decided by term_frequency-inverse_document_frequency ( tf-idf ) weighting and word_vector trained by word2vec . this matrix is a 3-dimensional matrix and it can describe the meaning of every word and the content of every document exactly . secondly , singular_value_decomposition ( svd ) is executed on the matrix and lower computational_complexity is gained from this . the model is named lsa + word2vec . then , document vector gained from the new model are put into convolutional_neural_network ( cnn ) to train . cnn is an efficient deep_learning algorithm , which improves the accuracy of classification greatly . we evaluate the performance based on the 20newsgroups corpus . the results show that our new model achieves better effects on document_categorization tasks , and the accuracy made about 15 % improvement than traditional_methods , such as lsa and vector_space_model ( vsm ) .
title : science , policy , and the public discourse of shark `` attack '' : a proposal for reclassifying human-shark interactions ; abstract : there are few phrases in the western world that evoke as much emotion or as powerful an image as the words `` shark '' and `` attack . '' however , not all `` shark attacks '' are created equal . under current labels , listings of shark_attack may even include instances where there is no physical contact between shark and human . the dominant perception of intent-laden shark `` attacks '' with fatal outcomes is outdated as a generic term and misleading to the public . we propose new descriptive labels based on the different outcomes associated with human-shark interactions , including sightings , encounters , bites , and the rare cases of fatal bites . we argue two central points : first , that a review of the scientific_literature shows that humans are `` not on the menu '' as typical shark prey . second , we argue that the adoption of a more prescriptive code of reporting by scientists , the media , and policy_makers will serve the public interest by clarifying the true risk posed by sharks and informing better policy_making . finally , we apply these new categories to the 2009 new south_wales shark meshing report in australia and the history of shark incidents in florida to illustrate how these changes in terminology can alter the narratives of human-shark interactions . © 2013 the author ( s ) .
title : english_grammar_error_correction algorithm based on classification model ; abstract : english_grammar_error_correction algorithm refers to the use of computer programming technology to automatically recognize and correct the grammar errors contained in english text written by nonnative language learners . classification model is the core of machine_learning and data_mining , which can be applied to extracting information from english text data and constructing a reliable grammar correction method . on the basis of summarizing and analyzing previous_research works , this paper expounded the research status and significance of english_grammar_error_correction algorithm , elaborated the development background , current status , and future challenges of the classification model , introduced the methods and principles of feature_extraction method and dynamic residual structure , constructed a basic model for english_grammar_error_correction based on the classification model , analyzed the classification model and translation model of english_grammar_error_correction , proposed the english_grammar_error_correction algorithm based on the classification model , performed the analyses of the model architecture and model optimizer of the grammar error_correction algorithm , and finally conducted a simulation experiment and its result analysis . the study results show that , with the continuous_increase of training_samples and the continuous progress of learning process , the proposed english_grammar_error_correction algorithm based on the classification model will continue to increase its classification_accuracy , further refine its recognition rules , and gradually improve correction efficiency , thereby reducing processing time , saving storage_space , and streamlining processing flow . the study results of this paper provide a certain reference for the further research on english_grammar_error_correction algorithm based on the classification model .
title : a sentiment_analysis method of chinese specialized field short commentary ; abstract : with the development of media technology , text sentiment_analysis has become one of the focuses in recent_years . how to solve the classification issue which is under the circumstance of lacking sentiment_lexicons and labeled_samples needs further study . in this paper , we choose chinese finance news commentary as research corpus and propose a text model based on the word popularity by mining the time feature of text . then , we use clustering methods to expand sentiment_lexicon and complete classification by the method of supervised_machine_learning approach . the experimental results show that this method has high accuracy and good pertinence of specialized news commentary .
title : offensive_language classification of code-mixed tamil with keras ; abstract : this paper presents the method adopted for completing task 1 of dravidian-codemix-hasoc ( hate_speech_and_offensive content identification in english and indo-european_languages ) shared_task proposed by the forum of information_retrieval evaluation in 2021 , for offensive_language_detection . for detecting offensive_language , a custom model architecture using convolutional_neural_networks was created using keras for supervised_learning , and trained on a dataset of youtube comments , written in code-mixed tamil in both roman and tamil scripts . the 5 layer neural_network was built only using keras , and required simple tokenized data , padded to an appropriate length . recurrent_neural_networks and transfer_learning were not used , and an f-score of 0.835 was achieved with the created cnn model .
title : developing amaia : a conversational_agent for helping portuguese entrepreneurs—an extensive exploration of question-matching approaches for portuguese ; abstract : this paper describes how we tackled the development of amaia , a conversational_agent for portuguese entrepreneurs . after introducing the domain corpus used as amaia ’ s knowledge_base ( kb ) , we make an extensive comparison of approaches for automatically matching user_requests with frequently_asked_questions ( faqs ) in the kb , covering information_retrieval ( ir ) , approaches based on static and contextual_word_embeddings , and a model of semantic textual similarity ( sts ) trained for portuguese , which achieved the best performance . we further describe how we decreased the model ’ s complexity and improved scalability , with minimal impact on performance . in the end , amaia combines an ir library and an sts model with reduced features . towards a more human-like behavior , amaia can also answer out-of-domain questions , based on a second corpus integrated in the kb . such interactions are identified with a text classifier , also described in the paper .
title : sentiment_classification of sinhala content in social_media : a comparison between stemmers and n-gram features ; abstract : sentiment_classification for non-english languages has gained significant attention from researchers in the past few years with the increasing use of non-english scripts and romanized scripts for expressing sentiments over social_media . in this study , we begin by classifying sinhala sentiments on social_media into positive and negative_polarity classes using n-gram feature_extraction . n-grams are a contiguous sequence of words or characters of a text . then we focus on improving the classification_accuracy by employing different stemming methods . stemming is generally used to reduce the dimensionality of the feature_set - something which needs to be carried out with great care as over reducing feature dimensionality causes the classification_accuracy to decrease . finally , we compare the accuracy and efficiency of n-gram feature_extraction and stemming based sentiment_analysis models .
title : automatic coding of text answers to open-ended_questions : should you double code the training_data ? ; abstract : open-ended questions in surveys are often manually_coded into one of several classes ( or categories ) . when the data are too large to manually code all texts , a statistical ( or machine ) learning model must be trained on a manually_coded subset of texts . uncoded texts are then coded automatically using the trained model . the quality of automatic coding depends on the trained statistical_model , and the model relies on manually_coded data on which it is trained . while survey scientists are acutely aware that the manual_coding is not always accurate , it is not clear how double coding affects the classification errors of the statistical learning model . we investigate several budget allocation strategies when there is a limited budget for manual classification : single coding versus various options for double coding where the number of training texts is reduced to maintain the fixed budget . under fixed budget , double coding improved prediction of the learning algorithm when the coding error is greater than about 20–35 % , depending on the data . among double-coding strategies , paying for an expert to resolve differences performed best . when no expert is available , removing differences from the training_data outperformed other double-coding strategies . when there is no budget_constraint and the texts have already been double coded , all double-coding strategies generally outperformed single coding . as under fixed budget , having an expert to solve disagreement in training texts improves accuracy most , followed by removing differences .
title : motifclass : weakly_supervised text_classification with higher-order metadata information ; abstract : we study the problem of weakly_supervised text_classification , which aims to classify text documents into a set of pre-defined categories with category surface names only and without any annotated training document provided . most existing classifiers leverage textual_information in each document . however , in many domains , documents are accompanied by various types of metadata ( e.g. , authors , venue , and year of a research paper ) . these metadata and their combinations may serve as strong category indicators in addition to textual_contents . in this paper , we explore the potential of using metadata to help weakly_supervised text_classification . to be specific , we model the relationships between documents and metadata via a heterogeneous_information network . to effectively capture higher-order structures in the network , we use motifs to describe metadata combinations . we propose a novel framework , named motifclass , which ( 1 ) selects category-indicative motif instances , ( 2 ) retrieves and generates pseudo-labeled training_samples based on category_names and indicative motif instances , and ( 3 ) trains a text classifier using the pseudo training_data . extensive_experiments on real-world datasets demonstrate the superior performance of motifclass to existing weakly_supervised text_classification approaches . further analysis shows the benefit of considering higher-order metadata information in our framework .
title : a two-teacher framework for knowledge distillation ; abstract : knowledge distillation aims at transferring_knowledge from a teacher network to a student network . commonly , the teacher network has high capacity , while the student network is compact and can be deployed to embedded_systems . however , existing distillation methods use only one teacher to guide the student network , and there is no guarantee that the knowledge is sufficiently transferred to the student network . thus , we propose a novel framework to improve the performance of the student network . this framework consists of two teacher networks trained with different strategies , one is trained strictly to guide the student network to learn sophisticated features , and the other is trained loosely to guide the student network to learn general decision based on learned features . we perform extensive_experiments on two standard image_classification datasets : cifar-10 and cifar-100 . and results demonstrate that the proposed framework can significantly_improve the classification_accuracy of a student network .
title : effective pruning algorithm for least squares support_vector_machine classifier ; abstract : a well-known drawback in the least squares support_vector_machine ( ls-svm ) is that the sparseness is lost . in this study , an effective pruning algorithm is developed to deal with this problem . to avoid solving the primal set of linear equations , the bottom to the top strategy is adopted in the proposed algorithm . during the training process of the algorithm , the chunking incremental and decremental learning procedures are used alternately . a small support_vector set , which can cover most of the information in the training_set , can be formed adaptively . using the support_vector set , one can construct the final classifier . in order to test the validation of the proposed algorithm , it has been applied to five benchmarking uci datasets . in order to show the relationships among the chunking size , the number of support_vector_machine , the training time , and the testing accuracy , different chunking sizes are tested . the experimental results show that the proposed algorithm can adaptively obtain the sparse solutions without almost losing generalization performance when the chunking size is equal to 2 , and also its training_speed is much faster than that of the sequential_minimal_optimization ( smo ) algorithm . the proposed algorithm can also be applied to the least squares support_vector regression machine as well as ls-svm classifier .
title : explainable and transferrable text_categorization ; abstract : automated argument stance ( pro/contra ) detection is a challenging text_categorization problem , especially if said arguments are to be detected for new topics . in previous_research , we designed and evaluated an explainable machine_learning based classifier . it was capable to achieve 96 % f1 for argument stance recognition within the same topic and 60 % f1 for previously_unseen topics , which informed our hypothesis , that there are two sets of features in argument stance recognition : general features and topic specific features . an advantage of the described system is its quick transferability to new problems . besides providing further details about the developed c3 tfidf-svm classifier , we investigate the classifiers effectiveness for different text_categorization problems spanning two natural_languages . besides the quick transferability , the generation of human readable explanations about why specific results were achieved is a key feature of the described approach . we further investigate the generated explanation understandability and conduct a survey about how understandable the classifier ’ s explanations are .
title : impact of word_embedding methods on software vulnerability severity prediction models ; abstract : software vulnerability severity prediction has recently gained_popularity in software_engineering . to perform this task , a prediction model has to be developed . the model 's performance greatly depends on the feature_vectors used to train it . these feature_vectors are formed using textual_data present on software vulnerability and converting it to numerical form . there are various methods for feature_vector representation using word_embeddings . unlike_traditional methods such as tf-idf , word_embedding methods using deep_learning map the words into vectors , preserving the semantic relationships between words as well as doing automatic feature_selection , which holds great importance when dealing with textual_data . therefore , word_embedding methods using deep_learning showed promising_results over traditional tf-idf . hence , in this paper , we conducted a controlled experiment to examine the effect of various word_embedding methods for feature_vector representation on the performance of the prediction model . the models developed in this study used tf-idf , word2vec , and glove , coupled with svm , lstm , and bi-lstm classification_algorithms . the classification performance of different word_embedding methods is analyzed on five different vulnerability datasets of mozilla products . the experiments showed that the best performance is achieved by using the glove method with the bi-lstm algorithm giving an average auc value of 0.81 , which is an improvement of 12.5 % from the traditional method of feature_vector representation .
title : spam_filtering with knn : investigation of the effect of k value on classification performance ; abstract : in this study , it is aimed to filter spam e-mails by using machine_learning and text_mining techniques . k-nearest_neighbor ( knn ) algorithm which is one of the techniques of machine_learning is used . knn_algorithm is an easy to use and high performance classification algorithm . but the main problem of this algorithm is what will be the k value at the beginning . the performance of the algorithm changes according to the selected k value . in this study , three different data sets are discussed . these are enron , ling-spam and smsspam-collection data sets . firstly , basic text_mining techniques and term_frequency-inverse_document_frequency ( tf-idf ) term_weighting_method are applied to all data sets . by , according to the chi-square_feature_selection method , the best 500 attributes are selected and given to knn_algorithm . finally , extensive_experiments are carried out by giving the values of 1 , 3 , 5 , 7 and 9 to the k value of the algorithm . in all three data sets , the most successful result is obtained when k is 1 . the most successful results obtained from ling-spam , enron and smsspam-collection data sets according to f-measure are 0:9324 , 0:9215 and 0:9196 respectively .
title : improved distant_supervision relation_extraction based on edge-reasoning hybrid graph model ; abstract : distant_supervision relation_extraction ( dsre ) trains a classifier by automatically labeling data through aligning triples in the knowledge_base ( kb ) with large-scale corpora . training_data generated by distant_supervision may contain many mislabeled instances , which is harmful to the training of the classifier . some recent methods show that relevant background information in kbs , such as entity_type ( e.g. , organization and book ) , can improve the performance of dsre . however , there are three main problems with these methods . firstly , these methods are tailored for a specific type of information . a specific type of information only has a positive effect on a part of instances and will not be beneficial to all cases . secondly , different background information is embedded independently , and no reasonable interaction is achieved . thirdly , previous methods do not consider the side effect of the introduced noise of background information . to address these issues , we leverage five types of background information instead of a specific type of information in previous_works and propose a novel edge-reasoning hybrid graph ( er-hg ) model to realize reasonable interaction between different kinds of information . in addition , we further employ an attention_mechanism for the er-hg model to alleviate the side effect of noise . the er-hg model integrates all types of information efficiently and is very robust to the noise of information . we conduct_experiments on two widely used datasets . the experimental results demonstrate that our model outperforms the state-of-the-art methods significantly in held-out metric and robustness tests .
title : robust sentence_classification by solving out-of-vocabulary problem with auxiliary word predictor ; abstract : in recent_years , deep_learning methods have achieved outstanding performances in sentence_classification . however , many sentence_classification models do not consider the out-of-vocabulary ( oov ) problem , which generally appears in sentence_classification tasks . input units smaller than words , such as characters or subword units , have been considered the basic unit for sentence_classification to cope with the oov problem . although this approach naturally solves the oov problem , it has obvious performance limitations because a character by itself has no meaning , whereas a word has a definite meaning . in this paper , we propose a neural sentence_classification model that is robust to the oov problem , even though the proposed model utilizes words as the basic unit . to this end , we introduce the unknown word prediction ( uwp ) task as an auxiliary task to train the proposed model . owing to joint_training of the proposed model with the objectives of classification and uwp , the proposed model can represent the meanings of entire sentences robustly even if a sentence includes a number of unseen words . to demonstrate the effectiveness of the proposed model , a number of experiments are conducted using several sentence_classification benchmarks . the proposed model consistently_outperforms two baselines over all four benchmark_datasets in terms of the classification_accuracy .
title : multi-label_classification of e-commerce customer_reviews via machine_learning ; abstract : the multi-label customer_reviews classification_task aims to identify the different thoughts of customers about the product they are purchasing . due to the impact of the covid-19_pandemic , customers have become more prone to shopping online . as a consequence , the amount of text data on e-commerce is continuously increasing , which enables new studies to be carried out and important findings to be obtained with more detailed analysis . nowadays , e-commerce customer_reviews are analyzed by both researchers and sector experts , and are subject to many sentiment_analysis studies . herein , an analysis of customer_reviews is carried out in order to obtain more in-depth thoughts about the product , rather than engaging in emotion-based analysis . initially , we form a new customer_reviews dataset made up of reviews by turkish consumers in order to perform the proposed analysis . the created dataset contains more than 50,000 reviews in three different categories , and each review has multiple_labels according to the comments made by the customers . later , we applied machine_learning methods employed for multi-label_classification to the dataset . finally , we compared and analyzed the results we obtained using a diverse set of statistical metrics . as a result of our experimental studies , we found the micro precision 0.9157 , micro recall 0.8837 , micro_f1 score 0.8925 , and hamming_loss 0.0278 to be the most successful approaches .
title : detecting suspicious texts using machine_learning techniques ; abstract : due to the substantial growth of internet users and its spontaneous access via electronic devices , the amount of electronic contents has been growing enormously in recent_years through instant_messaging , social_networking posts , blogs , online portals and other digital platforms . unfortunately , the misapplication of technologies has increased with this rapid_growth of online content , which leads to the rise in suspicious activities . people misuse the web media to disseminate malicious activity , perform the illegal movement , abuse other people , and publicize suspicious contents on the web . the suspicious contents usually available in the form of text , audio , or video , whereas text contents have been used in most of the cases to perform suspicious activities . thus , one of the most challenging_issues for nlp researchers is to develop a system that can identify suspicious text efficiently from the specific contents . in this paper , a machine_learning ( ml ) -based classification model is proposed ( hereafter called std ) to classify bengali text into non-suspicious and suspicious categories based on its original contents . a set of ml_classifiers with various features has been used on our developed corpus , consisting of 7000 bengali text documents where 5600 documents used for training and 1400 documents used for testing . the performance of the proposed system is compared with the human baseline and existing ml_techniques . the sgd classifier 'tf-idf ' with the combination of unigram_and_bigram features are used to achieve the highest_accuracy of 84.57 % .
title : an intelligent hybrid sentiment_analyzer for personal protective medical equipments based on word_embedding technique : the covid-19 era ; abstract : due to the accelerated growth of symmetrical sentiment data across different platforms , experimenting with different sentiment_analysis ( sa ) techniques allows for better decision-making and strategic planning for different sectors . specifically , the emergence of covid-19 has enriched the data of people ’ s opinions and feelings about medical products . in this paper , we analyze people ’ s sentiments about the products of a well-known e-commerce website named alibaba.com . people ’ s sentiments are experimented with using a novel evolutionary approach by applying advanced pre-trained_word_embedding for word presentations and combining them with an evolutionary feature_selection mechanism to classify these opinions into different levels of ratings . the proposed approach is based on harmony search_algorithm and different classification techniques including random_forest , k-nearest_neighbor , adaboost , bagging , svm , and reptree to achieve competitive_results with the least possible features . the experiments are conducted on five different datasets including medical gloves , hand_sanitizer , medical oxygen , face masks , and a combination of all these datasets . the results show that the harmony search_algorithm successfully reduced the number of features by 94.25 % , 89.5 % , 89.25 % , 92.5 % , and 84.25 % for the medical glove , hand_sanitizer , medical oxygen , face masks , and whole datasets , respectively , while keeping a competitive_performance in terms of accuracy and root mean square error ( rmse ) for the classification techniques and decreasing the computational time required for classification .
title : local ensemble_learning from imbalanced and noisy_data for word_sense_disambiguation ; abstract : natural_language_processing plays a key role in man-machine interactions , allowing computers to understand and analyze human language . one of its more challenging sub-domains is word_sense_disambiguation , the task of automatically identifying the intended sense ( or concept ) of an ambiguous_word based on the context in which the word is used . this requires proper feature_extraction to capture specific data properties and a dedicated machine_learning solution to allow for the accurate labeling of the appropriate sense . however , the pattern classification problem posed here is highly challenging , as we must deal with high-dimensional and multi-class imbalanced_data that additionally may be corrupted with class_label noise . to address these issues , we propose a local ensemble_learning solution . it uses a one-class decomposition of the multi-class problem , assigning an ensemble of one-class classifiers to each of the distributions . the classifiers are trained on the basis of low-dimensional subsets of features and a kernel feature_space transformation to obtain a more compact representation . instance_weighting is used to filter out potentially noisy instances and reduce overlapping among classes . finally , a two-level classifier fusion technique is used to reconstruct the original multi-class problem . our results show that the proposed learning approach displays robustness to both multi-class skewed distributions and class_label noise , making it a useful tool for the considered task .
title : evaluation of classification techniques for identifying fake_reviews about products and services on the internet ; abstract : with the e-commerce growth , more people are buying products over the internet . to increase customer_satisfaction , merchants provide spaces for product and service reviews . products with positive reviews attract customers , while products with negative reviews lose customers . following this idea , some individuals and corporations write fake_reviews to promote their products and services or defame their competitors . the difficulty for finding these reviews was in the large amount of information available . one solution is to use data mining techniques and tools , such as the classification function . exploring this situation , the present work evaluates classification techniques to identify fake_reviews about products and services on the internet . the research also presents a literature systematic_review on fake_reviews . the research used 8 classification_algorithms . the algorithms were trained and tested with a hotels database . the concenso algorithm presented the best result , with 88 % in the precision indicator . after the first test , the algorithms classified reviews on another hotels database . to compare the results of this new classification , the review skeptic algorithm was used . the svm and glmnet algorithms presented the highest convergence with the review skeptic algorithm , classifying 83 % of reviews with the same result . the research contributes by demonstrating the algorithms ability to understand consumers ' real reviews to products and services on the internet . another contribution is to be the pioneer in the investigation of fake_reviews in brazil and in production engineering .
title : integrated question_classification based on rules and pattern_matching ; abstract : in question_answering , task of question_classification has remained in sharp focus since long . this paper presents our research about question_classification in higher_education domain . the questions for the domain are divided into 9 coarse_grained categories and 61 fine_grained categories . in the proposed integrated classification method , rules are adopted to extract coarse grain categories and focus words while pattern_matching is implemented to classify the questions in fine_grained categories . experimental_result_shows the efficiency of our method which achieves satisfactory_performance with the state of the art in this field .
title : analysis of the relation between stock price returns and headline_news using text_categorization ; abstract : in this paper , we analyze about the relation between stock price returns and headline_news . headline_news is very important sources of information in asset_management , and is sent in large_quantities every day . we study the effect of more than 13,000 headline_news sent from jiji_press . we classify headline_news using text_categorization and analyze the reaction of a stock price return for every type of news . from our research , we figure out following issues ; 1 ) we make the text_categorization system that has about 80 % of classification_accuracy , 2 ) this system can extract effective information to stock price returns from headline_news . ©_springer-verlag_berlin_heidelberg 2007 .
title : angular contour parameterization for signature identification ; abstract : this present work presents a parameterization system based on angles from signature edge ( 2d-shape ) for off-line signature identification . we have used three different classifiers , the nearest_neighbor classifier ( k-nn ) , neural_networks ( nn ) and hidden_markov_models ( hmm ) . our off-line database has 800 writers with 24 samples per each writer ; in total , 19200 images have been used in our experiments . we have got a success_rate of 84.64 % , applying as classifier hidden_markov_model , and only used the information from this edge_detection method . ©_2009_springer-verlag_berlin_heidelberg .
title : optimal_feature_selection for learning-based algorithms for sentiment_classification ; abstract : sentiment_classification is an important branch of cognitive computation—thus the further studies of properties of sentiment_analysis is important . sentiment_classification on text data has been an active topic for the last two decades and learning-based methods are very popular and widely used in various applications . for learning-based methods , a lot of enhanced technical strategies have been used to improve the performance of the methods . feature_selection is one of these strategies and it has been studied by many researchers . however , an existing unsolved difficult problem is the choice of a suitable number of features for obtaining the best sentiment_classification performance of the learning-based methods . therefore , we investigate the relationship between the number of features selected and the sentiment_classification performance of the learning-based methods . a new method for the selection of a suitable number of features is proposed in which the chi_square_feature_selection algorithm is employed and the features are selected using a preset score threshold . it is discovered that there is a relationship between the logarithm of the number of features selected and the sentiment_classification performance of the learning-based method , and it is also found that this relationship is independent of the learning-based method involved . the new findings in this research indicate that it is always possible for researchers to select the appropriate number of features for learning-based methods to obtain the best sentiment_classification performance . this can guide researchers to select the proper features for optimizing the performance of learning-based algorithms . ( a preliminary version of this paper received a best paper award at the international_conference on extreme_learning machines 2018 . )
title : language identification of controlled systems : modeling , control and anomaly_detection ; abstract : formal_language techniques have been used in the past to study autonomous dynamical_systems . however , for controlled systems , new features are needed to distinguish between information generated by the system and input control . we show how the modeling framework for controlled dynamical_systems leads naturally to a formulation in terms of context-dependent grammars . a learning algorithm is proposed for on-line generation of the grammar productions , this formulation being then used for modeling , control , and anomaly_detection . practical_applications are described for electromechanical drives . grammatical interpolation techniques yield accurate results , and the pattern detection capabilities of the language-based formulation makes it a promising technique for the early detection of anomalies or faulty behavior .
title : conceptualization of time in the context of patau_syndrome ; abstract : this article studies communication in the context of the mosaic variant of patau_syndrome . the analysis of two sample situations focuses on creating and understanding the concept of time . the article discusses the semiotic foundation of communication dimensions ( peirce 1931-1958 ) as well as its physical basis ( itkonen 2005 ) . the approach is based on discourse_analysis and considers semiotic categorization of signs . the article describes communication possibilities of people with language impairments , in the context of research done on aphasia and conversation by charles goodwin ( 1995 , 2003 ) . further , the article introduces discourse analyses in clinical communication_studies ( müller , guendouzi , wilson 2008 ) . we also give an overview of human cognition and the mosaic variant of patau_syndrome . specifically , we examine how an abstract concept ( time ) is formed , presented and forwarded when the traditional communication modality - the ability to speak - is missing . the subject ( n ) comprehends the speech addressed to her , understands everyday topics and questions and is able to answer them , but her means of replying are incomplete . the majority of her communication proceeds by hand and head movements accompanied by sounds . thus we also consider the role of how gestures are used to bring out meaning in cooperative communication . the examples are two videotaped communicative situations where the subject of our research is one of the interlocutors . the topic of time is present in both of them . the analysis of the communicative episodes showed that the subject is able to indicate a calendar month ( june ) using the hand sign for april . however , she visualizes as well as generalizes the name of one month to all the other eleven months . from the discussion we concluded that with the help of communicative gestures , the subject is able to have a `` conversation '' on the topic when someone 's birthday is ? while using the hand movement of 'flower ' to indicate the event of birthday . she has the ability to communicate and express herself via hand gestures . we also found out that the subject has a certain memory of situations , e.g . a birthday frame and the ability of conceptual_categorization , which is mainly expressed via hand movements .
title : an empirical_study on learning based methods for user consumption intention_classification ; abstract : recently , huge amount of text with user consumption intentions have been published on the social_media platform , such as twitter and weibo , and classifying the intentions of users has great values for both scientific_research and commercial_applications . user consumption analysis in social_media concerns about the text content representation and intention_classification , whose solutions mainly focus on the traditional_machine_learning and the emerging deep_learning techniques . in this paper , we conduct a comprehensive empirical_study on the user intension classification problem with learning based techniques using different text_representation methods . we compare different machine_learning , deep_learning methods and various combinations of them in tweet text presentation and users ’ consumption intention_classification . the experimental results show that lstm models with pre-trained word_vector representation can achieve the best classification performance .
title : application of bagging_ensemble classifier based on genetic_algorithm in the text_classification of railway fault hazards ; abstract : the safety accident hidden danger of on-site inspection by railway workers are stored in text_format , and this kind of data contains a lot of valuable_information related to railway safety , so it is urgent to classify and manage the data by classification model . in this paper , we analyze the characteristics of such data . firstly , we use tf-idf method to extract text features and convert them into vectors . then , decision_tree classifier is used to classify the data . in order to improve classification_accuracy , the bagging_ensemble classifier conduct a random sample training to text vector converted by tf-idf which decision as the base_classifier , produce bagging classification results , considering the bagging algorithm is a the number of base_classifiers results voting combination classification model which has a better classification performance , we use genetic_algorithm to calculate bagging classifier_combination optimization , better classification results are produced by the ensemble_classifier based on genetic_algorithm ( evolutionary ensemble_classifier ) . through the experimental analysis of text data on safety accident hidden danger of power supply catenary in a railway bureau , it is proved that the safety classification_accuracy , recall_rate and f-score value of the evolutionary ensemble_classifier model are significantly_improved .
title : arabic text_classification using principal_component_analysis with different supervised_classifiers ; abstract : text_classification ( tc ) is the process of automatically labeling a text based on its topic . generally , tc suffers from data high dimensionality of the feature_space . in this paper , we use the principal_components_analysis ( pca ) as a feature_extraction method for reducing the space features size with an application to arabic texts . to evaluate the effect of this technique on the classification process , we use five popular classifiers : logistic_regression , k-nearest_neighbors , decision_trees , random_forest and support_vector_machines . the results show that the proposed method yields a substantial_improvement of classification_accuracy of most of the classifiers with reduction in execution time . to the best of our knowledge , it is the first time pca is used for arabic tc . in addition , the gain in training time is comprised between 5 and 800 . accuracy is improved in 60 % of the cases .
title : self-organizing word map for context-based document_classification ; abstract : in this paper , a novel som-based system for document_organization is presented . the purpose of the system is the classification of a document_collection in terms of document content . the system possesses a two-level hybrid connectionist architecture that comprises ( i ) an automatically created word map using a som , which functions as a feature_extraction module and ( ii ) a supervised mlp-based classifier , which provides the final classification result . the experiments , which have been performed on modern_greek text documents , indicate that the proposed system separates effectively the different types of text .
title : employing structural and textual feature_extraction for semistructured document_classification ; abstract : this paper addresses xml document_classification by considering both structural and content-based features of the documents . this approach leads to better constructing a set of informative feature_vectors that represents both structural and textual aspects of xml documents . for this purpose , we integrate soft_clustering of words and feature_reduction into the process . to extract structural_information , we employ an existing frequent tree-mining algorithm combined with an information gain filter to retrieve the most informative substructures from xml documents . however , for extracting content information , we propose soft_clustering of words using each cluster as a textual feature . we have conducted extensive_experiments on a benchmark_dataset , namely 20newsgroups , and an xml documents dataset given in logml that describes the web-server logs of user sessions . with regards to the classifier built only using our textual_features , the results show that it outperforms a naive support-vector-machine ( svm ) -based classifier , as well as an information_retrieval classifier ( irc ) . we further demonstrate the effectiveness of incorporating both structural and content information into the process of learning , by comparing our classifier model and several xml document classifiers . in particular , by applying svm and decision_tree algorithms using our feature_vector representation of xml documents dataset , we have achieved 85.79 % and 87.04 % classification_accuracy , respectively , which are higher than accuracy achieved by xrules , a well-known structural-based xml document classifier . © 1998-2012 ieee .
title : at the frontiers of ocr ; abstract : it is time for a major change of approach to character_recognition research . the traditional approach , focusing on the the correct classification of isolated_characters , has been exhausted . the demonstration of the superiority of a new classification method under operational conditions requires large experimental facilities and data bases beyond the resources of most researchers . in any case , even perfect classification of individual characters is insufficient for the conversion of complex archival documents to a useful computer-readable form . many practical ocr tasks require integrated treatment of entire documents and well-organized typographic and domain-specific knowledge . new ocr systems should take advantage of the typographic uniformity of paragraphs or other layout components . they should also exploit the unavoidable interaction with human operators to improve themselves without explicit “ training ” . © 1992 ieee
title : parallel sequence tagging for concept recognition ; abstract : background : named_entity_recognition ( ner ) and normalisation ( nen ) are core components of any text-mining system for biomedical_texts . in a traditional concept-recognition pipeline , these tasks are combined in a serial way , which is inherently prone to error_propagation from ner to nen . we propose a parallel architecture , where both ner and nen are modeled as a sequence-labeling task , operating directly on the source text . we examine different harmonisation strategies for merging the predictions of the two classifiers into a single output sequence . results : we test our approach on the recent version 4 of the craft corpus . in all 20 annotation sets of the concept-annotation task , our system outperforms the pipeline system reported as a baseline in the craft shared_task , a competition of the bionlp open shared_tasks 2019 . we further refine the systems from the shared_task by optimising the harmonisation strategy separately for each annotation set . conclusions : our analysis shows that the strengths of the two classifiers can be combined in a fruitful way . however , prediction harmonisation requires individual calibration on a development_set for each annotation set . this allows achieving a good trade-off between established knowledge ( training_set ) and novel information ( unseen concepts ) .
title : fuzzy_rule_based_systems for gender classification from blog data ; abstract : gender classification is a popular machine_learning task , which has been undertaken in various domains , e.g . business_intelligence , access_control and cyber_security . in the context of information granulation , gender related information can be divided into three types , namely , biological information , vision based information and social_network based information . in traditional_machine_learning , gender identification has been typically treated as a discriminative classification t ask , i.e . it is aimed at learning a classifier t hat d is criminates between male_and_female . i n this paper , we argue that it is not always appropriate to identify gender in the way of discriminative classification , especially when considering the case that both male_and_female people are of high diversity and thus individuals of different genders could have high similarity to each other in terms of their characteristics . in order to address the above issue , we propose the use of a fuzzy method for generative classification o f g ender . in particular , we focus on gender classification based on social_network information . we conduct an experiment study by using a blog data set , and compare the fuzzy method with c4.5 , naive_bayes and support_vector_machine in terms of classification performance . the results show that the fuzzy method_outperforms the other methods and is also capable of capturing the diversity of both male_and_female people and dealing with the fuzziness in terms of gender identification .
title : minimally_supervised question_classification on fine-grained taxonomies ; abstract : this article presents a minimally_supervised approach to question_classification on fine-grained taxonomies . we have defined an algorithm that automatically obtains lists of weighted terms for each class in the taxonomy , thus identifying which terms are highly related to the classes and are highly discriminative between them . these lists have then been applied to the task of question_classification . our approach is based on the divergence of probability_distributions of terms in plain_text retrieved from the web . a corpus of questions with which to train the classifier is not therefore necessary . as the system is based purely on statistical information , it does not require additional linguistic resources or tools . the experiments were performed on english questions and their spanish translations . the results reveal that our system surpasses current supervised_approaches in this task , obtaining a significant improvement in the experiments carried out . ©_2012_springer-verlag london limited .
title : research on patent classification based on hierarchical label semantics ; abstract : patent classification is an essential task in patent information_management and knowledge mining . most existing_studies are based on the textual_content of individual patent texts ( e.g. , titles and abstracts ) for classification , but the patent also has label information with hierarchical_structure and semantic description . however , the semantics correlation of patent texts and their labels have been largely ignored . in this paper , we propose a novel framework called hlspc for patent classification by leveraging the hierarchical semantics correlation of patent texts and their labels . specifically , we first apply a patent representation_learning module for capturing the semantics representation of patent texts and hierarchical labels . then , we design a label attention learning module to build the semantics correlation between patent texts and hierarchical labels , which enhances patent representation . finally , we deploy a multi-level fusion module to get the refined category prediction for each patent which can preserve both local and global hierarchical prediction information . extensive experimental results on two english patent datasets demonstrate the effective power of the hlspc model .
title : data-based prediction of sentiments using heterogeneous model ensembles ; abstract : in this paper , we present an ensemble modeling approach for sentiment_analysis using machine_learning algorithms . the main_goal of sentiment_analysis is to develop estimators that are able to identify the sentiment_orientation ( positive , negative , or neutral ) of sentences found in any arbitrary source . the novel approach presented here relies on the analysis of the words found in sentences and the formation of large sets of heterogeneous models , i.e. , binary as well as multi-class classification models that are calculated by various different machine_learning methods ; these models shall represent the relationship between the presence of given words ( or combination of words ) and sentiments . all models trained during the learning phase are applied during the test phase and the final sentiment assessment is annotated with a confidence value that specifies , how reliable the models are regarding the presented decision . in the empirical part of this paper , we show results achieved using a german corpus of amazon recensions and a set of machine_learning methods ( decision_trees and adaptive_boosting , gaussian_processes , random_forests , k-nearest_neighbor classification , support_vector_machines and artificial_neural_networks with evolutionary feature and parameter_optimization , and genetic_programming ) . using a heterogeneous model ensemble_learning approach that combines multi-class classifiers as well as binary classifiers , the classification_accuracy can be increased significantly and the ratio of totally wrongly classified samples ( i.e. , those that are assigned to the completely opposite sentiment_orientation ) can be decreased significantly .
title : improving transformer-based end-to-end speech_recognition with connectionist temporal classification and language model integration ; abstract : the state-of-the-art neural_network architecture named transformer has been used successfully for many sequence-to-sequence transformation tasks . the advantage of this architecture is that it has a fast iteration speed in the training stage because there is no sequential operation as with recurrent_neural_networks ( rnn ) . however , an rnn is still the best option for end-to-end automatic_speech_recognition ( asr ) tasks in terms of overall training_speed ( i.e. , convergence ) and word_error_rate ( wer ) because of effective joint_training and decoding methods . to realize a faster and more accurate asr system , we combine transformer and the advances in rnn-based asr . in our experiments , we found that the training of transformer is slower than that of rnn as regards the learning_curve and integration with the naive language model ( lm ) is difficult . to address these problems , we integrate connectionist temporal classification ( ctc ) with transformer for joint_training and decoding . this approach makes training faster than with rnns and assists lm integration . our proposed asr system realizes significant_improvements in various asr tasks . for example , it reduced the wers from 11.1 % to 4.5 % on the wall_street_journal and from 16.1 % to 11.6 % on the ted-lium by introducing ctc and lm integration into the transformer baseline .
title : sentiment weight of n-grams in dataset ( send ) : a feature-set for cross-domain_sentiment_classification ; abstract : generation of labeled reviews is a costly and time consuming effort . cross-domain classification reduces such effort by considering source and target dataset from two different domains . in this paper , we propose send- a lexicon-based feature_space for sentiment n-grams that performs well compared to the existing unigram and n-gram based features typically used in same domain analysis . the features are constructed by extracting intensifiers , negations and sentiment unigrams from the dataset . we find an importance-value for each feature by computing the product of 1 ) the number of times the feature appears in the review and 2 ) the logarithmic operation of its inverse frequency in the corpus . next , we calculate the sentiment_score of the n-grams by using the individual sentiment_scores of the unigrams and pre-calculated values of intensifiers , negations attached with it . these scores are multiplied with the correspondingfeature-importance- value to generate the final score of send features for each review . we experiment with maximum_entropy classifier on two benchmark_datasets for cross-domain classification problem and obtain a substantial_improvement in performance measure compared with state-of-the-art methods .
title : improving implicit_discourse_relation_classification by modeling inter-dependencies of discourse_units in a paragraph ; abstract : we argue that semantic meanings of a sentence or clause can not be interpreted independently from the rest of a paragraph , or independently from all discourse_relations and the overall paragraph-level discourse_structure . with the goal of improving implicit_discourse_relation_classification , we introduce a paragraph-level neural_networks that model inter-dependencies between discourse_units as well as discourse_relation continuity and patterns , and predict a sequence of discourse_relations in a paragraph . experimental results show that our model outperforms the previous state-of-the-art systems on the benchmark_corpus of pdtb .
title : conceptual_framework for stock_market classification model using sentiment_analysis on twitter based on hybrid naïve_bayes classifiers ; abstract : sentiment_analysis has gained a lot of importance in last decade especially on the availability of data from twitter that has created more interest for research in this field . nevertheless , stock_market classification models still suffer less accuracy and this has affected negatively the stock_market indicators . in this paper , a new framework related to sentiment_analysis from twitter posts is proposed . the proposed framework represents an improved design of classification model that works to improve the classification_accuracy to support decision_makers in the domain of stock_market exchange . this model starts with data collection part and in second phase filtration is done on data to get only the relevant data . the most important phase is the labelling part in which polarity of data is determined and negative , positive or neutral values are assigned to statements of people . the fourth part is the classification phase in which suitable patterns of stock_market will be identified by hybridizing nbcs . the last phase is performance and evaluation . this study proposes to a hybrid naïve_bayes classifiers ( hnbcs ) as a machine_learning method for stock_market classification , hence represents a useful study for investors , companies and researchers and will help them to formulate their policies according to sentiments of people .
title : employing hierarchical bayesian networks in simple and complex emotion topic analysis ; abstract : traditional emotion models , when tagging single emotions in documents , often ignore the fact that most documents convey complex human emotions . in this paper , we join emotion analysis with topic_models to find complex emotions in documents , as well as the intensity of the emotions , and study how the document emotions vary with topics . hierarchical bayesian networks are employed to generate the latent_topic variables and emotion variables . on average , our model on single emotion classification outperforms the traditional supervised_machine_learning models such as svm and naive_bayes . the other model on the complex emotion classification also achieves promising_results . we thoroughly analyze the impact of vocabulary quality and topic quantity to emotion and intensity prediction in our experiments . the distribution of topics such as friend and job are found to be sensitive to the documents ' emotions , which we call emotion topic variation in this paper . this reveals the deeper relationship between topics and emotions . © 2012 elsevier ltd. all rights_reserved .
title : chinese text_categorization based on fuzzy association_rules ; abstract : with the rapid evolution of internet and the broad_application of computer technology , the number of electronic texts increases quickly . classifying these texts effectively becomes more and more important . this paper proposes a novel approach for automatic chinese text_categorization . by introducing fuzzy_sets concept , it improves the existing categorization approaches based on association_rules . the mined fuzzy association_rules are used as a classifier to classify chinese texts . experimental results show that the new approach is feasible and the categorization effectiveness is enhanced . © 2006 ieee .
title : emotion classification and intensity prediction using hybrid deep_learning models ; abstract : the study of people 's feelings , emotions , and opinions toward a specific event or issue is known as sentiment_analysis . individuals can utilize social_media to share their views and opinions on any subject or issue , resulting in massive_amounts of unstructured_data . by processing and evaluating these feelings , you can get insights and understand them . multiple machine_learning ( ml ) based approaches were used to study various points of view . due to changes in the sequential order , stream lengths , and advanced logic , determining the exact thoughts expressed in the consumer comments is still difficult . this paper proposed the emotion analysis model by vectorizing all the training_data and creating hybrid models with the conv 1d model with the lstm model . the proposed outcomes show outstanding_performance as compared to the other base models .
title : categorization of patient diseases for chinese electronic_health_record analysis : a case_study ; abstract : the electronic_health_record ( ehr ) analysis has become an increasingly important landing area for machine_learning and text_mining algorithms to leverage the full potential of the big_data for improving human health_care . in a lot of our chinese ehr analysis applications , it is very important to categorize the patients ’ diseases according to the chinese national medical coding standard . in this paper , we develop nlp and machine_learning algorithms to automatically categorize each patient ’ s diseases into one or more categories . we take each patient ’ s disease description as a document . also , for each disease category , we make use of its description information in the medical coding standard and take it as a document . according to the characteristics of our data , we define the categorization problem as the unsupervised classification problem with the nearest neighborhood ( nn ) algorithm using different vector representations to represent the documents . experimental results show that the averaged word_embeddings of word2vec works best with very promising classification performance .
title : an approach to assess the existence of a proposed intervention in essay-argumentative texts ; abstract : this paper presents an approach for grading essays based on the presence of one or more theses , arguments , and intervention proposals . the research was developed by means of the following steps : ( i ) corpus delimitation and annotation ; ( ii ) features selection ; ( iii ) extraction of the training corpus , and ( iv ) class balancing , training and testing . our study shows that features related to argumentation_mining can improve the automatic essay scoring performance compared to the set of usual features . the main_contribution of this paper is to demonstrate that argument marking procedures to improve score prediction in essays classification can produce better results . moreover , it remained clear that essays classification does not depends on the number of features but rather on the ability of creating meaningful features for a given domain .
title : a comprehensive comparative study on term_weighting_schemes for text_categorization with support_vector_machines ; abstract : term_weighting_scheme , which has been used to convert the documents as vectors in the term space , is a vital step in automatic_text_categorization . in this paper , we conducted comprehensive experiments to compare various term_weighting_schemes with svm on two widely-used benchmark_data_sets . we also presented a new term_weighting_scheme tf-rf to improve the term 's discriminating_power . the controlled experimental results showed that this newly_proposed tf-rf scheme is significantly better than other widely-used term_weighting_schemes . compared with schemes related with tf factor alone , the idf factor does not improve or even decrease the term 's discriminating_power for text_categorization .
title : improving textual emotion recognition based on intra- and inter-class variation ; abstract : textual emotion recognition ( ter ) is an important task in natural_language_processing ( nlp ) , due to its high impact in real-world applications . prior_research has tackled the automatic classification of emotion expressions in text by maximising the probability of the correct emotion class using cross-entropy_loss . however , this approach does not account for intra- and inter-class variations within and between emotion classes . to overcome this problem , we introduce a variant of triplet centre loss as an auxiliary task to emotion classification . this allows ter models to learn compact and discriminative_features . furthermore , we introduce a method for evaluating the impact of intra- and inter-class variations on each emotion class . experiments performed on three data sets demonstrate the effectiveness of our method when applied to each emotion class in comparison to previous_approaches . finally , we present analyses that illustrate the benefits of our method in terms of improving the prediction scores as well as producing discriminative_features .
title : automatic detection and classification of cognitive distortions in mental_health text ; abstract : in cognitive_psychology , automatic and self-reinforcing irrational thought patterns are known as cognitive distortions . left unchecked , patients exhibiting these types of thoughts can become stuck in negative_feedback loops of unhealthy thinking , leading to inaccurate perceptions of reality commonly associated with anxiety and depression . in this paper , we present a machine_learning framework for the automatic detection and classification of 15 common cognitive distortions in two novel mental_health free text datasets collected from both crowdsourcing and a real-world online therapy program . when differentiating between distorted and non-distorted passages , our model achieved a weighted_f1_score of 0.88 . for classifying distorted passages into one of 15 distortion categories , our model yielded weighted f1 scores of 0.68 in the larger crowdsourced dataset and 0.45 in the smaller online counseling dataset , both of which outperformed random baseline metrics by a large_margin . for both tasks , we also identified the most discriminative words and phrases between classes to highlight common thematic elements for improving targeted and therapist-guided mental_health treatment . furthermore , we performed an exploratory_analysis using unsupervised content-based clustering and topic_modeling algorithms as first efforts towards a data-driven perspective on the thematic relationship between similar cognitive distortions traditionally deemed unique . finally , we highlight the difficulties in applying mental_health-based machine_learning in a real-world setting and comment on the implications and benefits of our framework for improving automated delivery of therapeutic treatment in conjunction with traditional cognitive-behavioral therapy .
title : fuzzy string_matching with a deep_neural_network ; abstract : a deep_learning neural_network for character-level text_classification is described in this work . the system spots keywords in the text output of an optical_character_recognition system using memoization and by encoding the text into feature_vectors related to letter frequency . recognizing error messages in a set of generated images , dictionary and spell-check-based approaches achieved 69 % to 88 % accuracy , while various deep_learning approaches achieved 91 % to 96 % accuracy , and a combination of deep_learning with a dictionary achieved 97 % accuracy . the contribution of this work to the state of the art is to describe a new approach for character-level deep_neural_network classification of noisy text .
title : short-text representation using diffusion wavelets ; abstract : usual text document_representations such as tf-idf do not work well in classification_tasks for short-text documents and across diverse data domains . optimizing different representations for different data domains is infeasible in a practical setting on the internet . mining such representations from the data in an unsupervised manner is desirable . in this paper , we study a representation based on the multi-scale harmonic analysis of term-term co-occurrence graph . this representation is not only sparse , but also leads to the discovery of semantically coherent topics in data . in our experiments on user-generated short documents e.g. , newsgroup messages , user_comments , and meta-data , we found this representation to outperform other representations across different choice of classifiers . similar improvements were also observed for data sets in chinese and portuguese languages .
title : new sharpness features for image type classification based on textual_information ; abstract : achieving good recognition results from a single method for text_lines in video/natural_scene_images captured by high resolution cameras or low_resolution mobile cameras , and images in web_pages , is often hard . in this paper , we propose new sharpness based features of textual portion of each input text_line image using hsi color_space for the classification of an input image into one of the four classes ( video , scene , mobile or born_digital ) . this helps in choosing an appropriate method based on the class type of the input text for its improved recognition_rate . for a given input text_line image , the proposed method obtains h , s and i images . then canny_edge images are obtained for h , s and i spaces , which results in text_candidates . we perform sliding_window operation over the text candidate image of each text_line of each color_space to estimate new sharpness by calculating stroke width and gradient information . the sharpness values of the text_lines of the three color spaces are then fed to k-means_clustering with maximum , minimum and average guesses , which results in three respective clusters . the mean of each cluster for respective color spaces outputs a feature_vector having nine feature_values for image_classification with the help of an svm classifier . experimental results on standard_datasets , namely , icdar_2013 , icdar 2015 video , icdar 2015 natural_scene data , icdar_2013 born_digital data and the images captured by a mobile camera ( our own data ) show that the proposed classification method helps in improving recognition results .
title : a supervised_machine_learning approach of extracting coexpression relationship among genes from literature ; abstract : it is vital to develop automatic information_extraction systems to help researchers cope up with the vast amount of data available on the internet . in this paper , we describe a framework to extract precise information about coexpression relationship among genes , from published_literature using a supervised_machine_learning approach . we use a graphical_model , dynamic conditional_random_fields ( dcrfs ) , for training our classifier . our approach is based on semantic analysis of text to classify the predicates describing coexpression relationship rather than detecting the presence of keywords . we compared our results of sentence_classification with the baseline technique of word matching and a naïve_bayes classification algorithm . our framework outperformed the baseline by almost 45 % , with dcrfs showing superior performance to naïve_bayes . © 2010 ieee .
title : machine_learning or lexicon based sentiment_analysis techniques on social_media posts ; abstract : social_media provides an accessible and effective platform for individuals to offer thoughts and opinions across a wide range of interest areas . it also provides a great opportunity for researchers and businesses to understand and analyse a large_volume of online data for decision-making purposes . opinions on social_media platforms , such as twitter , can be very important for many industries due to the wide variety of topics and large_volume of data available . however , extracting and analysing this data can prove to be very challenging due to its diversity and complexity . recent methods of sentiment_analysis of social_media content rely on natural_language_processing techniques on a fundamental sentiment_lexicon , as well as machine_learning oriented techniques . in this work , we evaluate representatives of different sentiment_analysis methods , make recommendations and discuss advantages and disadvantages . specifically we look into : 1 ) variation of vader , a lexicon based method ; 2 ) a machine_learning neural_network based method ; and 3 ) a sentiment classifier using word_sense_disambiguation , maximum_entropy and naive_bayes classifiers . the results indicate that there is a significant correlation among all three sentiment_analysis methods , which demonstrates their ability to accurately determine the sentiment of social_media posts . additionally , the modified version of vader , a lexicon based method , is considered to be the most accurate and most appropriate method to use for the semantic analysis of social_media posts , based on its strong correlation and low computational time . obtained findings and recommendations can be valuable for researchers working on sentiment_analysis techniques for large data sets .
title : an extensive empirical_study of feature_selection metrics for text_classification ; abstract : machine_learning for text_classification is the cornerstone of document_categorization , news filtering , document routing , and personalization . in text domains , effective feature_selection is essential to make the learning task efficient and more accurate . this paper presents an empirical comparison of twelve feature_selection methods ( e.g . information gain ) evaluated on a benchmark of 229 text_classification problem instances that were gathered from reuters , trec , ohsumed , etc . the results are analyzed from multiple goal perspectives-accuracy , f-measure , precision , and recall-since each is appropriate in different situations . the results reveal that a new feature_selection metric we call 'bi-normal separation ' ( bns ) , outperformed the others by a substantial margin in most situations . this margin widened in tasks with high class skew , which is rampant in text_classification problems and is particularly challenging for induction algorithms . a new evaluation methodology is offered that focuses on the needs of the data_mining practitioner faced with a single dataset who seeks to choose one ( or a pair of ) metrics that are most likely to yield the best performance . from this perspective , bns was the top single choice for all goals except precision , for which information gain yielded the best result most often . this analysis also revealed , for example , that information gain and chi-squared have correlated failures , and so they work poorly together . when choosing optimal pairs of metrics for each of the four performance goals , bns is consistently a member of the pair-e.g. , for greatest recall , the pair bns + f1-measure yielded the best performance on the greatest number of tasks by a considerable margin . © 2003 hewlett-packard .
title : classification of sentiments using predictive analysis over social_network : a review ; abstract : sentiment is the way of representing our self in front of other so that other can respond accordingly or oppose it . the process of defining the emotion or opinion of a part of data is called sentiment_analysis . the main reason for sentiment_analysis is to categorize an author 's attitude toward numerous subjects into positive , terrible or impartial classes . however not limited to , commercial enterprise intelligence , politics , sociology , etc .. predictive analysis is required to predict the future for the validity of content on social_network . the sentiments of the people can be collected from social_networking_websites , micro_blogs , wikis and web packages etc .. in the proposed work predictive analysis techniques are investigated and compared on the ground of performance . it is observed that most of the high performing techniques are based on neural_network machine_learning algorithm .
title : empirical_study of the model generalization for argument_mining in cross-domain and cross-topic settings ; abstract : to date , the number of studies that address the generalization of argument models is still relatively small . in this study , we extend our stacking model from argument_identification to an argument_unit classification_task . using this model , and for each of the learned tasks , we address three real-world scenarios concerning the model robustness over multiple datasets , different domains and topics . consequently , we first compare single-datset learning ( sdl ) with multi-dataset learning ( mdl ) . second , we examine the model generalization over completely unseen dataset in our cross-domain experiments . third , we study the effect of sample and topic sizes on the model performance in our cross-topic experiments . we conclude that , in most cases , the ensemble_learning stacking approach is more stable over the generalization tests than a transfer_learning distilbert model . in addition , the argument_identification task seems to be easier to generalize across shifted domains than argument_unit classification . this work aims at filling the gap between computational argumentation and applied machine_learning with regard to the model generalization .
title : a new term_weighting_method by introducing class information for sentiment_classification of textual_data ; abstract : with the popularity of text based communication tools such as blogs , plurk , twitter , and so on , customers can easily express their opinions , reviews or comments about purchased products/services . these personal opinions , especially negative comments , might have a significant influence on other consumers ' purchasing_decisions . therefore , how to detect users ' sentiment from textual_data to assist companies to carefully respond to customers ' comments has become a crucial task . recently , machine_learning methods have been considered as one of solutions in sentiment_classification . when applying machine_learning approaches to classify sentiment , term_frequency ( tf ) , term presence ( tp ) and term_frequency-inverse_document_frequency ( tf-idf ) usually have been employed to describe collected textual_data . however , these traditional term_weighting methods can not have positive influence on improving classification performance . therefore , this work proposes a new term_weighting_method called categorical difference weights ( cdw ) by introducing class information . besides , cdw will be integrated into support_vector_machines ( svm ) . finally , an actual case will be provided to illustrate the effectiveness of our proposed method . compared with traditional term_weighting methods , tf and tf-idf , experimental results indicated that the proposed cdw method indeed can improve the classification performance .
title : an automatic classification system of online e-learning resources ; abstract : this work focuses on how to development a categorizing system that can find the appropriate classification for on-line learning resources , which must follow some document standards , automatically . the proposed system can help users to share their creativities without the dilemma of following the standards of learning resources . the major works are learning resource classification and learning resource document matching . learning resource classification module takes in charge of finding corresponding keywords of learning resources in the document database of the e-learning web_site . the similarity of users ' uploading resources and documents in the web_site document database is done by learning resource document matching . the proposed automatic categorizing system for online learning resources can suggest the appropriate classification and learning object standard information for users ' uploading resources after performing these modules . © 2012 ieee .
title : semi-supervised text_classification via self-pretraining ; abstract : we present a neural semi-supervised_learning model termed self-pretraining . our model is inspired by the classic self-training algorithm . however , as opposed to self-training , self-pretraining is threshold-free , it can potentially update its belief about previously labeled_documents , and can cope with the semantic drift problem . self-pretraining is iterative and consists of two classifiers . in each iteration , one classifier draws a random set of unlabeled_documents and labels them . this set is used to initialize the second classifier , to be further trained by the set of labeled_documents . the algorithm proceeds to the next iteration and the classifiers ' roles are reversed . to improve the flow of information across the iterations and also to cope with the semantic drift problem , self-pretraining employs an iterative distillation process , transfers hypotheses across the iterations , utilizes a two-stage training model , uses an efficient learning_rate schedule , and employs a pseudo-label transformation heuristic . we have evaluated our model in three publicly available social_media datasets . our experiments show that self-pretraining outperforms the existing state-of-the-art semi-supervised classifiers across multiple settings . our code is available at https : //github.com/p-karisani/self_pretraining .
title : fairness and robustness in invariant learning : a case_study in toxicity classification ; abstract : robustness is of central importance in machine_learning and has given rise to the fields of domain_generalization and invariant learning , which are concerned with improving performance on a test distribution distinct from but related to the training distribution . in light of recent work suggesting an intimate connection between fairness and robustness , we investigate whether algorithms from robust ml can be used to improve the fairness of classifiers that are trained on biased data and tested on unbiased data . we apply invariant risk minimization ( irm ) , a domain_generalization algorithm that employs a causal discovery inspired method to find robust predictors , to the task of fairly predicting the toxicity of internet comments . we show that irm achieves better out-of-distribution accuracy and fairness than empirical risk minimization ( erm ) methods , and analyze both the difficulties that arise when applying irm in practice and the conditions under which irm will likely be effective in this scenario . we hope that this work will inspire further studies of how robust machine_learning methods relate to algorithmic fairness .
title : multi-label_classification of bills from the italian_senate ; abstract : the classification of legal_texts is usually carried out by domain_experts in force at institutions . the classification process is very complex because the reference thesauri are very rich , both in terms of variety of concepts and in terms of numbers . in addition , they often contain very rarely used labels . in this paper we show how to implement a machine_learning system that can support the domain_experts of the italian_senate , handling infrequently used labels ( zero/few-shot classification ) and making the output of the model explainable to humans .
title : a model text recommendation system for engaging english_language learners : facilitating selections on cefr ; abstract : a pedagogically informed multimodal education system is defined by how well reading tasks are assigned to students in a contemporary classroom . a source that becomes a provider of readings is the web , where it is possible to find information on practically all areas of knowledge and in a wide variety of languages . however , selecting the appropriate material for the level and theme becomes a tedious job to which language teachers must devote a significant amount of their time . selecting suitable readings to accompany the teaching-learning process is thus not a ‘ trivial ’ task . basic-level texts for language competence are easy to recognize and obtain but as is seen in case of the common_european_framework_of_reference_for_languages recommendations ( cefr ) , selection of appropriate texts that impart language competencies , especially of vocabulary and grammar at higher levels of communicativeness , selection becomes increasingly complex for teachers . furthermore , the suggested readings should be raked by complexity in accordance with student capabilities . we suggest , that automatic classifiers based on cefr levels may help in this process of selections from the already available corpora of authentic texts on the web . the existing facility of access of readers to such material on the web may come to the aid of automated classifiers . teachers use interest to motivate reading in classrooms , but automatic recommendation_systems will allow specific or even individualized recommendations . the authors explore the impact of such multimodal methods on the acquisition of better linguistic and communicative skills .
title : auto-classification of government department-specific news articles ; abstract : the purpose of this study is to propose an unsupervised_learning-based method for automatic classification of news articles using a dictionary , incorporating the attributes of each administrative department . the results of auto-classification of news articles for individual departments showed 71 % accuracy . a classification technique using unsupervised_learning may be utilized to automatically_classify documents without labels when gathering policy issues for individual administrative departments .
title : hierarchical classification for spoken arabic dialect identification using prosody : case of algerian dialects ; abstract : in daily communications , arabs use local dialects which are hard to identify automatically using conventional classification methods . the dialect identification challenging task becomes more complicated when dealing with an under-resourced dialects belonging to a same county/region . in this paper , we start by analyzing statistically algerian dialects in order to capture their specificities related to prosody information which are extracted at utterance_level after a coarse-grained consonant/vowel segmentation . according to these analysis findings , we propose a hierarchical classification approach for spoken arabic algerian dialect identification ( hadid ) . it takes advantage from the fact that dialects have an inherent property of naturally structured into hierarchy . within hadid , a top-down hierarchical classification is applied , in which we use deep_neural_networks ( dnns ) method to build a local classifier for every parent node into the hierarchy dialect structure . our framework is implemented and evaluated on algerian_arabic dialects corpus . whereas , the hierarchy dialect structure is deduced from historic and linguistic knowledges . the results reveal that within { \hd } , the best classifier is dnns compared to support_vector_machine . in addition , compared with a baseline flat_classification system , our hadid gives an improvement of 63.5 % in term of precision . furthermore , overall results evidence the suitability of our prosody-based hadid for speaker independent dialect identification while requiring less than 6s test utterances .
title : classifying patient_portal messages using convolutional_neural_networks ; abstract : objective patients communicate with healthcare providers via secure messaging in patient portals . as patient_portal adoption increases , growing messaging volumes may overwhelm providers . prior_research has demonstrated promise in automating classification of patient_portal messages into communication types to support message triage or answering . this paper examines if using semantic features and word context improves portal message classification . materials and methods portal_messages were classified into the following categories : informational , medical , social , and logistical . we constructed features from portal_messages including bag of words , bag of phrases , graph representations , and word_embeddings . we trained one-versus-all random_forest and logistic_regression classifiers , and convolutional_neural_network ( cnn ) with a softmax output . we evaluated each classifier 's performance using area under the curve ( auc ) . results representing the messages using bag of words , the random_forest detected informational , medical , social , and logistical communications in patient_portal messages with aucs : 0.803 , 0.884 , 0.828 , and 0.928 , respectively . graph representations of messages outperformed simpler features with aucs : 0.837 , 0.914 , 0.846 , 0.884 for informational , medical , social , and logistical communication , respectively . representing words with word2vec_embeddings , and mapping features using a cnn had the best performance with aucs : 0.908 for informational , 0.917 for medical , 0.935 for social , and 0.943 for logistical categories . discussion and conclusion word2vec and graph representations improved the accuracy of classifying portal_messages compared to features that lacked semantic information such as bag of words , and bag of phrases . furthermore , using word2vec along with a cnn model , which provide a higher_order representation , improved the classification of portal_messages .
title : stochastic tokenization with a language model for neural text_classification ; abstract : for unsegmented languages such as japanese and chinese , tokenization of a sentence has a significant_impact on the performance of text_classification . sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word ( or subword ) representations for neural_networks . however , segmentation is potentially ambiguous , and it is unclear whether the segmented tokens achieve the best performance for the target task . in this paper , we propose a method to simultaneously learn tokenization and text_classification to address these problems . our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously . to make the model robust against infrequent tokens , we sampled segmentation for each sentence stochastically during training , which resulted in improved performance of text_classification . we conducted experiments on sentiment_analysis as a text_classification task and show that our method_achieves better performance than previous methods .
title : a novel approach for classifying gene_expression data using topic_modeling ; abstract : understanding the role of differential gene_expression in cancer etiology and cellular process is a complex problem that continues to pose a challenge due to sheer number of genes and inter-related biological_processes involved . in this paper , we employ an unsupervised topic_model , latent_dirichlet_allocation ( lda ) to mitigate overfitting of high-dimensionality gene_expression data and to facilitate understanding of the associated pathways . lda has been recently applied for clustering and exploring genomic data but not for classification and prediction . here , we proposed to use lda in clustering as well as in classification of cancer and healthy tissues using lung_cancer and breast_cancer messenger_rna ( mrna ) sequencing data . we describe our study in three phases : clustering , classification , and gene interpretation . first , lda is used as a clustering_algorithm to group the data in an unsupervised manner . next we developed a novel lda-based classification approach to classify unknown samples based on similarity of co-expression patterns . evaluation to assess the effectiveness of this approach shows that lda can achieve high accuracy compared to alternative approaches . lastly , we present a functional_analysis of the genes identified using a novel topic profile matrix formulation . this analysis identified several genes and pathways that could potentially be involved in differentiating tumor samples from normal . overall , our results project lda as a promising approach for classification of tissue types based on gene_expression data in cancer studies .
title : icd code retrieval : novel approach for assisted disease classification ; abstract : the task of assigning classification codes to short medical text is a hard text_classification problem , especially when the set of possible codes is as big as the icd-9-cm set . the problem , which has been only partially tamed for a subset of icd-9-cm , becomes even harder in real_world applications , where the labeled_data are scarce and noisy . in this paper we first show the ineffectivenesss of current text_classification algorithms on large_datasets , then we present a novel incremental approach to clinical_text classification , which overcomes the low accuracy problem through the top-k retrieval , exploits transfer_learning techniques in order to expand a skewed dataset and improves the overall accuracy over time , learning from user selection .
title : fake_news spreaders detection : sometimes attention is not all you need ; abstract : guided by a corpus_linguistics approach , in this article we present a comparative evaluation of state-of-the-art ( sota ) models , with a special focus on transformers , to address the task of fake_news spreaders ( i.e. , users that share fake_news ) detection . first , we explore the reference multilingual dataset for the considered task , exploiting corpus_linguistics techniques , such as chi-square_test , keywords and word sketch . second , we perform experiments on several models for natural_language_processing . third , we perform a comparative evaluation using the most recent transformer-based models ( roberta , distilbert , bert , xlnet , electra , longformer ) and other deep and non-deep sota models ( cnn , multicnn , bayes , svm ) . the cnn tested outperforms all the models tested and , to the best of our knowledge , any existing approach on the same dataset . fourth , to better understand this result , we conduct a post-hoc analysis as an attempt to investigate the behaviour of the presented best performing black-box model . this study highlights the importance of choosing a suitable classifier given the specific task . to make an educated decision , we propose the use of corpus_linguistics techniques . our results suggest that large pre-trained deep models like transformers are not necessarily the first choice when addressing a text_classification task as the one presented in this article . all the code developed to run our tests is publicly available on github .
title : relation_extraction between medical entities using deep_learning approach ; abstract : medical discharge_summaries or patient prescriptions contain a variety of medical terms . the semantic relation_extraction between medical terms is essential for the discovery of significant medical knowledge . the relation_classification is one of the imperative tasks of biomedical information_extraction . the automatic identification of relations between medical diseases , tests , and treatments can improve the quality of patient_care . this paper presents the deep_learning based proposed system for relation_extraction between medical entities . in this paper , a convolution neural_network is used for relation_classification . the system is divided into four modules : word_embedding , feature_extraction , convolution , and softmax_classifier . the output contains classified relations between medical entities . in this work , data set provided by i2b2 2010 challenge is used for relation detection which consisted of total 9070 relations in test data and 5262 total relations in the train dataset . the performance evaluation of relation_extraction task is done using precision and recall . the system achieved an average of 75 % precision and 72 % recall . the performance of the system is compared with the awarded i2b2 participated systems .
title : a novel approach to document_classification using wordnet ; abstract : content based document_classification is one of the biggest challenges in the context of free text_mining . current algorithms on document classifications mostly rely on cluster_analysis based on bag-of-words approach . however that method is still being applied to many modern scientific dilemmas . it has established a strong presence in fields like economics and social_science to merit serious attention from the researchers . in this paper we would like to propose and explore an alternative grounded more securely on the dictionary classification and correlatedness of words and phrases . it is expected that application of our existing knowledge about the underlying classification structure may lead to improvement of the classifier 's performance .
title : heuristic approach towards covid-19 : big_data analytics and classification with natural_language_processing ; abstract : data has tremendously incorporated our lifestyle . with advancements in technology and reduced internet cost , data usage has increased many folds resulting in generation of huge heaps of unstructured_data called as big_data . this unstructured big_data is difficult to handle using existing database_management technology . we observed that genetic information related to coronavirus is tremendously increasing everyday . with implementation of big_data analytics , these databases will be easily manageable leading to advancements in covid-19 research . in this article , we have used hdfs system for efficient data_management . in our work , we classified gene classes present in complete sequence so as to quickly detect mutation in no time . to achieve this , we predicted machine_learning models to classify gene sequences faster in-class with libraries like matplotlib to construct detailed graph of the data . we choose three different sequences to classify gene sequence using natural_language_processing technique of sklearn library and tested our results using logical regression .
title : multi-class twitter sentiment_classification with emojis ; abstract : purpose : recently , various twitter sentiment_analysis ( tsa ) techniques have been developed , but little has paid attention to the microblogging feature – emojis , and few works have been conducted on the multi-class sentiment_analysis of tweets . the purpose of this paper is to consider the popularity of emojis on twitter and investigate the feasibility of an emoji training heuristic for multi-class sentiment_classification of tweets . tweets from the “ 2016 orlando nightclub shooting ” were used as a source of study . besides , this study also aims to demonstrate how mapping can contribute to interpreting sentiments . design/methodology/approach : the authors presented a methodological framework to collect , pre-process , analyse and map public twitter postings related to the shooting . the authors designed and implemented an emoji training heuristic , which automatically prepares the training_data set , a feature needed in big_data research . the authors improved upon the previous framework by advancing the pre-processing techniques , enhancing feature_engineering and optimising the classification models . the authors constructed the sentiment model with a logistic_regression classifier and selected features . finally , the authors presented how to visualise citizen sentiments on maps dynamically using mapbox . findings : the sentiment model constructed with the automatically annotated training_sets using an emoji approach and selected features performs well in classifying tweets into five different sentiment classes , with a macro-averaged f-measure of 0.635 , a macro-averaged accuracy of 0.689 and the maem of 0.530 . compared to those experimental results in related_works , the results are satisfactory , indicating the model is effective and the proposed emoji training heuristic is useful and feasible in multi-class tsa . the maps authors created , provide a much easier-to-understand visual representation of the data , and make it more efficient to monitor citizen sentiments and distributions . originality/value : this work appears to be the first to conduct multi-class sentiment_classification on twitter with automatic annotation of training_sets using emojis . little attention has been paid to applying tsa to monitor the public ’ s attitudes towards terror attacks and country ’ s gun policies , the authors consider this work to be a pioneering work . besides , the authors have introduced a new data set of 2016 orlando shooting tweets , which will be made available for other researchers to mine the public ’ s political opinions about gun policies .
title : a sentiment_polarity prediction model using transfer_learning and its application to sns flaming event_detection ; abstract : in recent_years , with the popularization of sns , the incidents called flaming , in which a large number of negative comments are retweeted and spread to many followers on sns , are increasing . since a flaming event sometimes causes severe criticism by public people , it is becoming a great thread to companies and therefore it is important for companies to protect their reputation from such flaming events . in order to protect companies from serious damages in reputation , we propose a machine_learning approach to the detection of flaming events by monitoring the sentiment_polarity of sns comments . from the nature of sns comments such as the spread of a large number of retweets with the same content for a short time , the word_distributions are often strongly biased and it leads to poor_performance in sentiment_polarity prediction . to alleviate this problem , we introduce transfer_learning into the conventional naive_bayes_classifier . more concretely , in the naive_bayes_classifier , the occurrence probabilities of words on a target domain are recalculated using those on other domains , where a domain corresponds to a company to be protected . the experimental results demonstrate that the proposed transfer_learning contribute to the improvement in the sentiment_polarity prediction for sns comments . in addition , we show that the proposed system can detect flaming events correctly by monitoring the number of negative comments .
title : crypto-currency sentiment analyse on social_media ; abstract : the amount of data produced with the introduction of social_media tools has reached gigantic size . in analyzing data , and now traditional_methods are no longer enough , the concept of 'big_data ' has entered our lives . there is an inevitable need to produce meaningful summaries by analyzing data at huge size . large data tools are used to meet this need . in order to have knowledge about human behaviors using these tools and to develop solutions in this direction , social_media and especially twitter data are being studied . as is known , the concept of crypto currency , which has come from trends in recent_years , attracts more and more people . in this study , approaches to the most commonly used crypto_currencies were examined for the first time using large data tools . the meaningless data on twits which is taken from twitter has been cleared and the approaches of users against crypto paralysis have been analyzed by using various classifiers and the results have been shown .
title : graph classification system using normalised graph_convolutional_networks ; abstract : recent_years have authenticated a dramatic increase in graph applications due to their advancements in their informative and social connectivity . in large-scale networks , graph data contains huge information and exhibits distinct characteristics . traditional graph_convolutional_networks can not handle the problem of covariate-shift in the neural-networks during analysing the patterns . this research presents an intelligence-based graph-classification model for citation_networks by using normalised graph convolution networks to handle covariate-shift problem and provides greater regularisation with a decrement in loss to a greater extent . this problem is vanquished by utilising the normalisation constraint for the individual batch constructed as per network developed for graph-structures which eventually lead to building a robust model . the model classifies the underlying connectivity patterns of the structural and informative relationships with appropriate embeddings . this information is fed to neural-network and learns hidden_layer representation that encode both local graphs and features of the nodes in the citation_network .
title : classifying short texts for a social mediam monitoring system ; abstract : we present the system for the classification of sentences and short texts into marketing mix classes developed within the lps-bigger project . the system classifies short texts from social_media into categories that are considered business indicators to monitor consumer 's opinion .
title : performance evaluation of analytics models for trends analysis of news ; abstract : microblogging services , especially twitter , allow the user to share their most recent thoughts , feelings or news freely and almost immediately . hence , the number of news tweets generated by the news_media is increasing_exponentially . mining the valuable data from the large_volume of tweets can help increase the revenue of organisations by allowing them to engage with the public faster and better by responding to the latest topics of interest . in this work , mining the hot keywords and being able in classifying the news tweets , trending topic and keywords in the news tweets . both supervised and unsupervised machine_learning models are used . several machine_learning algorithms are being used to compare the accuracy in classifying the tweets .
title : fake_news classification of social_media through sentiment_analysis ; abstract : the impacts of the internet and the ability for information to flow in real-time to all corners of the globe has brought many benefits to society . however , this capability has downsides . information can be inexact , misleading or indeed downright and deliberately false . fake_news has now entered the common vernacular . in this work , we consider fake_news with specific regard to social_media . we hypothesise that fake_news typically deals with emotive topics that are deliberated targeted to cause a reaction and encourages the spread of information . as such , we explore sentiment_analysis of real and fake_news as reported in social_networks ( twitter ) . specifically , we develop an aws-based cloud platform utilising news contained in the untrustworthy resource fakenewsnet and a more trusted resource credbank . we train algorithms using naive_bayes , decision_tree and bi-lstm for sentiment_classification and feature_selection . we show how social_media sentiment can be used to improve the accuracy in identification of fake_news from real_news .
title : election result prediction using twitter sentiment_analysis ; abstract : the proliferation of social_media in the recent past has provided end_users a powerful platform to voice their opinions . businesses ( or similar entities ) need to identify the polarity of these opinions in order to understand user orientation and thereby make smarter decisions . one such application is in the field of politics , where political entities need to understand public opinion and thus determine their campaigning strategy . sentiment_analysis on social_media data has been seen by many as an effective tool to monitor user_preferences and inclination . popular text_classification algorithms like naive_bayes and svm are supervised_learning algorithms which require a training_data set to perform sentiment_analysis . the accuracy of these algorithms is contingent upon the quantity as well as the quality ( features and contextual relevance ) of the labeled_training_data . since most applications suffer from lack of training_data , they resort to cross_domain sentiment_analysis which misses out on features relevant to the target data . this , in turn , takes a toll on the overall accuracy of text_classification . in this paper , we propose a two stage framework which can be used to create a training_data from the mined twitter data without compromising on features and contextual relevance . finally , we propose a scalable machine_learning model to predict the election results using our two stage framework .
title : arabic light-based stemming : a comparative study among ligh10 stemmer , p-stemmer , and conditional light stemmer ; abstract : arabic stemming is a key stage in natural_language_processing 's preprocessing ( nlp ) . it takes affixes out of words . it improves text_classification ( tc ) as well as information_retrieval ( ir ) . light-based stemming and root-based stemming are the two types of stem . when compared to root-based stemming , light-based stemming consumes more energy . only suffixes and prefixes are removed from the words . the light10 stemmer , the p-stemmer , and conditional light stemming ( condlight ) are three well-known methods of light stemming . prefixes and suffixes are removed by light10 stemmers under a few conditions . only prefixes are removed by the p-stemmer , while the condlight stemmer is the same as the light10 stemmer but with eight conditions . we measured the extent of improvement in arabic tc by evaluating the stemmers . three classifiers employ the support_vector_machine ( svm ) , the k-nearest_neighbor algorithm ( knn ) , nave bays ( nb ) , and statistical similarity_measurement . with stemming , the outcome indicates a small improvement ( about 2 percent improvement ) .
title : learning representations for vietnamese sentence_classification ( extended abstract ) ; abstract : in this study , we propose a new deep language model that taking the advantage of transformer model towards the task of vietnamese sentence_classification . we construct a new vietnamese dataset for evaluating the model . we also conduct_experiments on english corpora to evaluate our proposed model .
title : foodsis : a text_mining system to improve the state of food_safety in singapore ; abstract : food_safety is an important health issue in singapore as the number of food_poisoning cases have increased significantly over the past few decades . the national_environment_agency of singapore ( nea ) is the primary government_agency responsible for monitoring and mitigating the food_safety risks . in an effort to pro-actively monitor emerging food_safety issues and to stay abreast with developments related to food_safety in the world , nea tracks the world_wide_web as a source of news feeds to identify food_safety related articles . however , such information gathering is a difficult and time consuming process due to information_overload . in this paper , we present foodsis , a system for end-to-end web information gathering for food_safety . foodsis improves efficiency of such focused information gathering process with the use of machine_learning techniques to identify and rank relevant content . we discuss the challenges in building such a system and describe how thoughtful system design and recent advances in machine_learning provide a framework that synthesizes interactive learning with classification to provide a system that is used in daily operations . we conduct_experiments and demonstrate that our classification approach results in improving the efficiency by average 35 % compared to a conventional approach and the ranking approach leads to average 16 % improvement in elevating the ranks of relevant_articles . © 2014 acm .
title : seeing both the forest and the trees : multi-head_attention for joint classification on different compositional levels ; abstract : in natural_languages , words are used in association to construct sentences . it is not words in isolation , but the appropriate combination of hierarchical_structures that conveys the meaning of the whole sentence . neural_networks can capture expressive language features ; however , insights into the link between words and sentences are difficult to acquire automatically . in this work , we design a deep_neural_network architecture that explicitly wires lower and higher linguistic components ; we then evaluate its ability to perform the same task at different hierarchical_levels . settling on broad text_classification tasks , we show that our model , mhal , learns to simultaneously solve them at different levels of granularity by fluidly transferring_knowledge between hierarchies . using a multi-head_attention_mechanism to tie the representations between single words and full sentences , mhal systematically outperforms equivalent models that are not incentivized towards developing compositional representations . moreover , we demonstrate that , with the proposed architecture , the sentence information flows naturally to individual words , allowing the model to behave like a sequence labeller ( which is a lower , word-level task ) even without any word supervision , in a zero-shot fashion .
title : sentiment_classification on tamil and telugu text using rnns and transformers ; abstract : people use social_media to express their opinion , thoughts , and views about a target . sentiment_analysis is found to be a common application that prevails currently and is adopted by all business and social domains . through this , the business could be progressed and the profit could be raised marginally . yet , people have the habit of expressing their views in the form of figurative_language in which the precise meaning of the word should not be considered and as an alternative , the contrary analysis of that view is considered . this is found to be a difficult task since it might not be accounted for either positive or negative . the performance of the sentiment_analysis tasks could be improved by considering these kinds of sarcastic_sentences . several works have been done on forming a model and training the model to detect sarcasm in several languages like hindi , english , telugu , etc . keeping these considerations , a model has been developed which takes the input in the form of bilingual text and does the training to detect the presence of sentiment in the text . the model that has been developed works for the bilingual text and it identifies the existence of sentiment experimental results show that the best model has an overall accuracy of 81 % on the test set .
title : anomaly_detection of software system logs based on natural_language_processing ; abstract : system logs record the daily status of operating_systems , application_software , firewalls , etc . analyzing system logs can help to prevent and eliminate information_security events in real time . in this paper , we propose to analyze the system logs for anomalous event_detection based on natural_language_processing . first , we use doc2vec of natural_language_processing algorithm to construct sentence_vectors , then apply several state-of-the-art classification_algorithms on the sentence_vectors for anomaly_detection . the system logs generated by the thunderbird supercomputer are adopted here to verify the proposed method . the results show that doc2vec combined with machine_learning classification_algorithms could not only effectively extract the semantic information of the logs , but also perform excellent anomaly_detection .
title : investigation of support_vector_machine classifier for opinion_mining ; abstract : complicated text understanding technology which extracts opinion , and sentiment_analysis is called opinion_mining . building systems to collect/examine opinions about a product in blog posts , comments , and reviews/tweets is sentiment_analysis . product_reviews are the focus of existing work on review mining and summarization . this study focuses on movie reviews , investigating opinion classification of online movie reviews based on opinion/corpus words used regularly in reviewed documents . © 2005 - 2014 jatit_&_lls . all rights_reserved .
title : aspect-opinion correlation aware and knowledge-expansion few shot cross-domain_sentiment_classification ; abstract : cross-domain sentiment_analysis has recently_attracted significant attention , which can effectively_alleviate the problem of lacking large-scale labeled_data for deep_neural_network based methods . however , most of the existing cross-domain_sentiment_classification models neglect the domain-specific features , which limits their performance especially when the domain discrepancy becomes larger . meanwhile , the relations between the aspect and opinion terms can not be effectively modeled and thus the sentiment transfer error problem is suffered in the existing unsupervised_domain-adaptation methods . to address these two issues , we propose an aspect-opinion correlation aware and knowledge-expansion few shot cross-domain_sentiment_classification model . sentiment_classification can be effectively conducted with only a few support instances of the target domain . extensive_experiments are conducted and the experimental results show the effectiveness of our proposed model .
title : an efficient machine_learning bayes sentiment_classification method based on review_comments ; abstract : a major concern while incorporating semantic knowledge bases for opinion_mining is that the words selected does not solve attribute relevancy and could not ground positive and negative usage of ambiguous terms . these concerns often make it difficult to classify the opinion_words from user_review_comments . this paper presents a novel method called machine_learning bayes sentiment_classification ( mlbsc ) to improve the classification_accuracy by forming classes ( i.e. , positive , neutral and negative ) based on the extracted words from user_review_comments . initially , related opinion_words are organized for its semantic equivalence of sentiments based on prior training list ( i.e . using extracted words ) . then probabilistic bayes classifiers are applied on semantic opinion_words to evaluate sentiment class_label . the sentiment class_labels are trained for positive , neutral and negative_sentiments with the user_review_comments . the method mlbsc is evaluated for customer_review data sets from research repositories . the mlbsc method produces attribute relevancy and economically significant_gains for customers and performs better out of sample based on review_comments . an intensive and comparative study shows the efficiency of these enhancements and shows better performance in terms of classification_accuracy , size of classes , density of class_label , execution time for class generation .
title : discriminative learning of generative_models : large_margin multinomial mixture_models for document_classification ; abstract : in this paper , a novel discriminative learning method is proposed to estimate generative_models for multi-class pattern classification_tasks , where a discriminative objective_function is formulated with separation margins according to certain discriminative learning criterion , such as large_margin estimation ( lme ) . furthermore , the so-called approximation-maximization ( am ) method is proposed to optimize the discriminative objective_function w.r.t . parameters of generative_models . the am approach provides a good framework to deal with latent_variables in generative_models and it is flexible enough to discriminatively learn many rather complicated generative_models . in this paper , we are interested in a group of generative_models derived from multinomial distributions . under some minor relaxation conditions , it is shown that the am-based discriminative learning methods for these generative_models result in linear_programming ( lp ) problems that can be solved effectively and efficiently even for rather large-scale models . as a case_study , we have studied to learn multinomial mixture_models ( mmms ) for text document_classification based on the large_margin criterion . the proposed methods have been evaluated on a standard rcv1 text_corpus . experimental results show that large_margin mmms significantly_outperform the conventional mmms as well as pure discriminative models such as support_vector_machines ( svm ) , where over 25 % relative classification error_reduction is observed in three independent rcv1 test sets .
title : evaluating preprocessing by turing machine in text_categorization ; abstract : by developing the world_wide_web , text_categorization becomes a key way to deal with a large number of data and organize them . automatic_text_categorization has three steps : preprocessing , extracting relevant features and categorization documents into specified categories . in this article , we propose a new preprocessing method by turing_machine . all of four steps in preprocessing such as sentence segmentation , tokenization , stop_word_removal and word stemming are done by turing_machine . the support_vector_machine model on the reuters and pagod dataset is used to present importance of preprocessing by turing_machine . we used from term_weighting , feature_subset_selection and feature_reduction techniques to find the best document_representation . experiments show that our proposed method is more accurate than other methods . © 2014 ieee .
title : a feature_selection approach based on term_distributions ; abstract : feature_selection has a direct impact on text_categorization . most existing algorithms are based on document_level , and they haven ’ t considered the influence of term_frequency on text_categorization . based on these , we put forward a feature_selection approach , fsatd , based on term_distributions in the paper . in our proposed algorithm , three critical factors which are term_frequency , the inter-class distribution and the intra-class distribution of the terms are all considered synthetically . finally , experiments are made with the help of knn_classifier . and the corresponding results on 20newsgroup and sougoucs corpus show that fsatd algorithm achieves better performance than df and t-test algorithms .
title : less is more : mitigate spurious_correlations for open-domain dialogue response_generation models by causal discovery ; abstract : in this paper , we conduct the first study on spurious_correlations for open-domain response_generation models based on a corpus cgdialog curated in our work . the cur rent models indeed suffer from spurious_correlations and have a tendency of generating irrelevant and generic responses . inspired by causal discovery algorithms , we propose a novel model-agnostic method for training and inference of response_generation model using a conditional_independence classifier . the classifier is trained by a constrained self-training method , coined constrain , to overcome data scarcity . the experimental results based on both human and automatic evaluation show that our method significantly_outperforms the competitive_baselines in terms of relevance , informativeness , and fluency .
title : convolutional lstm network with hierarchical_attention for relation_classification in clinical_texts ; abstract : identifying relation from clinical_texts is a complex and challenging task due to the specific biomedical knowledge . existing_methods for this work generally have the misclassification problem caused by sample class_imbalance . in this paper , we propose a hierarchical_attention-based convolutional long short-term_memory ( convlstm ) network model to solve this problem . we construct a sentence as multi-dimensional hierarchical sequence and directly learn local and global_context information by a single-layer convlstm network . besides , a hierarchical_attention-based pooling is built to capture the parts of a sentence that are relevant with the target semantic relation . experiments on the 2010 i2b2/va relation dataset show that our model outperforms several previous state-of-the-art models without relying on any external features .
title : docscan : unsupervised text_classification via learning from neighbors ; abstract : we introduce docscan , a completely unsupervised text_classification approach using semantic clustering by adopting nearest-neighbors ( scan ) . for each document , we obtain semantically informative vectors from a large pre-trained_language_model . similar documents have proximate vectors , so neighbors in the representation space tend to share topic labels . our learnable clustering approach uses pairs of neighboring datapoints as a weak learning signal . the proposed approach learns to assign classes to the whole dataset without provided ground-truth_labels . on five topic classification benchmarks , we improve on various unsupervised baselines by a large_margin . in datasets with relatively few and balanced outcome classes , docscan approaches the performance of supervised classification . the method fails for other types of classification , such as sentiment_analysis , pointing to important conceptual and practical differences between classifying images and texts .
title : long text to image converter for financial_reports ; abstract : in this study , we proposed a novel article analysis method . this method converts the article classification problem into an image_classification problem by projecting texts into images and then applying cnn models for classification . we called the method the long text to image converter ( ltic ) . the features are extracted automatically from the generated images , hence there is no need of any explicit step of embedding the words or characters into numeric vector representations . this method saves the time to experiment pre-process . this study uses the financial_domain as an example . in companies ' financial_reports , there will be a chapter that describes the company 's financial trends . the content has many financial terms used to infer the company 's current and future 's financial position . the ltic achieved excellent convolution matrix and test data accuracy . the results indicated an 80 % accuracy_rate . the proposed ltic produced excellent_results during practical_application . the ltic achieved excellent_performance in classifying corporate financial_reports under review . the return on simulated investment is 46 % . in addition to tangible returns , the ltic method reduced the time required for article analysis and is able to provide article classification references in a short period to facilitate the decisions of the researcher .
title : performance analysis of ad_hoc classifiers for categorization of uyghur texts ; abstract : this paper starts from the characteristics and writing rules of uyghur , have established a relatively large text_corpus which include 20 categories , 300 documents for each category . and studied the knn , naive_bayes ( nb ) , and svm classification_algorithms more thoroughly , which have widely been used in domestic and foreign academic research fields , then classified the uyghur text by using these algorithms , and analyzed the performance of each algorithm separately . finally , some research directions on uyghur text_classification are also given in this paper .
title : a supervised parameter_estimation method of lda ; abstract : latent_dirichlet_allocation ( lda ) probabilistic_topic_model is a very effective dimension-reduction tool which can automatically_extract latent_topics and dedicate to text_representation in a lower-dimensional semantic topic space . but the original lda and its most variants are unsupervised without reference to category label of the documents in the training corpus . and most of them view the terms in vocabulary as equally_important , but the weight of each term is different , especially for a skewed corpus in which there are many more samples of some categories than others . as a result , we propose a supervised parameter_estimation method based on category and document information which can estimate the parameters of lda according to term_weight . the comparative_experiments show that the proposed method is superior for the skewed text_classification , which can largely improve the recall and precision of the minority category .
title : aspect based sentiment_analysis and smart classification in uncertain feedback pool ; abstract : the data used in the opinion can be either factual or subjective , or both . the subjective form includes a positive or negative view , whereas there are facts in the objective form . as the result of opinion_mining , the subjectivity and objectivity of knowledge are described as created . the outcome may be either positive or negative or a combination of both . machine_learning helps the computer to behave without a specific task being specifically programmed . many consumers will critically evaluate everything online , especially food and amenities in eateries , to show their modest viewpoint . these views are critical in the decision-making process , especially in an uncertain feedback pool . manually evaluating and deriving genuine opinions from these evaluations is very tough despite the growing count of opinions that are accessible in every category . so , to solve this issue , an automated methodology is needed . sentiment evaluations or opinion_extraction are returned methodologies to evaluate and identify uncertain feedback subjects as either positive or negative in these reviews . three different dimensions of recommender_systems are present ; document-based , sentence-based , and aspect-based . report and statement-based opinion_mining concentrates on the broader orientation of the review and does not identify more accurately the crucial aspects of all the extracted reviews . the subject of trend is thus view-based opinion_mining , and the emphasis of this research is on the restaurant 's feedback sector . creative marketing strategies are planned using sentiment assessments and opinion_mining . the sentiment_classifications are managed through a smart method called neuro-fuzzy sentiment_classification . it is an entirely automated method of sentiment estimation and prediction . it can be used to group textual_data and then used to identify patterns about the contents of the textual_information . the resultant data aids in creating autonomous systems that consider the input from customers and prepare restaurant strategies .
title : performance analysis of machine_learning classifiers on improved concept vector_space models ; abstract : this paper provides a comprehensive performance analysis of parametric and non-parametric machine_learning classifiers including a deep feed-forward multi-layer_perceptron ( mlp ) network on two variants of improved concept vector_space ( icvs ) model . in the first variant , a_weighting scheme enhanced with the notion of concept importance is used to assess weight of ontology concepts . concept importance shows how important a concept is in an ontology and it is automatically computed by converting the ontology into a graph and then applying one of the markov based algorithms . in the second variant of icvs , concepts provided by the ontology and their semantically_related terms are used to construct concept vectors in order to represent the document into a semantic vector_space . we conducted various experiments using a variety of machine_learning classifiers for three different models of document_representation . the first model is a baseline concept vector_space ( cvs ) model that relies on an exact/partial match technique to represent a document into a vector_space . the second and third model is an icvs model that employs an enhanced concept weighting_scheme for assessing weights of concepts ( variant 1 ) , and the acquisition of terms that are semantically_related to concepts of the ontology for semantic document_representation ( variant 2 ) , respectively . additionally , a comparison between seven different classifiers is performed for all three models using precision , recall , and f1 score . results for multiple configurations of deep_learning architecture are obtained by varying the number of hidden_layers and nodes in each layer , and are compared to those obtained with conventional classifiers . the obtained results show that the classification performance is highly dependent upon the choice of a classifier , and that the random_forest , gradient boosting , and multilayer_perceptron are among the classifiers that performed rather well for all three models .
title : naive_bayes_classifier and word2vec for sentiment_analysis on bahasa_indonesia cosmetic product_reviews ; abstract : cosmetic_products are products that are widely sold on e-commerce . a product , including a cosmetic product can generate mixed sentiments in the form of customer_reviews . therefore , customer_reviews are one of the most important to be paid attention to . this is because from the customer_reviews , it can be known the level of customer_satisfaction about the product that has been purchased . sentiment_analysis is a solution that can be used to measure customer_satisfaction . sentiment_analysis is a text-based research field that is suitable to discuss the problem of customer_satisfaction about the product . the analysis used is based on several aspects of cosmetic_products , namely aroma , packaging , price , and product . in this study , the problem was solved by analyzing sentiment using the naive_bayes and word2vec methods . based on experiment results , naïve_bayes and word2vec could be used to classify the sentiment . the best model of this research produces an accuracy of 68.17 % with an accuracy of 56.36 % for product aspects , 70.96 % for price aspects , 68.79 % for packaging aspects , and 76.57 % for aroma aspects . this result is obtained when the translated data is used and validated using 2-fold_cross_validation . the parameters for word2vec are window = 7 and size = 300 .
title : how to evaluate machine_translation : a review of automated and human metrics ; abstract : this article presents the most up-to-date , influential automated , semiautomated and human metrics used to evaluate the quality of machine_translation ( mt ) output and provides the necessary background for mt evaluation projects . evaluation is , as repeatedly admitted , highly_relevant for the improvement of mt . this article is divided into three parts : the first one is dedicated to automated metrics ; the second , to human metrics ; and the last , to the challenges posed by neural_machine_translation ( nmt ) regarding its evaluation . the first part includes reference translation-based metrics ; confidence or quality_estimation ( qe ) metrics , which are used as alternatives for quality_assessment ; and diagnostic evaluation based on linguistic checkpoints . human_evaluation metrics are classified according to the criterion of whether human judges directly express a so-called subjective_evaluation judgment , such as 'good ' or 'better than ' , or not , as is the case in error classification . the former methods are based on directly expressed judgment ( dej ) ; therefore , they are called 'dej-based evaluation methods ' , while the latter are called 'non-dej-based evaluation methods ' . in the dej-based evaluation section , tasks such as fluency and adequacy annotation , ranking and direct assessment ( da ) are presented , whereas in the non-dej-based evaluation section , tasks such as error classification and postediting are detailed , with definitions and guidelines , thus rendering this article a useful guide for evaluation projects . following the detailed presentation of the previously mentioned metrics , the specificities of nmt are set forth along with suggestions for its evaluation , according to the latest studies . as human translators are the most adequate judges of the quality of a translation , emphasis is placed on the human metrics seen from a translator-judge perspective to provide useful methodology tools for interdisciplinary_research groups that evaluate mt systems .
title : a discriminative deep_neural_network for text_classification ; abstract : text_classification plays an important role in natural_language_processing . it has been widely applied to sentiment_analysis , stance_detection and fake_news_detection . although previous work on text_classification has made great_progress in recent_years , these methods do not consider discriminative . and the ignoration degrades the performance of text_classification . hence , this paper proposes a novel cnn-based method , which incorporates the power of discriminative by adding an extra regularization term . we conduct_experiments on three datasets , and the results demonstrate that our model is superior to other popular text_classification methods .
title : an impact analysis of features in a classification approach to irony detection in product_reviews ; abstract :
title : intelligent query optimization and course recommendation during online lectures in e-learning system ; abstract : this article explores the possibility of disaggregating query/question information in e-learning system online lectures or course recommendations . information arrangement includes reading , parsing and classification of inquiry/question messages . data extraction is a kind of shallow content processing . it finds a set of predefined applicable content in the feature language archives and performs common language processing through artificial_intelligence strategies . during online lectures , many problems emerged in the listener ’ s minds , and the development of query optimization systems is of great significance to the evaluation of problems in online lectures . the results shows that our proposed method improve the classification of action verbs to a more accurate level . later , we evaluated our proposed method , and measured a very high macro_average for all one-sixth of the cognitive domain . we also provide the analytical examination in which we compared the designed method with the state of the art methods . the results indicate that the proposed method outperform the traditional_methods
title : optical_character_recognition for printed tamil text using unicode ; abstract : optical_character_recognition ( ocr ) refers to the process of converting printed tamil text documents into software translated unicode tamil text . the printed_documents available in the form of books , papers , magazines , etc . are scanned using standard scanners which produce an image of the scanned document . as part of the preprocessing_phase the image file is checked for skewing . if the image is skewed , it is corrected by a simple rotation technique in the appropriate direction . then the image is passed through a noise elimination phase and is binarized . the preprocessed image is segmented using an algorithm which decomposes the scanned text into paragraphs using special space detection technique and then the paragraphs into lines using vertical histograms , and lines into words using horizontal histograms , and words into character_image glyphs using horizontal histograms . each image glyph is comprised of 32 × 32 pixels . thus a database of character_image glyphs is created out of the segmentation phase . then all the image glyphs are considered for recognition using unicode mapping . each image glyph is passed through various routines which extract the features of the glyph . the various features that are considered for classification are the character height , character width , the number of horizontal lines ( long and short ) , the number of vertical lines ( long and short ) , the horizontally oriented curves , the vertically oriented curves , the number of circles , number of slope lines , image centroid and special dots . the glyphs are now set ready for classification based on these features . the extracted features are passed to a support_vector_machine ( svm ) where the characters are classified by supervised_learning algorithm . these classes are mapped onto unicode for recognition . then the text is reconstructed using unicode fonts .
title : document_classification plans in archives and classification theory : a dialogue among knowledge domains ; abstract : this study presents and discusses the theoretical and methodological principles that guide the elaboration of document_classification plans in archives . the emphasis is on the knowledge of the institutional framework . the principles of classification theory pertaining the construction of documentary languages and specially , classification schemes in the scope of the organization of knowledge must be taken into consideration in document_classification plans in archives . finally , it is concluded that for elaboration of such plans , interaction between several areas of knowledge , specifically between archivology and information_science should occur regarding analysis and document_representation .
title : sentiment_analysis of russian reviews to estimate the usefulness of drugs using the domain-specific xlm-roberta model ; abstract : this paper considers the problem of classifying russian-language drug reviews into five classes ( corresponding to the authors ’ rating scores ) and into two classes ( the drug is helpful or useless ) in terms of the sentiment_analysis task . the dataset of reviews with a markup of pharmaceutically-significant entities and the set of neural_network models based on language models are used for the study . the result obtained in this task formulation is compared to a solution based on extracting the named_entities relevant to the drug-taking effectiveness , including the positive dynamics after the drug , the side effects occurring , the worsening of the condition , and the absence of the effect . it is shown that both approaches ( a classification one and one based on the extracted entities ) demonstrate close results in the case of using the best-performing model – xml-roberta-sag .
title : natural_language_processing and the conceptual_model self-organizing_map ; abstract : self-organizing_map can be an effective tool for the textual_data classification . in this paper , we represent the methodology of an integration of the information system modeling and the development of the information system natural_language interface . the main idea of the paper is to build the set of self-organising maps from information system documentation and then reuse it in human-machine communication as a semantic parsing component . the ibm 's information framework ( ifw ) financial_services data_model has been used in an experiment where we tested how appropriate is presented methodology and what is classification_accuracy of the received self-organizing maps . we compare classification_accuracy with the ibm 's websphere voice server nlu solution and demonstrate that self-organising maps can be a competitive components in the information systems natural_language interfaces . ©_springer-verlag_berlin_heidelberg 2007 .
title : knowledge-based clustering scheme for collection management and retrieval of library books ; abstract : we propose a knowledge-based clustering scheme for grouping books in a library . such a grouping is achieved with the help of domain_knowledge in the form of the acm cr ( computing reviews ) category hierarchy . a new knowledge-based similarity_measure is defined and used in clustering books . the proposed scheme is useful in overcoming several problems associated with the existing book collection management and document_retrieval systems . more specifically , it can be used in : ( 1 ) helping the user select an appropriate collection of books in a library which contains the topics of interest ; ( 2 ) assigning a classification number to a new book ; ( 3 ) designing a more appropriate and uniform classification_scheme for books ; and ( 4 ) comparison of libraries based on their collections . initial experiments on a collection of hundred books using the proposed clustering scheme have given us encouraging_results . © 1995 .
title : amplifying the polarity categorization on twitter data using tweet polarizer algorithm and emoticons score ; abstract : mining is utilized to help individuals to separate important data from huge amount of information . opinion_mining or sentiment_analysis concentrates on the exploration and grasp of the feelings from the content generated in social_media . it recognizes the supposition or attitude that an individual has towards a point or an article and it looks to distinguish the perspective hidden in the large content range . knowing clients ’ sentiments and giving the best arrangement or administration is an outstanding business sector procedure pursued by each business . in this paper , we center around producing polarities of tweets to know the general feeling on a given word or a trump card string . an algorithm tweet polarizer is used in this paper to categorize the tweets . a couple of nlp procedures are used to develop a superior methodology for making the most appropriate and possible fringe for a given tweet and to imagine the few trademark features of customers like from which area he has posted the tweet and when .
title : graph representation and semi-clustering approach for label space reduction in multi-label_classification of documents ; abstract : an increasing number of large online text repositories require effective techniques of document_classification . in many cases , more than one class_label should be assigned to documents . when the number of labels is big , it is difficult to obtain required multi-label_classification accuracy . efficient label space dimension reduction may significantly_improve classification performance . in the paper , we consider applying graph-based semi-clustering_algorithm , where documents are represented by vertices with edge_weights calculated according to the similarity of associated texts . semi-clusters are used for finding patterns of labels that occur together . such approach enables reducing label dimensionality . the performance of the method is examined by experiments conducted on real medical documents . the assessment of classification results , in terms of classification_accuracy , f-measure and hamming_loss , obtained for the most popular multi-label classifiers : binary relevance , classifier_chains and label powerset showed good potential of the proposed methodology .
title : benchmarks and models for entity-oriented polarity_detection ; abstract : we address the problem of determining entityoriented polarity in business news . this can be viewed as classifying the polarity of the sentiment expressed toward a given mention of a company in a news article . we present a complete , end-to-end approach to the problem . we introduce a new dataset of over 17,000 manually_labeled documents , which is substantially larger than any currently available resources . we propose a benchmark solution based on convolutional_neural_networks for classifying entity-oriented polarity . although our dataset is much larger than those currently available , it is small on the scale of datasets commonly used for training robust neural_network models . to compensate for this , we use transfer_learning-pre-train the model on a much larger dataset , annotated for a related but different classification_task , in order to learn a good representation for business text , and then fine-tune it on the smaller polarity dataset .
title : neural labeled_lda : a topic_model for semi-supervised document_classification ; abstract : recently , some statistical topic_modeling approaches based on lda have been applied in the field of supervised document_classification , where the model generation procedure incorporates prior_knowledge to improve the classification performance . however , these customizations of topic_modeling are limited by the cumbersome derivation of a specific inference algorithm for each modification . in this paper , we propose a new supervised topic_modeling approach for document_classification problems , neural labeled_lda ( nl-lda ) , which builds on the vae framework , and designs a special generative network to incorporate prior information . the proposed model can support semi-supervised_learning based on the manifold assumption and low-density assumption . meanwhile , nl-lda has a consistent and concise inference method while semi-supervised_learning and predicting . quantitative experimental results demonstrate our model has outstanding_performance on supervised document_classification relative to the compared approaches , including traditional statistical and neural topic_models . specially , the proposed model can support both single-label and multi-label document_classification . the proposed nl-lda performs significantly well on semi-supervised classification , especially under a small amount of labeled_data . further comparisons with related_works also indicate our model is competitive with state-of-the-art topic_modeling approaches on semi-supervised classification .
title : category classification of text data with machine_learning technique for visualizing flow of conversation in counseling ; abstract : the beginner counselors have more likely to continue counseling in their own interest , they have a high tendency to make great use of the closed-ended question in order to confirm the interpretation with the client . while expert counselors are instructing the counseling skill to beginner counselors , we consider that the reaction of a client for a beginner counselor 's question is important to visualize in an appropriate method . to respond the request , we have developed a system for visualizing the flow of conversation in counseling . however , the expert counselor as the system user requires to correct the initial classification result manually , and the work burden is large , because the accuracy of the category classification of conversation data is very low in the current system . to improve this problem , we have implemented on the category classification method of text data with svm ( support_vector_machine ) as machine_learning technique to visualize the flow of conversation in counseling . in addition , we have compared and evaluated with results of the initial classification method of the current system . as these results , we have shown that the accuracy_rate of the classification method with svm become higher than the results in the current system .
title : snad arabic dataset for deep_learning ; abstract : natural_language_processing ( nlp ) captured the attention of researchers for the last years . nlp is applied in various applications and several disciplines . arabic is a language that also benefited from nlp . however , only few arabic datasets are available for researchers . for that , applying the arabic nlp is limited in these datasets . hence , this paper introduces a new dataset , snad . snad is collected to fill the gap in arabic datasets , especially for classification using deep_learning . the dataset has more than 45,000 records . each record consists of the news title , news details , in addition to the news class . the dataset has six different classes . moreover , cleaning and preprocessing are applied to the raw_data to make it more efficient for classification purpose . finally , the dataset is validated using the convolutional_neural_networks and the result is efficient . the dataset is freely available online .
title : a novel semantic smoothing kernel for text_classification with class-based weighting ; abstract : in this study , we propose a novel methodology to build a semantic smoothing kernel to use with support_vector_machines ( svm ) for text_classification . the suggested approach is based on two key_concepts ; class-based term_weighting and changing the orthogonality of vector_space . a class-based term_weighting methodology is used for transformation of documents from the original space to the feature_space . this class-based weighting basically groups terms based on their importance for each class and consequently smooths the representation of documents . this is accomplished by changing the orthogonality of the vector_space_model ( vsm ) with introducing class-based dependencies between terms . as a result , on the extreme case , two documents can be seen as similar even if they do not share any terms but their terms are similarly weighted for a particular class . the resulting semantic kernel can directly make use of class information in extracting semantic information between terms , therefore it can be considered as a supervised kernel . for our experimental evaluation , we analyze the performance of the suggested kernel with a large number of experiments on benchmark textual datasets and present results with respect to varying experimental conditions . to the best of our knowledge , this is the first study to use class-based term_weighting in order to build a supervised semantic kernel for svm . we compare our results with kernels that are commonly used in svm such as linear_kernel , polynomial kernel , radial_basis_function ( rbf ) kernel and with several corpus-based semantic kernels . according to our experimental results the proposed method favorably improves classification_accuracy over linear_kernel and several corpus-based semantic kernels in terms of both accuracy and speed .
title : gcc-git change classifier for extraction and classification of changes in software systems ; abstract : software repositories are used for many purposes like version_control , source_code management , bug and issue_tracking and change log management . github is one of the popular software repositories . github contains commit history of software that lists all changes recorded in the software system , but it does not classify the changes according to the reason for change . in this study a mechanism for extraction and classification of changes is proposed and git change classifier ( gcc ) tool is developed . the tool uses regular_expression to extract changes and employs text_mining to determine the type of change . gcc tool reports the year-wise number of changes for a file and classifies the changes into three types : ( a ) bug repairing changes ( brc ) , ( b ) feature introducing changes ( fic ) and ( c ) general changes ( gc ) . this classification is useful for predicting the effort required for new changes , tracking the resolution of bugs in software and understanding the evolution of the software as it may depend on the type of change .
title : exploring wikipedia 's category graph for query classification ; abstract : wikipedia 's category graph is a network of 400,000 interconnected category_labels , and can be a powerful resource for many classification_tasks . however , its size and the lack of order can make it difficult to navigate . in this paper , we present a new algorithm to efficiently explore this graph and discover accurate classification labels . we implement our algorithm as the core of a query classification system and demonstrate its reliability using the kdd cup 2005 competition as a benchmark . ©_2011_springer-verlag .
title : netclass : a network-based relational_model for document_classification ; abstract : aiming to handle the complexity inherent to the human textual communication , automatic document_classification ( adc ) methods often adopt several simplifications . one such simplification is to consider independent the terms that compose documents , which may hide important relationships between them . these relationships can encapsulate non-trivial and effective patterns to improve classification effectiveness . in this work , we propose netclass , a new network-based model for documents that explicitly considers term relationships and introduce a family of relational algorithms for adc , such as the lrn-wrn classifier—a lazy relational adc algorithm that not only exploits relationships between terms but also neighborhood information . as our extensive experimental evaluation shows , the proposed lrn-wrm achieves_competitive_performance when compared to the state-of-the-art in adc , including svm , considering seven distinct domains . more specifically , lrn-wrn outperforms state-of-the-art classifiers in 5 out of 7 domains , being within the top-2 best-performing classifier in all assessed domains . our evaluation highlights the high effectiveness of our proposal , as well as its efficiency in terms of runtime . indeed , besides effectiveness and efficiency , the simplicity and the absence of a complex parameter_tuning of our proposal are key characteristics that make our algorithms interesting alternatives for adc . particularly , as highlighted by our experimental evaluation , lrn-wrm was shown to be a promising alternative to dynamic domains with a huge volume of short texts ( e.g. , social_media content ) or with several classes .
title : indonesia china_trade relations , social_media and sentiment_analysis : insight from text_mining technique ; abstract : sentiment_analysis ( sa ) employed for detecting , extracting , and classifying people opinions about an issue . social_media is a channel to show people opinions and thoughts . this study aimed to detect and classify indonesia public opinion from twitter written in indonesia language for trade relations between indonesia and china topic with text_mining techniques . the result was the model that detected and classified sentiment of public opinion into negative , neutral or positive_sentiment . the sentiment detected by lexicon-based and rule-based sentiment_analysis . vader was chosen as a tool for sentiment_analysis lexicon-based . the text_classification process was a training stage for the model . the experiment revealed svm classifier performed higher_accuracy value than naïve_bayes , 67.28 % and 64.68 % respectively .
title : language identification as part of the text_corpus creation pipeline at the language bank of finland ; abstract : the language bank of finland hosts text_corpora originating from finland . two of the most used ones are the newspaper and periodical corpus of the national_library of finland and the suomi24 corpus . the language bank has received considerable additions to both corpora and is currently creating new versions of the corpora . we are debuting language identification as part of the corpus creation pipeline . as a language identifier , we are using our recently_published heli-ots software . this paper investigates the results and the quality of language identification . we created a new dataset for evaluating the efficacy of language identification by extracting random samples from both corpora and manually_annotating their language . we were especially interested in seeing how the relatively low ocr quality of the oldest part of the klk-fi collection will affect language identification when using an off-the-shelf program like heli-ots . the oldest part of the klk-fi collection and many parts of the suomi24 corpus contain finnish written dialectally or otherwise differing from the standard written finnish . heli-ots software includes several separate language models for dialectal finnish , and in this article , we evaluate their usefulness using the new data set . for both corpora , the overall micro_f1 scores increase when using the additional dialectal models . additionally , we take a detailed look at some language identification errors and discuss possible solutions .
title : exploring textual_features for multi-label_classification of portuguese film synopses ; abstract : the multi-label_classification of film genres by using features extracted from their synopses has recently gained some attention from the scientific community , however , the number of studies is still limited . these studies are even scarcer for languages other than english . in this work we present the p-tmdb dataset , which contains 13 , 394 portuguese film synopses , and explore the film_genre classification by experimenting with nine different groups of textual_features and four multi-label algorithms . as our dataset is unbalanced , we also conducted experiments with an oversampled version of the dataset . the best result obtained for the original dataset was achieved by a tf-idf based classifier , presenting an average f1 score of 0.478 , while the best result for the oversampled dataset was achieved by a combination of several feature groups and presented an average f1 score of 0.611 .
title : efficient bug_triage for industrial environments ; abstract : bug_triage is an important task for software_maintenance , especially in the industrial environment , where timely bug_fixing is critical for customer_experience . this process is usually done manually and often takes significant time . in this paper , we propose a machine-learning-based solution to address the problem efficiently . we argue that in the industrial environment , it is more suitable to assign bugs to software components ( then to responsible developers ) than to developers directly . because developers can change their roles in industry , they may not oversee the same software module as before . we also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher_accuracy . our solution is based on text-projection features extracted from bug descriptions . we use a deep_neural_network to train the classification model . the proposed solution achieves state-of-the-art performance based on extensive_experiments using multiple data sets . moreover , our solution is computationally_efficient and runs in near real-time .
title : maximum a posteriori based fusion method for speech_emotion_recognition ; abstract : this chapter extends the hybrid scheme into a more general maximum a posteriori ( map ) based fusion method . the proposed fusion method is capable of effectively combining the strengths of several classification methods for recognition of emotional_states in speech_signals . in order to illustrate the effectiveness of the proposed method , probabilistic_neural_network ( pnn ) , universal background model-gaussian_mixture_model ( ubm-gmm ) , and k-nearest_neighbor ( k-nn ) are used as base_classifiers in the numerical_experiments presented in the chapter . numerical results show that higher accuracies can be achieved compared with those obtained using the base_classifiers alone in the classification of 15 emotional_states for the samples extracted from the linguistic data consortium ( ldc ) database . the chapter also discusses the acoustic feature_extraction process for emotion recognition . it presents the proposed map-based fusion classification method .
title : data_integration for toxic_comment_classification : making more than 40 datasets easily_accessible in one unified format ; abstract : with the rise of research on toxic_comment_classification , more and more annotated_datasets have been released . the wide variety of the task ( different languages , different labeling processes and schemes ) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings . despite recent efforts to create web_pages that provide an overview , most publications still use only a single dataset . they are not stored in one central database , they come in many different data formats and it is difficult to interpret their class_labels and how to reuse these labels in other projects . to overcome these issues , we present a collection of more than thirty datasets in the form of a software tool that automatizes downloading and processing of the data and presents them in a unified data format that also offers a mapping of compatible class_labels . another advantage of that tool is that it gives an overview of properties of available datasets , such as different languages , platforms , and class_labels to make it easier to select suitable training and test data .
title : on the role of text_preprocessing in bert embedding-based dnns for classifying informal texts ; abstract : due to highly unstructured and noisy_data , analyzing society reports in written_texts is very challenging . classifying informal text data is still considered a difficult task in natural_language_processing since the texts could contain abbreviated words , repeating characters , typos , slang , et cetera . therefore , text_preprocessing is commonly performed to remove the noises and make the texts more structured . however , we argued that most tasks of preprocessing are no longer required if suitable word_embeddings approach and deep_neural_network ( dnn ) architecture are correctly chosen . this study investigated the effects of text_preprocessing in fine-tuning a pre-trained bidirectional_encoder_representations_from_transformers ( bert ) model using various dnn architectures such as multilayer_perceptron ( mlp ) , long short-term_memory ( lstm ) , bidirectional_long-short_term_memory ( bi-lstm ) , convolutional_neural_network ( cnn ) , and gated_recurrent_unit ( gru ) . various experiments were conducted using numerous learning rates and batch_sizes . as a result , text_preprocessing had insignificant effects on most models such as lstm , bi-lstm , and cnn . moreover , the combination of bert embeddings and cnn produced the best classification performance .
title : from accuracy to versatility : analysing text_classification models regarding transfer_learning ; abstract : while the discriminative performance and classification benchmarks capture most of the attention , the vast amount of data required to train state-of-the-art models is often overlooked . inductive transfer_learning suggests shifting a model between different data domains and hence introduces versatility . in this work , we ( 1 ) thoroughly analyse the ability of text_classification models to adapt to transfer_learning tasks , whether they are specifically designed for it or not . we directly compare the transformer model with an attention-based bi-directional_lstm and naive logistic_regression as a baseline . ( 2 ) exploiting semantic embeddings , we quantify the domain_shift between different classes and predict the expected model transferability and performance . ( 3 ) drawing from our extensive analysis , we experimentally modify the vocabulary size , layer freezing and learning rates , pursing the goal to improve eligibility for transfer_learning . our study reveals that simplistic models may be advantageous in easy transfer_learning tasks due to faster_convergence . an inter-active , online colaboratory notebook that allows reproducing all results , is available here11https : //colab.research.google.com/drive/18cxphah1ym4t5dysuivjjl-b2ggiti4d .
title : a bi-lstm-rnn model for relation_classification using low-cost sequence features ; abstract : relation_classification is associated with many potential applications in the artificial_intelligence area . recent approaches usually leverage neural_networks based on structure features such as syntactic or dependency features to solve this problem . however , high-cost structure features make such approaches inconvenient to be directly used . in addition , structure features are probably domain-dependent . therefore , this paper proposes a bi-directional_long-short-term-memory recurrent-neural-network ( bi-lstm-rnn ) model based on low-cost sequence features to address relation_classification . this model divides a sentence or text segment into five parts , namely two target entities and their three contexts . it learns the representations of entities and their contexts , and uses them to classify relations . we evaluate our model on two standard benchmark_datasets in different domains , namely semeval-2010 task 8 and bionlp-st 2016 task bb3 . in the former dataset , our model achieves comparable performance compared with other models using sequence features . in the latter dataset , our model obtains the third best results compared with other models in the official evaluation . moreover , we find that the context between two target entities plays the most important role in relation_classification . furthermore , statistic experiments show that the context between two target entities can be used as an approximate replacement of the shortest_dependency_path when dependency_parsing is not used .
title : sentiment_classification of web review using association_rules ; abstract : sentiment_classification of web reviews or comments is an important and challenging task in web_mining and data_mining . this paper presents a novel approach using association_rules for sentiment_classification of web reviews . a new restraint measure ad-sup is used to extract discriminative frequent_term_sets and eliminate terms with no sentiment_orientation which contain close frequency in both positive and negative reviews . an optimal classification rule_set is then generated which abandons the redundant general rule with lower confidence than the specific one . in the class_label prediction procedure , we proposed a new metric voting_scheme to solve the problem when the covered rules are not adequately confident or not applicable . the final score of a test review depends on the overall contributions of four metrics . extensive_experiments on multiple domain datasets from web_site demonstrate that 50 % is the best min-conf to guarantee classification rules both abundant and persuasive and the voting strategy obtains improvements on other baselines of using confidence . another comparison to popular machine_learning algorithms such as svm , naïve_bayes and knn also indicates that the proposed method_outperforms these strong benchmarks . ©_2013_springer-verlag_berlin_heidelberg .
title : model-induced term-weighting_schemes for text_classification ; abstract : the bag-of-words representation of text data is very popular for document_classification . in the recent literature , it has been shown that properly weighting the term feature_vector can improve the classification performance significantly beyond the original term-frequency based features . in this paper we demystify the success of the recent term-weighting strategies as well as provide possibly more reasonable modifications . we then propose novel term-weighting_schemes that can be induced from the well-known document probabilistic models such as the naive_bayes and the multinomial term model . interestingly , some of the intuition-based term-weighting_schemes coincide exactly with the proposed derivations . our term-weighting_schemes are tested on large-scale text_classification problems/datasets where we demonstrate improved prediction performance over existing_approaches .
title : a novel text localization scheme for camera captured document_images ; abstract : in this paper , a hybrid model for detecting text_regions from scene_images as well as document_image is presented . at first , background is suppressed to isolate foreground regions . then , morphological operations are applied on isolated foreground regions to ensure appropriate region boundary of such objects . statistical features are extracted from these objects to classify them as text or non-text using a multi-layer_perceptron . classified text components are localized , and non-text ones are ignored . experimenting on a data set of 227 camera captured images , it is found that the object isolation accuracy is 0.8638 and text non-text_classification accuracy is 0.9648 . it may be stated that for images with near homogenous background , the present method yields reasonably satisfactory accuracy for practical_applications .
title : goal-directed extractive_summarization of financial_reports ; abstract : financial_reports filed by various companies discuss compliance , risks , and future plans , such as goals and new projects , which directly impact their stock price . quick consumption of such information is critical for financial analysts and investors to make stock buy/sell decisions and for equity evaluations . hence , we study the problem of extractive_summarization of 10-k reports . recently , transformer-based summarization models have become very popular . however , lack of in-domain labeled summarization data is a major roadblock to train such finance-specific summarization models . we also show that zero-shot inference on such pretrained_models is not as effective either . in this paper , we address this challenge by modeling 10-k report summarization using a goal-directed setting where we leverage summaries with labeled goal-related data for the stock buy/sell classification goal . further , we provide improvements by considering a multi-task_learning method with an industry classification auxiliary task . intrinsic_evaluation as well as extrinsic_evaluation for the stock buy/sell classification and portfolio construction tasks shows that our proposed method significantly_outperforms strong_baselines .
title : classifying cloud provider security conformance to cloud controls matrix ; abstract : security of cloud services is a major concern to cloud consumers when selecting cloud providers . sufficient security information should be provided so that consumer trust in cloud services can be built , but in practice , security information is critical and may not be publicized . during the service selection_process , cloud consumers therefore have to study published information on the cloud providers ' web_sites or the cloud providers registry in order to assess how secure the services are . to assist cloud consumers in service selection , this paper presents an initial attempt to apply text_classification to classify published information on the providers ' web_pages to determine which security best practices and guidelines the providers have followed in providing their services . we take the security best practices and guidelines from the cloud controls matrix ( ccm ) and the accompanying consensus assessments initiative questionnaire ( caiq ) , and compile a set of security concepts before using it as a basis for classifying the providers ' web_pages . the classification result roughly signifies the security conformance level of the providers . we demonstrate this method and present an evaluation using the case of five public cloud providers . © 2014 ieee .
title : cross-domain aspect-level sentiment_analysis based on adversarial distribution alignment ; abstract : the source_domain data with rich sentiment labels is utilized to classify the aspect-level sentiment_polarity for the target domain data without labels . therefore , a cross-domain aspect-level_sentiment_classification model based on adversarial distribution alignment is proposed in this paper . the interactive_attention of aspect words and context is employed to learn semantic relations , and the shared feature_representations are learned by domain classifiers based on gradient reversal layers . the adversarial_training is conducted to expand the alignment boundary of the domain distribution . and then the misclassification problem caused by fuzzy features is alleviated effectively . the experimental results on semeval-2014 and twitter datasets show that the performance of the proposed model is better than other classic aspect-level sentiment_analysis models . the ablation experiment proves that the classification performance can be improved significantly by the strategy of capturing fuzzy features of decision_boundary and expanding the distance between sample and decision_boundary .
title : an intelligent_agent-based system for multilingual financial_news digest ; abstract : nowadays , online financial_news from different sources is widely available on the internet and there exists systems to help investors extracting and analyzing the financial_news . however , many of these systems present news articles in a way without categorization and do not provide enough query options to search or get the specific aspect of news that they want . in this paper , we extend our previous work to develop an intelligent_agent-based system for multi-lingual news extraction . we adopt a document_categorization approach based on fuzzy keyword classification . the system applies fuzzy_clustering to obtain a classification of keywords by concepts of the category . a category profile is developed and worked as a searching interface for document browsing . experimental results show that the proposed categorize news agent is capable of categorizing news documents with a reasonable rate of accuracy . ©_2008_springer-verlag_berlin_heidelberg .
title : a systematic_review of text_classification research based on deep_learning models in arabic_language ; abstract : classifying or categorizing texts is the process by which documents are classified into groups by subject , title , author , etc . this paper undertakes a systematic_review of the latest research in the field of the classification of arabic texts . several machine_learning techniques can be used for text_classification , but we have focused only on the recent trend of neural_network algorithms . in this paper , the concept of classifying texts and classification processes are reviewed . deep_learning techniques in classification and its type are discussed in this paper as well . neural_networks of various types , namely , rnn , cnn , ffnn , and lstm , are identified as the subject of study . through systematic_study , 12 research papers related to the field of the classification of arabic texts using neural_networks are obtained : for each paper the methodology for each type of neural_network and the accuracy ration for each type is determined . the evaluation_criteria used in the algorithms of different neural_network types and how they play a large role in the highly_accurate classification of arabic texts are discussed . our results provide some findings regarding how deep_learning models can be used to improve text_classification research in arabic_language .
title : modeling anchor text and classifying queries to enhance web document_retrieval ; abstract : several types of queries are widely used on the world_wide_web and the expected retrieval method can vary depending on the query type . we propose a method for classifying queries into informational and navigational types . because terms in navigational queries often appear in anchor text for links to other pages , we analyze the distribution of query terms in anchor texts on the web for query classification purposes . while content-based retrieval is effective for informational queries , anchor-based retrieval is effective for navigational queries . our retrieval system combines the results obtained with the content-based and anchor-based retrieval methods , in which the weight for each retrieval result is determined automatically depending on the result of the query classification . we also propose a method for improving anchor-based retrieval . our retrieval method , which computes the probability that a document is retrieved in response to the given query , identifies synonyms of query terms in the anchor texts on the web and uses these synonyms for smoothing purposes in the probability estimation . wte use the ntcir test collections and show the effectiveness of individual methods and the entire web retrieval system experimentally .
title : language in a bottle : language model guided concept bottlenecks for interpretable image_classification ; abstract : concept bottleneck models ( cbm ) are inherently interpretable models that factor model decisions into human-readable concepts . they allow people to easily understand why a model is failing , a critical feature for high-stakes applications . cbms require manually specified concepts and often under-perform their black box counterparts , preventing their broad adoption . we address these shortcomings and are first to show how to construct high-performance cbms without manual specification of similar accuracy to black box models . our approach , language guided bottlenecks ( labo ) , leverages a language model , gpt-3 , to define a large space of possible bottlenecks . given a problem domain , labo uses gpt-3 to produce factual sentences about categories to form candidate concepts . labo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information . ultimately , gpt-3 's sentential concepts can be aligned to images using clip , to form a bottleneck layer . experiments demonstrate that labo is a highly effective prior for concepts important to visual_recognition . in the evaluation with 11 diverse datasets , labo bottlenecks excel at few-shot classification : they are 11.7 % more accurate than black box linear probes at 1 shot and comparable with more data . overall , labo demonstrates that inherently interpretable models can be widely applied at similar , or better , performance than black box approaches .
title : lagan : deep semi-supervised linguistic-anthropology classification with conditional generative_adversarial neural_network ; abstract : education is a right of all , however , every individual is different than others . teachers in post-communism era discover inherent individualism to equally train all towards job market of fourth industrial_revolution . we can consider scenario of ethnic minority education in academic practices . ethnic minority_group has grown in their own culture and would prefer to be taught in their native way . we have formulated such linguistic_anthropology ( how people learn ) based engagement as semi-supervised problem . then , we have developed an conditional deep generative_adversarial_network algorithm namely la-gan to classify linguistic ethnographic features in student engagement . theoretical justification proves the objective , regularization and loss_function of our semi-supervised adversarial model . survey questions are prepared to reach some form of assumptions about z-generation and ethnic minority_group , whose learning style , learning approach and preference are our main area of interest .
title : document_classification using symbolic classifiers ; abstract : in this paper , we present symbolic classifiers to classify text documents . we propose to use cluster_based symbolic_representation followed by symbolic feature_selection methods to classify text documents . in particular , we propose symbolic clustering approaches ; symbolic cluster_based without feature_selection ; symbolic cluster_based with feature_selection ( using similarity_measure ) ; symbolic cluster_based with feature_selection ( using dissimilarity measure ) and symbolic feature clustering approaches . the above mentioned representation methods are very powerful in reducing the dimensionality of feature_vectors for text_classification . to corroborate the efficacy of the proposed model , we conducted extensive_experimentation on various standard text datasets . the experimental results reveal that the symbolic feature clustering approach achieves better classification_accuracy over the existing cluster_based symbolic approaches .
title : literature automatic categorization of chinese academic_journals based on the manual_labeling ; abstract : a new literature categorization method based on the manual_labeling in chinese academic_journals is introduced to solve the text_categorization problem for electronic_journal data processing . in this method , the term vector_space of text is described by automatic word_segmentation . a categorization rule integrates both the term_frequency and the inverse_document_frequency weights by considering the key effect of the manual_labeling . the class expert database is built through sample training and the similarity between the known class and the text to be categorized can be computed to determine the text class . experiments show that the recognition_rate of this method is about 85 % .
title : segmentation of mixed chinese/english document based on afmpf model ; abstract : this paper proposes a novel method to segment document_image based on adaptive feature and multiple phase feedback ( afmpf ) model , which is adaptive to the blurring of document_image and various patterns of mixed chinese/english document . first , the whole process of segmentation is divided into several phases and each phase is allocated a primary feature to segment document_image . second , the segmentation result of each phase is fed back to the current or previous phase , and directs the next phase segmentation . this method causes good and effective combination among character_segmentation , pre-classification of chinese/english and character_recognition , which improves greatly the accuracy of character_segmentation and recognition .
title : half-against-half multi-class text_classification using progressive transductive support_vector_machine ; abstract : progressive transductive support_vector_machine extends transductive support_vector_machine in different class_distribution . it is the solution to the problem that it has to estimate the ratio of positive_negative examples from the sets which are not an easy task to deal with . the paper introduces a half-against-half multi-class text_classification algorithm using progressive transductive support_vector_machine . it shows both in theoretical estimation and experimental results on reuters-21578 data set that hah has significant_advantages over the methods of oaa , oao and ddag in the testing speed , the training_speed and the size of the classifier model , and the accuracy of classification is close to oaa , oao and ddag . it is a promising approach to solving the issues of large-scale multi-class text_classification . ©2009 ieee .
title : duluth at semeval-2019 task 6 : lexical approaches to identify and categorize offensive_tweets ; abstract : this paper describes the duluth systems that participated in semeval -- 2019 task 6 , identifying and categorizing_offensive_language in social_media ( offenseval ) . for the most part these systems took traditional_machine_learning_approaches that built classifiers from lexical_features found in manually_labeled training_data . however , our most successful system for classifying a tweet as offensive ( or not ) was a rule-based black -- list approach , and we also experimented with combining the training_data from two different but related semeval tasks . our best systems in each of the three offenseval tasks placed in the middle of the comparative evaluation , ranking 57th of 103 in task a , 39th of 75 in task b , and 44th of 65 in task c .
title : an efficient approach for ensemble of svm and ann for sentiment_classification ; abstract : nowadays opinion_mining is given more important , since it provides decision_makers to estimate the success of a newly_proposed techniques , novel ad campaign or novel product launch . in general , supervised methods such as support_vector_machine ( svm ) and artificial_neural_network ( ann ) are used to classify the opinions . in some cases svm performs better classification and some cases ann performs better than svm . to overcome this constraint , we proposed a novel ensemble classification method for opinion_mining . this novel technique aims to ensemble artificial_neural_network ( ann ) and support_vector_machine ( svm ) for opinion_analysis . in which , weight factor of both ann and svm is recalibrated by comparing error value between them . the experimental results show that ensemble of ann and svm techniques provides high accuracy than individual .
title : sentiment_analysis of customers ’ review in bangla using machine_learning approaches ; abstract : nowadays , the most deliberate subject , with the goal of assisting one in extracting key information from a large dataset is called sentiment_analysis . while social platform plays a significant part in worldwide , this transforms the valuable public opinion 's medium . it provides an overwhelming amount of textual_information . by examining a huge number of documents , sentiment_analysis is the best technique to discover the author 's thoughts conveyed in evaluations as positive or negative comments . firstly , collected bangla 's review on kn95 , 5 layer mask . so , we prepared a dataset from their statements . we processed our collection of data to eliminate words that are unneeded and phrases taken from the dataset . afterward , we utilized tf-idf vectorizer and support_vector_machine to vectorize the data , gaussian_naïve_bayes and multinomial_naïve_bayes classifier for classify the information in visible form . the dataset secondhand in this place experiment was composed of candidly ready for use comments from connected to the internet shops . using support_vector_machine treasure , this model achieves 83.04 % precision or correctness in contact with the test set and by using gaussian_naïve_bayes this model brings to a successful conclusion 88.23 % , multinomial_naïve_bayes bring to successful conclusion 86.37 % . according to our findings , gaussian_naive_bayes has higher_accuracy , recall , and f1-score than others , whereas multinomial_naive_bayes has higher_precision . it also indicates that the support_vector_machine classifier outperforms the multinomial_naive_bayes classifier in terms of f1-score .
title : bibliometric survey on incremental_learning in text_classification algorithms for false_information detection ; abstract : the false_information or misinformation over the web has severe effects on people , business and society as a whole . therefore , detection of misinformation has become a topic of research among many researchers . detecting misinformation of textual articles is directly connected to text_classification problem . with the massive and dynamic generation of unstructured textual_documents over the web , incremental_learning in text_classification has gained more popularity . this survey explores recent_advancements in incremental_learning in text_classification and review the research publications of the area from scopus , web of science , google_scholar , and ieee databases and perform quantitative_analysis by using methods such as publication statistics , collaboration degree , research network_analysis , and citation_analysis . the contribution of this study in incremental_learning in text_classification provides researchers insights on the latest status of the research through literature survey , and helps the researchers to know the various applications and the techniques used recently in the field .
title : classifying commas for patent machine_translation ; abstract : commas are widely distributed and used in chinese and play important role in detecting boundary of basic units in sentences and discourses . towards chinese-english patent machine_translation , this paper presents two methods using rich linguistic information to identify commas which separate sub-sentences and non-sub-sentences . the first method employs word knowledge_base and formal rules to determine roles of commas , while the second one uses machine_learning approaches . the experimental results show that overall f1 scores of rule-based method are higher than 93 % , indicating the approach performs well in classifying commas . on the other hand , the classifiers show some differences . we also draw the conclusion that identifying commas is actually able to improve the quality of translation outputs .
title : a comparison of svm against pre-trained_language_models ( plms ) for text_classification tasks ; abstract : the emergence of pre-trained_language_models ( plms ) has shown great_success in many natural_language_processing ( nlp ) tasks including text_classification . due to the minimal to no feature_engineering required when using these models , plms are becoming the de facto choice for any nlp task . however , for domain-specific corpora ( e.g. , financial , legal , and industrial ) , fine-tuning a pre-trained model for a specific task has shown to provide a performance_improvement . in this paper , we compare the performance of four different plms on three public domain-free datasets and a real-world dataset containing domain-specific words , against a simple svm linear_classifier with tfidf vectorized text . the experimental results on the four datasets show that using plms , even fine-tuned , do not provide significant gain over the linear_svm classifier . hence , we recommend that for text_classification tasks , traditional svm along with careful feature_engineering can pro-vide a cheaper and superior performance than plms .
title : alternative non-bert model choices for the textual classification in low-resource_languages and environments ; abstract : natural_language_processing ( nlp ) tasks in non-dominant and low-resource_languages have not experienced significant_progress . although pre-trained bert models are available , gpu-dependency , large memory requirement , and data scarcity often limit their applicability . as a solution , this paper proposes a fusion chain architecture comprised of one or more layers of cnn , lstm , and bilstm and identifies precise configuration and chain length . the study shows that a simpler , cpu-trainable non-bert fusion cnn + bilstm + cnn is sufficient to surpass the textual classification performance of the bert-related models in resource-limited languages and environments . the fusion architecture competitively approaches the state-of-the-art accuracy in several bengali nlp tasks and a six-class emotion detection task for a newly_developed bengali dataset . interestingly , the performance of the identified fusion model , for instance , cnn + bilstm + cnn , also holds for other lowresource languages and environments . efficacy study shows that the cnn + bilstm + cnn model outperforms bert implementation for vietnamese languages and performs almost equally in english nlp tasks experiencing artificial data scarcity . for the glue benchmark and other datasets such as emotion , imdb , and intent_classification , the cnn + bilstm + cnn model often surpasses or competes with bert-base , tinybert , distilbert , and mbert . besides , a position-sensitive selfattention layer role further improves the fusion models ’ performance in the bengali emotion classification . the models are also compressible to as low as ≈ 5× smaller through pruning and retraining , making them more viable for resource-constrained environments . together , this study may help nlp practitioners and serve as a blueprint for nlp model choices in textual classification for low-resource_languages and environments .
title : mece method for categorising typing errors ; abstract : this research aims to create an mece ( mutually_exclusive , collectively exhaustive ) categorisation method for typing errors . the research is grounded in theory by gathering typing error_types found in both hci and psychology literature . empirical_studies gathering typing errors from children are used to validate these error_types . it is hoped that at a later date , this categorisation method can be used to detect dyslexia in children by inspecting their typing errors .
title : knowledge-enhanced classification : a scheme for identification of high-quality articles ; abstract : how to find high-quality articles from many articles is the topic of this competition and also the problem that many enterprises want to solve . in this classification problem , from tf-idf to word2vec , then to rnn and lstm , and now to transformer-based models , such as bert , have achieved great improvement in nlu_tasks . however , for many specific problems , such as recognition of high-quality article , directly inputting the text content into the transformer model can not get the optimal_solution , and many other optimizations are needed . in this paper , we try to add statistical features of articles and knowledge graphs , and add entities name of knowledge_graph into bert-based model , specific methods are in sect . 2 and sect . 3 . finally , our model achieved 83.6 f1-score in the official test set and ranked first among all teams in task 2 of ccks-2022 . this paper is divided into four parts : 1 ) the introduction of our task ; 2 ) main ideas of our model ; 3 ) other innovation strategies ; 4 ) experiments and result .
title : graph-based multimodal music mood classification in discriminative latent_space ; abstract : automatic music mood classification is an important and challenging problem in the field of music information_retrieval ( mir ) and has attracted growing attention from variant research areas . in this paper , we proposed a novel multimodal method for music mood classi- fication that exploits the complementarity of the lyrics and audio infor- mation of music to enhance the classification_accuracy . we first extract descriptive sentence-level lyrics and audio features from the music . then , we project the paired low-level features of two different modalities into a learned common discriminative latent_space , which not only eliminates between modality heterogeneity , but also increases the discriminability of the resulting descriptions . on the basis of the latent representation of music , we employ a graph learning based multi-modal classification model for music mood , which takes the cross-modality similarity between local audio and lyrics descriptions of music into account for effective exploitation of correlations between different modalities . the acquired predictions of mood category for every sentence of music are then aggregated by a simple voting_scheme . the effectiveness of the proposed method has been demonstrated in the experiments on a real dataset composed of more than 3,000 min of music and corresponding lyrics .
title : extensions to the speech_disorders classification system ( sdcs ) ; abstract : this report describes three extensions to a classification system for paediatric speech_sound disorders termed the speech_disorders classification system ( sdcs ) . part i describes a classification extension to the sdcs to differentiate motor speech_disorders from speech delay and to differentiate among three sub-types of motor speech_disorders . part ii describes the madison speech assessment protocol ( msap ) , an ∼ 2-hour battery of 25 measures that includes 15 speech tests and tasks . part iii describes the competence , precision , and stability analytics ( cpsa ) framework , a current set of ∼ 90 perceptual- and acoustic-based indices of speech , prosody , and voice used to quantify and classify sub-types of speech_sound disorders ( ssd ) . a companion paper provides reliability estimates for the perceptual and acoustic data reduction methods used in the sdcs . the agreement estimates in the companion paper support the reliability of sdcs methods and illustrate the complementary roles of perceptual and acoustic methods in diagnostic analyses of ssd of unknown origin . examples of research using the extensions to the sdcs described in the present report include diagnostic findings for a sample of youth with motor speech_disorders associated with galactosemia , and a test of the hypothesis of apraxia of speech in a group of children with autism_spectrum_disorders . all sdcs methods and reference databases running in the pepper ( programs to examine phonetic and phonologic evaluation records ) environment will be disseminated without cost when complete . © 2010 informa uk ltd .
title : intent_classification_and_slot_filling for privacy policies ; abstract : understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them . sentences written in a privacy_policy document explain privacy practices , and the constituent text_spans convey further specific information about that practice . we refer to predicting the privacy practice explained in a sentence as intent_classification and identifying the text_spans sharing specific information as slot_filling . in this work , we propose policyie , an english corpus consisting of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of websites and mobile applications . policyie corpus is a challenging real-world benchmark with limited labeled_examples reflecting the cost of collecting large-scale annotations from domain_experts . we present two alternative neural approaches as baselines , ( 1 ) intent_classification_and_slot_filling as a joint sequence tagging and ( 2 ) modeling them as a sequence-to-sequence ( seq2seq ) learning task . the experiment results show that both approaches perform comparably in intent_classification , while the seq2seq method_outperforms the sequence tagging approach in slot_filling by a large_margin . we perform a detailed error_analysis to reveal the challenges of the proposed corpus .
title : entity based sentiment_analysis using syntax patterns and convolutional_neural_network ; abstract : this paper provides an alternative method to extracting object-based sentiment in text_messages , based on modified method previously proposed by mingbo [ 8 ] , in which we first parse the syntax , and then correlate the sentiment with the object of analysis ( also referred to as entity by some , therefore , used in this article interchangeably ) . we show two approaches for the sentiment_polarity classification : syntactic rule patterns and convolutional_neural_network ( cnn ) . even without domain_specific vocabulary and sophisticated classification_algorithms , rule-based approach demonstrates an average macro-f1 based rank among the participants , whereas domainspecific vocabularies show a slightly higher macro-f1_score , but still close to an average result . cnn approach uses syntax dependencies and linear word_order to obtain more extensive information about object_relations . convolution patterns , designed in this approach , are very similar to rules , obtained with rule-based approach . in our proposed approach , the neural_network was trained with different word2vec ( wv ) models ; we compared their performance relative to each other . in this paper , we show that learning a domain-specific wv offers slight progress in performance . resulting macro-f1_score show performance in the into top three of the overall results among the competitors , participating in 2016 sentirueval event . originally , we have not submitted our results to this competition at the time it was held , but had a chance to compare them post-hoc . we also combine the cnn approach with the rule-based approach and discuss the obtained differences in results . all training_sets , evaluation_metrics and experiments are used according to sentirueval 2016 .
title : categorization of medical documents using hybrid competitive neural_network with string_vector , a novel approach ; abstract : text_categorization is one of the well studied problems in data_mining and information_retrieval . even if the research on text_categorization has been progressed very much , traditional_approaches to text_categorization require encoding documents into numerical_vectors which leads to the two main problems : huge dimensionality and sparse distribution in each numerical_vector . although many various feature_selection methods are developed to address the first problem , the reduced dimension remains still large . if the dimension is reduced excessively by a feature_selection method , robustness of text_categorization is degraded . the idea of this research as the solution to the problems is to encode medical documents into string_vectors and apply it to the novel competitive neural_network as a string_vector . the quantitative experiment_results_demonstrate that this method can significantly_improve the performance of medical document_classification . ©_2013_springer-verlag .
title : improved classification of speaking_styles for mental_health monitoring using phoneme dynamics ; abstract : this paper investigates the usefulness of segmental phonemedynamics for classification of speaking_styles . we modeled transition details based on the phoneme sequences emitted by a speech_recognizer , using data obtained from a recording of 39 depressed patients with 7 different speaking_styles - normal , pressured , slurred , stuttered , flat , slow and fast speech . we designed and compared two set of phoneme models : a language model treating each phoneme as a word unit ( one for each style ) and a context-dependent phoneme duration model based on gaussians for each speaking style considered . the experiments showed that language modeling at the phoneme level performed better than the duration model . we also found that better performance can be obtained by user normalization . to see the complementary effect of the phoneme-based models , the classifiers were combined at a decision_level with a hidden_markov_model ( hmm ) classifier built from spectral_features . the improvement was 5.7 % absolute ( 10.4 % relative ) , reaching 60.3 % accuracy in 7-class and 71.0 % in 4-class classification . copyright © 2011 isca .
title : automated annotation of landmark images using community contributed datasets and web_resources ; abstract : a novel solution to the challenge of automatic image_annotation is described . given an image with gps data of its location of capture , our system returns a semantically-rich annotation comprising tags which both identify the landmark in the image , and provide an interesting fact about it , e.g . `` a view of the eiffel_tower , which was built in 1889 for an international exhibition in paris '' . this exploits visual and textual web_mining in combination with content-based image_analysis and natural_language_processing . in the first stage , an input image is matched to a set of community contributed images ( with keyword tags ) on the basis of its gps information and image_classification techniques . the depicted landmark is inferred from the keyword tags for the matched set . the system then takes advantage of the information written about landmarks available on the web at_large to extract a fact about the landmark in the image . we report component evaluation results from an implementation of our solution on a mobile device . image localisation and matching offers 93.6 % classification_accuracy ; the selection of appropriate tags for use in annotation performs well ( f1m of 0.59 ) , and it subsequently automatically_identifies a correct toponym for use in captioning and fact extraction in 69.0 % of the tested cases ; finally the fact extraction returns an interesting caption in 78 % of cases . ©_2011_springer-verlag .
title : deep generalized max_pooling ; abstract : global pooling_layers are an essential part of convolutional_neural_networks ( cnn ) . they are used to aggregate activations of spatial locations to produce a fixed-size vector in several state-of-the-art cnns . global average pooling or global max_pooling are commonly used for converting convolutional features of variable size images to a fix-sized embedding . however , both pooling_layer types are computed spatially independent : each individual activation map is pooled and thus activations of different locations are pooled together . in contrast , we propose deep generalized max_pooling that balances the contribution of all activations of a spatially coherent region by re-weighting all descriptors so that the impact of frequent and rare ones is equalized . we show that this layer is superior to both average and max_pooling on the classification of latin medieval manuscripts ( clamm'16 , clamm'17 ) , as well as writer identification ( historical-wi'17 ) .
title : a hybrid approach for indexing and retrieval of archaeological textual_information ; abstract : this paper focuses on the problem of archaeological textual_information retrieval , covering various field-related topics , and investigating different issues related to special characteristics of arabic . the suggested hybrid retrieval approach employs various clustering and classification methods that enhances both retrieval and presentation , and infers further information from the results returned by a primary retrieval engine , which , in turn , uses latent_semantic_analysis ( lsa ) as a primary retrieval method . in addition , a stemmer for arabic words was designed and implemented to facilitate the indexing process and to enhance the quality of retrieval . the performance of our module was measured by carrying out experiments using standard_datasets , where the system showed promising_results with many possibilities for future_research and further development . ©_springer-verlag 2010 .
title : cognitive principle of binarity in the religious world_view ; abstract : the article discusses the problem of binarity regarded as a universal cognitive mechanism of thinking and world categorization . our observation of scientific_literature has shown that the theory of binary opposition makes the basis of many researches in the field of philosophy , mythology , culturology , structural anthropology and linguistics . the main task of the research is to prove that binary opposition is a fundamental prin-ciple of categorization that greatly influences conceptual and language world views . the article considers the role of binary opposition in the representation of religious world_view verbalized both in the language system and fictional text . we have tried to prove that the key religious concepts ( god - devil , heaven - hell , life - death , light - darkness ) are built ac-cording to a binary principle and express the main axiological categories ( psychological , aesthetic , ethi-cal , spiritual , social ) . the results of a detailed linguistic analysis of the binary concepts heaven - hell have shown that we can regard binarity as a cognitive principle of the religious world_view , built on complex interaction of oppositive but interconnected and interdependent conceptual domains . the distinctive features of the binary opposition in the religious world_view are : a ) its universal character ; b ) a complex cognitive structure , reflecting the interaction of oppositive conceptual features ; c ) various contextual transformations that reflect the author 's individual world_view .
title : algorithmic segmentation of job ads using textual analysis ; abstract : as job ads are getting more prevalent online , an automated analysis is becoming increasingly important , especially in the field of human_resource_management . in this paper , we propose an approach to automatically segment job ads by predefined_categories like the description of a job or the offering company , which is needed to categorize and quantify different aspects of job ads . using a manually_annotated data set , textual_features are extracted for each segment type in a first step and utilized to train state-of-the-art machine_learning classification methods . subsequently , these models are used by iterative algorithms to detect the individual segments . using several optimization techniques like detecting typical segment start phrases , comprehensive evaluations show promising_results .
title : an intelligent information agent for document title classification and filtering in document-intensive domains ; abstract : effective decision_making is based on accurate and timely information . however , human decision_makers are often overwhelmed by the huge amount of electronic data these days . the main_contribution of this paper is the development of effective information agents which can autonomously classify and filter incoming electronic data on behalf of their human users . the proposed information agents are innovative because they can quickly classify electronic documents solely based on the short titles of these documents . moreover , supervised_learning is not required to train the classification models of these agents . document_classification is based on information inference conducted over a high dimensional semantic information space . what is more , a belief_revision mechanism continuously maintains a set of user preferred information categories and filter documents with respect to these categories . preliminary experimental results show that our document_classification and filtering mechanism outperforms the support_vector_machines ( svm ) model which is regarded as one of the best performing classifiers . © 2007 elsevier b.v. all rights_reserved .
title : a novel hybrid approach for fake_news_detection ; abstract : this fake_news is false and misleading_information presented as news . it is frequently done to harm a person 's or company 's reputation , or to profit from advertising revenue . as we all know , the spread of false_news has become a major problem in today 's world of communication . it is disseminated with the goal of misleading people . this has resulted in a number of unpleasant situations in various countries . because of its global influence , false_news has become a topic of discussion among journalists and the general public . the rise of social_media as a medium has a problem with the creation and spread of false_information . this threatens not only the platform where the news content is published but also the internet as a source of communication for the whole story . this paper provides a proactive strategy for predicting fake_news on the internet based on a stance_detection model implemented with lstm + glove classifier . the simplest way to deal with this problem is to compare how well-respected sources feel about such statements , rather than studying the facts . we believe that our hybrid classifier-based system has a higher_level of reliability . the results and discussion section provides evidence for our claims and establishes the objectives .
title : research progress of text_classification technology based on deep_learning ; abstract : with the rapid development of deep_learning technology , many researchers have tried to utilize deep_learning to solve the text_classification problems . especially in terms of the convolutional_neural_network ( cnn）and recurrent_neural_network ( rnn ) many novel and effective classification methods have been proposed . this paper analyzes the problem of text_classification based on deep_neural_network ( dnn ) and introduces the application and development of the cnn , rnn , attention_mechanism and other methods in text_classification . then it analyzes the characteristics and performance of multiple text_classification methods based on deep_learning , and compares the basic network_structures in terms of accuracy and elapsed time , and shows that dnn methods outperform the traditional_machine_learning_methods , and among them , cnn provides better classification performance and generalization_ability . on this basis , this paper points out the deficiencies of the existing deep models for text_classification , and prospects the direction of future_research .
title : improving the accuracy of naïve_bayes algorithm for hoax classification using particle_swarm_optimization ; abstract : hoax news circulation is very widespread which occurs in the media information , both print and online_media . for some people hoax news can only appear on the online_media . but printed medias also often include hoax news in their published news . in the present era , it is very important providing information with relevant and additional facts otherwise it is categorized as hoax . therefore , hoax classification approach is needed . this paper focuses on improving the accuracy of hoax classification in textual_documents contents . naive_bayes algorithm is used to train dataset with the use of pso in the algorithm . experiment is conducted with the trained model over 600 documents . it shows that feature_selection with pso affects the classification results performed using naïve_bayes . accuracy increased from 91.17 % without using feature_selection , to 92.33 % when feature_selection is carried out using pso .
title : a data bootstrapping recipe for low_resource multilingual relation_classification ; abstract : relation_classification ( sometimes called 'extraction ' ) requires trustworthy datasets for fine-tuning large language models , as well as for evaluation . data collection is challenging for indian languages , because they are syntactically and morphologically diverse , as well as different from resource-rich languages like english . despite recent interest in deep generative_models for indian languages , relation_classification is still not well served by public data sets . in response , we present indore , a dataset with 21k entity and relation tagged gold sentences in three indian languages , plus english . we start with a multilingual_bert ( mbert ) based system that captures entity span positions and type information and provides competitive monolingual relation_classification . using this system , we explore and compare transfer mechanisms between languages . in particular , we study the accuracy efficiency tradeoff between expensive gold instances vs. translated and aligned 'silver ' instances . we release the dataset for future_research .
title : sarcasm detection in social_media based on imbalanced classification ; abstract : sarcasm is a pervasive linguistic phenomenon in online documents that express subjective and deeply-felt opinions . detection of sarcasm is of great importance and beneficial to many nlp applications , such as sentiment_analysis , opinion_mining and advertising . current studies consider automatic sarcasm detection as a simple text_classification problem . they do not use explicit features to detect sarcasm and ignore the imbalance between sarcastic and non-sarcastic samples in real applications . in this paper , we first explore the characteristics of both english and chinese sarcastic_sentences and introduce a set of features specifically for detecting_sarcasm in social_media . then , we propose a novel multi-strategy ensemble_learning approach ( msela ) to handle the imbalance_problem . we evaluate our proposed model on english and chinese data sets . experimental results show that our ensemble approach_outperforms the state-of-the-art sarcasm detection approaches and popular imbalanced classification methods . ©_2014_springer_international_publishing_switzerland .
title : inpretation of associative data as a methodogical issue of psycholinguistics ; abstract : associative experiments uncover people 's active attitude to the world represented by language means that determines their relevant strategies of verbal activity and mediates the specifics of their world conceptualization . a word 's associative field modeled on the basis of experimental data is a psychological structure of a word 's content that is relevant for native speakers . associative meaning distinguished via the analysis of distribution of reactions to a stimulus word proves to be an effective method of discovering emerging trends in the change of word_meanings . the author researches the issue of data interpretation in associative experiments . despite the long history of usage , the notion `` associative field '' and the correlation of stimulus and reaction are often interpreted in different ways because , firstly , they model the most complex processes of speech activity ; secondly , most of suggested typologies of associates do not have a common systematization criterion , which hinders the usage of such classifications in research practice and sometimes leads to an ambiguous interpretation of associative data . therefore , the author argues that classifications of associates should be developed depending on : ( 1 ) characteristics of psycholinguistic/linguistic object researched through an associative experiment ; ( 2 ) isomorphism of speech and the activity it accompanies ; ( 3 ) characteristics of mental supports in the cognitive process ; ( 4 ) the way of representation of these supports . such criteria of classification require an analysis of the correlation between stimulus and reaction as a unit of association . this correlation is a separate speech_act where the stimulus is a motive producing the reaction and the associate expresses the author 's communicative intention . this helps to establish motives of associating and thus acquire a more veracious database for modelling different components of speech activity and its overall production/comprehension processes . besides , this approach justifies the principles of worldview modelling . the author presents theoretical and methodological grounds for an effective analysis of associates on the basis of a psycholinguistic object defined by several parameters : strategy of association , dominant psychological function of a language sign that realizes the strategy and the motive of activity explicated in associates .
title : corrigendum : integrating a statistical topic_model and a diagnostic classification model for analyzing items in a mixed format assessment ( front . psychol , ( 2021 ) , 11 , ( 579199 ) , 10.3389/fpsyg.2020.579199 ) ; abstract : in the original publication , an author ’ s name was incorrectly spelled as ‘ h.-j . choi ’ . the correct spelling is ‘ hye-jeong choi ’ . the authors apologize for this error and state that this does not change the scientific conclusions of the article in any way . the original article has been updated .
title : domain_adaptation with category attention_network for deep sentiment_analysis ; abstract : domain_adaptation tasks such as cross-domain_sentiment_classification aim to utilize existing labeled_data in the source_domain and unlabeled or few labeled_data in the target domain to improve the performance in the target domain via reducing the shift between the data distributions . existing cross-domain_sentiment_classification methods need to distinguish pivots , i.e. , the domain-shared sentiment words , and non-pivots , i.e. , the domain-specific sentiment words , for excellent adaptation performance . in this paper , we first design a category attention_network ( can ) , and then propose a model named can-cnn to integrate can and a convolutional_neural_network ( cnn ) . on the one hand , the model regards pivots and non-pivots as unified category attribute words and can automatically capture them to improve the domain_adaptation performance ; on the other hand , the model makes an attempt at interpretability to learn the transferred category attribute words . specifically , the optimization objective of our model has three different components : 1 ) the supervised classification loss ; 2 ) the distributions loss of category feature_weights ; 3 ) the domain invariance loss . finally , the proposed model is evaluated on three public sentiment_analysis datasets and the results demonstrate that can-cnn can outperform other various baseline_methods .
title : news text_classification based on mlcnn and bigru hybrid neural_network ; abstract : in the era of knowledge explosion , text_classification is becoming increasingly crucial . at the same time , with the proposed blockchain , it is of great research significance to actively explore the combination of blockchain and ai , especially to apply text_classification technology to the security classification of blockchain technology . in this paper , we propose a hybrid neural_network model ( mlcnn bigru-att ) based on multilayer convolutional_neural_networks ( mlcnn ) and bidirectional_gated_recurrent_unit ( bigru ) with attention_mechanism in the news text_classification field . gru ( gate recurrent_unit ) , a variant of lstm ( long-short_term_memory ) , has the natural advantages in processing time series tasks , which can readily capture the characteristics of text context information . due to its prominent advantages in local feature_extraction , cnn is also applied to nlp area , in which the researchers have made substantial progress . the experiment results reveal that our model has achieved higher_accuracy on thucnews_dataset and sougou news corpus classification .
title : a new model for making valuable decisions through user social_network profiles and insights ; abstract : recently so many users who are different in power/interest trade news on social_networking sites like arabic twitter and share their views on current_affairs . these opinions/comments ca n't be used to make good decisions or boost output in a particular area without differentiating users according to their closeness/interest to this area . this paper 's major goal is to present a generalized automatic model to analyze user_opinions to make valuable decisions in a particular area based on the degree of user closeness/interest to this area . the proposed model combines rough_set_theory , mendelow 's power-interest model , and data_mining decision-making techniques . rough_set_theory based on mendelow 's power-interest model supports the identification and classification of users by their account features . unsupervised k-means would then be used to cluster their replies/opinions into positive , negative , or neutral . the result generated from the classification of users and the clustering phase of arabic replies/opinions supports the making of valuable/important decisions in a particular area . a case_study is carried out to demonstrate the effectiveness and accuracy of the proposed model .
title : emoji prediction from sentence ; abstract : emojis are small pictures typically used in text_messages via social_media . the synthesis of the visual and textual quality of the same message creates a new way of conversation . despite being commonly used in social_media , from the prespective of natural_language_processing , the underlying semantics of emojis have gained limited attention . we explore the relationship between words and emojis in this project , researching the challenging task of predicting the emojis when conveyed by textual twitter posts . we experimented variant of word_embedding techniques , and train several models such as svc , linearsvc , random_forest classifier and decision_tree classifier .
title : probabilistic event categorization ; abstract : this paper describes the automation of a new text_categorization task . the categories assigned in this task are more syntactically , semantically , and contextually complex than those typically assigned by fully automatic systems that process unseen_test data . our system for assigning these categories is a probabilistic_classifier , developed with a recent method for formulating a probabilistic model from a predefined set of potential features . this paper focuses on feature_selection . it presents a number of fully automatic features . it identifies and evaluates various approaches to organizing collocational properties into features , and presents the results of experiments covarying type of organization and type of property . we find that one organization is not best for all kinds of properties , so this is an experimental parameter worth investigating in nlp systems . in addition , the results suggest a way to take advantage of properties that are low_frequency but strongly indicative of a class . the problems of recognizing and organizing the various kinds of contextual_information required to perform a linguistically complex categorization task have rarely been systematically investigated in nlp .
title : cached long short-term_memory neural_networks for document-level sentiment_classification ; abstract : recently , neural_networks have achieved great_success on sentiment_classification due to their ability to alleviate feature_engineering . however , one of the remaining challenges is to model long texts in document-level sentiment_classification under a recurrent architecture because of the deficiency of the memory unit . to address this problem , we present a cached long short-term_memory neural_networks ( clstm ) to capture the overall semantic information in long texts . clstm introduces a cache mechanism , which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent_unit . the proposed clstm outperforms the state-of-the-art models on three publicly available document-level sentiment_analysis datasets .
title : document_categorization using multilingual associative networks based on wikipedia ; abstract : associative networks are a connectionist language model with the ability to categorize large sets of documents . in this research we combine monolingual associative networks based on wikipedia to create a larger , multilingual associative network , using the cross-lingual connections between wikipedia articles . we prove that such multilingual associative networks perform better than monolingual associative networks in tasks related to document_categorization by comparing the results of both types of associative network on a multilingual dataset .
title : performance analysis of supervised_machine_learning_algorithms for text_classification ; abstract : the demand of text_classification is growing significantly in web searching , data_mining , web ranking , recommendation_systems and so many other fields of information and technology . this paper illustrates the text_classification process on different dataset using some standard supervised_machine_learning_techniques . text documents can be classified through various kinds of classifiers . labeled text documents are used to classify the text in supervised classifications . this paper applied these classifiers on different kinds of labeled_documents and measures the accuracy of the classifiers . an artificial_neural_network ( ann ) model using back propagation network ( bpn ) is used with several other models to create an independent platform for labeled and supervised text_classification process . an existing benchmark approach is used to analysis the performance of classification using labeled_documents . experimental analysis on real data reveals which model works well in terms of classification_accuracy .
title : audio-to-intent using acoustic-textual subword representations from end-to-end asr ; abstract : accurate prediction of the user_intent to interact with a voice assistant ( va ) on a device ( e.g . on the phone ) is critical for achieving naturalistic , engaging , and privacy-centric interactions with the va. to this end , we present a novel approach to predict the user 's intent ( the user speaking to the device or not ) directly from acoustic and textual_information encoded at subword tokens which are obtained via an end-to-end asr model . modeling directly the subword tokens , compared to modeling of the phonemes and/or full words , has at least two advantages : ( i ) it provides a unique vocabulary representation , where each token has a semantic meaning , in contrast to the phoneme-level representations , ( ii ) each subword token has a reusable `` sub '' -word acoustic pattern ( that can be used to construct multiple full words ) , resulting in a largely reduced vocabulary space than of the full words . to learn the subword representations for the audio-to-intent_classification , we extract : ( i ) acoustic information from an e2e-asr model , which provides frame-level ctc posterior_probabilities for the subword tokens , and ( ii ) textual_information from a pre-trained continuous bag-of-words model capturing the semantic meaning of the subword tokens . the key to our approach is the way it combines acoustic subword-level posteriors with text information using the notion of positional-encoding in order to account for multiple asr hypotheses simultaneously . we show that our approach provides more robust and richer representations for audio-to-intent_classification , and is highly_accurate with correctly mitigating 93.3 % of unintended user audio from invoking the smart assistant at 99 % true_positive_rate .
title : neural_networks classifier for data selection in statistical_machine_translation ; abstract : we address the data selection problem in statistical_machine_translation ( smt ) as a classification_task . the new data selection method is based on a neural_network classifier . we present a new method description and empirical results proving that our data selection method provides better translation quality , compared to a state-of-the-art method ( i.e. , cross_entropy ) . moreover , the empirical results reported are coherent across different language pairs .
title : feature_selection methods in persian sentiment_analysis ; abstract : with the enormous growth of digital content in internet , various types of online_reviews such as product and movie reviews present a wealth of subjective_information that can be very helpful for potential users . sentiment_analysis aims to use automated_tools to detect subjective_information from reviews . up to now as there are few researches conducted on feature_selection in sentiment_analysis , there are very rare works for persian sentiment_analysis . this paper considers the problem of sentiment_classification using different feature_selection methods for online_customer_reviews in persian_language . three of the challenges of persian text are using of a wide variety of declensional suffixes , different word spacing and many informal or colloquial words . in this paper we study these challenges by proposing a model for sentiment_classification of persian review documents . the proposed model is based on stemming and feature_selection and is employed naive_bayes algorithm for classification . we evaluate the performance of the model on a collection of cellphone reviews , where the results show the effectiveness of the proposed approaches . ©_2013_springer-verlag_berlin_heidelberg .
title : sarcasm detection in arabic short text using deep_learning ; abstract : recently , a growing interest among researchers has emerged in discovering ambiguous information in short sarcastic texts in arabic . nevertheless , sarcasm is a particularly challenging problem for sentiment_analysis algorithms due to its considerable impact on emotions . a short text evaluation can provide important information about a product or service . however , due to the currently small number of sarcastic datasets and their unbalanced nature , it is also important to preprocess data before classification , especially those with dialects . furthermore , to detect sarcasm , language models must be capable of capturing complicated relationships and ambiguous semantic meanings . in this paper , we attempt to detect sarcasm in arabic text using a large pre-trained_language_model ( bert ) . therefore , a new dataset for the `` isarcasmeval '' shared_task is examined in this paper . preprocessing of the dataset is performed first . moreover , the data is balanced by applying both random swap and random deletion , which are both data_augmentation techniques . following that , two transformer-based models , marbert and arabert , were used to analyze the data . experimental results reveal that the marbert model outperforms the arabert model in this dataset .
title : classification of turkish tweets by document_vectors and investigation of the effects of parameter changes on classification success ; abstract : natural_language_processing is an artificial_intelligence field which is gaining in popularity in recent_years . to make an emotional deduction from texts related to an issue , or classify documents are of great importance considering the increasing data size in today 's world . understanding and interpreting written_texts is a feature that pertains to people . but , it is possible to deduce from texts or classify texts using natural_language_processing which is a sub-branch of machine_learning and artificial_intelligence . in this study , both text_classification was made on turkish tweets , and text_classification success of method parameter changes was investigated using two different methods of the algorithm mentioned as document_vectors in the literature . it was found in the study that as well as higher_accuracy values were obtained by the dbow ( distributed bag of words ) method than dm ( distributed_memory ) method ; higher_accuracy values were also obtained by dbow-ns ( negative_sampling ) architecture than others .
title : classification of environmental resources based on support_vector_machine ; abstract : the large amount of knowledge that is structured and unstructured_data/information that has been overloaded on the business day by day . however , knowledge is not vital but the data clustering is very important . a huge knowledge is generated anytime and therefore the quantity of the information is approaching a massive_data level . however , there are a unit of several unstructured information that ca n't be synchronized with public data platform , as the quantity of their area unit is large . by using support_vector_machine , a large amount of data/information can be efficiently classified . the data /information is arranged in a cluster form so that we can easily identify the data . in this paper we have demonstrated the similar outcomes utilizing distinctive part works for all information . thus , it is important to remove information through arrange this information consequently . since the data of resources has their own particularities that must be contemplated in course of action , this paper characterize a svm based resources modified classifier into existence .
title : a technology of text_classification of data_mining ; abstract : study and application of text data_mining is one of the most important problems in the data_mining . in this paper , we firstly study a method of text data_mining . we first discuss the signification and importance of text data_mining , and present the definition of text_mining and some types of text_classification . then we give the key theory on text_classification in detail , such as data_processing , character mining , character denoting and character matching . finally , we get some results of experiment by using a simple system based on the text_classification method . these results of experiment mean that the method is feasible .
title : classifying arabic tweets based on credibility using content and user features ; abstract : social_media services , such as facebook and twitter , have recently become a huge and continuous source of daily news . people all around the world rely heavily on news published via social_media to know more about current events and activities . as a result , many users have started to exploit social_media by broadcasting misleading news for financial and political purposes , which has an adverse impact on society . in this paper , we utilize machine_learning to identify fake_news from arabic tweets based on a supervised classification model . twitter content published in arabic is very noisy with a high level of uncertainty , where little work has been accomplished to process and extract important features for classification purposes . in this paper , we utilize content-and user-related features , and employ sentiment_analysis to generate new features for the detection of fake arabic news . sentiment_analysis led to improving the accuracy of the prediction process . among a number of machine_learning algorithms used to train the classification models , four algorithms are chosen , namely random_forest , decision_tree , adaboost , and logistic_regression . the experimental evaluation shows that our system can filter out fake_news with an accuracy of 76 % .
title : multi-task deep_learning for legal document translation , summarization and multi-label_classification ; abstract : the digitalization of the legal domain has been ongoing for a couple of years . in that process , the application of different machine_learning ( ml ) techniques is crucial . tasks such as the classification of legal_documents or contract clauses as well as the translation of those are highly_relevant . on the other side , digitized documents are barely accessible in this field , particularly in germany . today , deep_learning ( dl ) is one of the hot_topics with many publications and various applications . sometimes it provides results outperforming the human level . hence this technique may be feasible for the legal domain as well . however , dl requires thousands of samples to provide decent results . a potential solution to this problem is multi-task dl to enable transfer_learning . this approach may be able to overcome the data scarcity problem in the legal domain , specifically for the german_language . we applied the state of the art multi-task model on three tasks : translation , summarization , and multi-label_classification . the experiments were conducted on legal document corpora utilizing several task combinations as well as various model parameters . the goal was to find the optimal configuration for the tasks at hand within the legal domain . the multi-task dl approach outperformed the state of the art results in all three tasks . this opens a new direction to integrate dl technology more efficiently in the legal domain .
title : research on accelerated degradation test of aero-generator based on text_mining ; abstract : as the main source of power for aerospace electromechanical products , the performance of generators has a great impact on the safety of aerospace products . therefore , it is of great significance to predict the life of the aircraft through appropriate accelerated degradation tests . in addition , with the continuous development of the information_age , more and more cases of accelerated degradation test of generators are recorded in the form of texts . these unstructured or semi-structured texts contain rich key features of faults and experimental information . however , redundant_information can not be processed using automated means . therefore , this paper proposes an accelerated research on aerospace mechanical and electrical products based on text_mining . firstly , the text_mining method is used to mine the information of the accelerated test of generator products : the problem of extracting text information is transformed into the classification problem of information , and then the sentence is represented by distributed text , using bidirectional long-term and short-term_memory network ( bilstm ) the classification model extracts the deep semantic features of the event sentence . then , using the mined information to analyze the generator failure mechanism , acceleration stress , performance_degradation index , detection method , degradation modeling method , etc. , the generator acceleration test is designed . finally , the degradation data obtained from the test is processed by a suitable degradation model and the service life of the product in the actual environment is extrapolated . the test methods presented in this paper can greatly_improve the efficiency of accelerated degradation tests . this paper also uses a typical generator as an example to verify the test .
title : capsule_network on social_media text : an application to automatic detection of clickbaits ; abstract : the advent of groundbreaking deep_learning techniques like capsule_network has changed the way of approaching a problem in data_science research . initially , capsule_networks were built and tested on image data and found to be of great use . their usage on textual_data is still very limited . in this paper , we try to investigate whether capsule_network can be used to address a research problem where the classification heavily depends on the textual_data . in various classification_task involving social_networks and online_sources , words and sentences across classes do not vary that much . but , the context and representation of those words play a significant role . one such problem is to correctly identify clickbaits . state of the art solutions either take into account various handcrafted_features from the data or use efficient text_classification techniques like lstm . our work is a stepping stone towards examining whether the need of network properties and feature_engineering can be omitted while using capsule_network . it relaxes the effort of manual feature_construction from the data and looks beyond the sequence to sequence modelling of an lstm based approach . our proposed approach of clickbait_detection using a capsule_network outperforms various existing_methods in terms of multiple performance metric .
title : graphics classification for enterprise knowledge_management ; abstract : enterprise content repositories often consist of business documents comprising not only of traditional text data but also graphics ( org charts , graphs , architecture diagrams , etc . ) that get reused by people across the enterprise . despite this diversity of content , most of the research in enterprise_search has focused on improving document search . we describe a machine_learning approach for graphics classification that automatically_classifies graphics within enterprise documents into an enterprise graphics taxonomy and enables graphics search functionality to augment traditional document-centric enterprise_search . this allows legacy enterprise documents to be automatically converted into a reusable , tagged , graphics repository . our approach works by extracting reusable graphics from enterprise documents , performing feature_extraction to create textual , visual and structural_features that are subsequently used to classify these graphics . we provide experimental results on a real-world data set from accenture . the contributions of this work are automating the creation of a categorized graphics database for enterprise km systems , studying the utility of different feature_sets , and in demonstrating that existing classification and feature_selection methods are appropriate for this task . finally we describe several applications currently being deployed at accenture that are enabled by the categorized graphics repository . © 2010 ieee .
title : ardoc : app_reviews development oriented classifier ; abstract : google_play , apple app_store and windows phone store are well known distribution platforms where users can download mobile_apps , rate them and write review_comments about the apps they are using . previous_research studies demonstrated that these reviews contain important information to help developers improve their apps . however , analyzing reviews is challenging due to the large amount of reviews_posted every day , the unstructured nature of reviews and its varying quality . in this demo we present ardoc , a tool which combines three techniques : ( 1 ) natural_language parsing , ( 2 ) text analysis and ( 3 ) sentiment_analysis to automatically_classify useful feedback contained in app_reviews important for performing software_maintenance and evolution tasks . our quantitative and qualitative_analysis ( involving mobile professional developers ) demonstrates that ardoc correctly classiffes feedback useful for maintenance perspectives in user_reviews with high precision ( ranging between 84 % and 89 % ) , recall ( ranging between 84 % and 89 % ) , and f-measure ( ranging between 84 % and 89 % ) . while evaluating our tool developers of our study confirmed the usefulness of ardoc in extracting important maintenance tasks for their mobile applications .
title : ssncse_nlp @ hasoc-dravidian-codemixfire2020 : offensive_language_identification on multilingual code_mixing text ; abstract : the number of social_media users is increasing_rapidly . a myriad of people have started using native languages in roman alphabets . therefore , it has becomes a big concern to regulate the quality of the text content and messages that are being shared to the internet . in this paper we study the task of offensive message identification for tamil-english and malayalam-english code-mixed content . the char n-gram , tfidf and fine-tuned bert are compared in combination with machine_learning models such as mlp , random_forest and naive_bayes . this work explains the submissions made by ssncse_nlp in hasoc code-mix tasks for hate_speech and offensive_language_detection . we achieve f1 scores of 0.94 for task1-malayalam , 0.75 for task2-malayalam and 0.88 for task2-tamil on the test-set .
title : confiscation detection of criminal judgment using text_classification approach ; abstract : as the system of confiscation becomes more and more perfect , grasping the distribution of the types of confiscations actually announced by the courts will enable you to understand changing of the trend . in addition to assisting legislators in formulating laws , it can also provide other people with an understanding of the actual operation of the confiscation system . in order to enable artificial_intelligence technology to automatically identify the distribution of confiscation , and consumes a lot of manpower and time costs of manual judgment . the purpose of this research is to establish an automated confiscation identification model that can quickly and accurately_identify the multiple label categories of confiscation , and provide the needs of all social circles for confiscation information , so as to facilitate subsequent law amendments or discretion . this research uses the first instance criminal cases as the main experimental data . according to the current laws , the confiscation is divided into three categories : contrabands , criminal tools and criminal proceeds , and perform multiple label identification . this research will use term_frequency_-_inverse_document_frequency ( tf-idf ) and word2vec algorithm as the feature_extraction algorithm , with random_forest classifier , and ckiplabbert pretrained_model for training and identification . the experimental results show that under the ckiplabbert pretrained_model , the best identification effect can be obtained when only use sentences with confiscated words mentioned in the judgment . when the task is case confiscation , the micro_f1 score can be as high as 96.2716 % , and when the task is defendant confiscation , the micro_f1 score is as high as 95.5478 % .
title : accidental learners : spoken_language_identification in multilingual self-supervised models ; abstract : in this paper , we extend previous self-supervised_approaches for language identification by experimenting with conformer based architecture in a multilingual pre-training paradigm . we find that pre-trained speech models optimally encode language discriminatory information in lower layers . further , we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training . after fine-tuning a pre-trained conformer model on the voxlingua107 dataset , we achieve results similar to current state-of-the-art systems for language identification . more , our model accomplishes this with 5x less parameters . we open-source the model through the nvidia nemo toolkit .
title : multi-level gated_recurrent neural_network for dialog_act classification ; abstract : in this paper we focus on the problem of dialog_act ( da ) labelling . this problem has recently_attracted a lot of attention as it is an important sub-part of an automatic question_answering system , which is currently in great demand . traditional_methods tend to see this problem as a sequence labelling task and deals with it by applying classifiers with rich features . most of the current neural_network models still omit the sequential_information in the conversation . henceforth , we apply a novel multi-level gated_recurrent neural_network ( grnn ) with non-textual_information to predict the da tag . our model not only utilizes textual_information , but also makes use of non-textual and contextual_information . in comparison , our model has shown significant improvement over previous_works on switchboard dialog_act ( swda ) task by over 6 % .
title : identifying training_sets for personalized article retrieval system ; abstract : retrieving documents that are relevant to a particular researcher 's purpose is a big challenge , especially when searching through large database , such as pubmed . researchers who use traditional keyword-based document_retrieval systems often end up with a large collection of documents that are not directly relevant to their needs . what is needed is a personalized document_retrieval system that can select only relevant_articles for one 's specific research interests . obtaining an appropriate training_data set is essential in building and testing personalized article retrieval_systems . this study describes one approach to form such training_data set based on articles categorized by domain_experts under mesh major topics . text classifiers , learned using support_vector_machines , were used to test to what degree the training_set categories are differentiable . preliminary_results and analysis of the results are discussed . © 2011 authors .
title : kerminsvm for imbalanced_datasets with a case_study on arabic comics classification ; abstract : many studies have been performed to classify large-sized text documents using different classifiers , ranging from simple distance classifiers such as k-nearest-neighbor ( knn ) to more advanced classifiers such as support_vector_machines . traditional_approaches fail when a short text is encountered due to sparsity resulting from a limited number of words . another common problem in text_classification is class_imbalance ( ci ) . ci occurs when one class of the data contains most of the samples while the other class contains only a few . standard classifiers , when applied to imbalanced_data , result in high accuracy for the majority_class and low accuracy for the minority one . we were motivated to propose a novel framework for classifying the content of arabic comics ; therefore , we propose kerminsvm , a kernel extension of our previously proposed minsvm coupled with a new dimensionality featuring a reduction scheme based on word root frequency ratios ( wrfr ) . kerminsvm was tested on multiple imbalanced benchmark_datasets , and the results were verified using three measures : accuracy , f-measure , and statistical_analysis . wrfr was applied to the manual construction of the arabic comic text dataset to detect strong content in children 's comic_books . test results revealed that our proposed framework outperformed most of the methods for imbalanced_datasets and short_text_classification .